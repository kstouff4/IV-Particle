Wed Feb 1 01:29:45 EST 2023
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  0% |  0% |
numbers of X: 57801215
numbers of Z: 26036
shape of features
(26036,)
shape of features
(26036,)
ZX	Vol	Parts	Cubes	Eps
Z	0.02000741199056093	26036	26.036	0.09159509500143252
X	0.017365800699068047	1677	1.677	0.21796524498346848
X	0.018900888918012886	28959	28.959	0.08674269071267894
X	0.018038357216506526	3047	3.047	0.18090122577401155
X	0.019459411585121402	23284	23.284	0.09419417306122181
X	0.017781937695253415	43651	43.651	0.0741301642305198
X	0.019327188672958698	79726	79.726	0.062352839686332054
X	0.018197063417930873	60881	60.881	0.06686104762006342
X	0.019367112751051414	60252	60.252	0.06850114085098767
X	0.018670377836391205	19723	19.723	0.09818836855297683
X	0.018921511422086854	35983	35.983	0.08071473374197688
X	0.017634079737787703	11510	11.51	0.11528137793593715
X	0.017954788717329997	110373	110.373	0.05458908530004277
X	0.01763778859378573	11822	11.822	0.11426618449859237
X	0.019503158463780226	480166	480.166	0.0343746154319588
X	0.017838045526708406	35926	35.926	0.07918558584602806
X	0.019294354044295258	64804	64.804	0.06677432040482857
X	0.019479705647268825	78393	78.393	0.06286878034879159
X	0.017892383203149412	24822	24.822	0.08966242348234825
X	0.01834107982687659	181296	181.296	0.04659566986922522
X	0.019766710525674795	104298	104.298	0.05744066695018247
X	0.01823199633748217	92153	92.153	0.05826953901536223
X	0.019733386725137272	322479	322.479	0.039406397110584045
X	0.017731371478189848	15844	15.844	0.10382274287887905
X	0.019638390111499777	57212	57.212	0.07001744372092757
X	0.017406204316413022	3794	3.794	0.166163481660134
X	0.01930907769898699	88907	88.907	0.06010932596357322
X	0.01979297442647371	72068	72.068	0.06500142304568457
X	0.018049416940442424	16286	16.286	0.10348631332644366
X	0.01911864804655449	85713	85.713	0.06064618608416004
X	0.019567024057006745	1821773	1821.773	0.022063623564756708
X	0.017561551820323523	23360	23.36	0.09092780005252081
X	0.01997465141579778	957361	957.361	0.027529679737784103
X	0.019843859712844664	37085	37.085	0.08118502989157741
X	0.019093163111225504	17950	17.95	0.10207932977626082
X	0.019172947674963015	51816	51.816	0.07179182623043029
X	0.019837837298186608	280719	280.719	0.04134352528059335
X	0.019854757125477844	114167	114.167	0.055818050371678446
X	0.017524882544500995	1758	1.758	0.21521807788697528
X	0.018126355974217606	4594	4.594	0.15801797957586844
X	0.017473166086694136	3668	3.668	0.16825995147648082
X	0.017675549756701228	5047	5.047	0.1518611001882848
X	0.017280887667218366	2067	2.067	0.20295909771482945
X	0.017399324267585965	865	0.865	0.27196024569873994
X	0.01767780958156084	5205	5.205	0.15031508700488633
X	0.017705585763629565	1572	1.572	0.22415714465706044
X	0.017480823974974507	1132	1.132	0.24902244388458863
X	0.01733194674759088	2214	2.214	0.19855915219635573
X	0.017551903082598002	6234	6.234	0.14120577911559748
X	0.018037589211241156	6555	6.555	0.14013133690104793
X	0.017680399009081485	14737	14.737	0.10625784230657927
X	0.018141195917186232	20216	20.216	0.09645473707850431
X	0.01748517617980764	1703	1.703	0.21734606821014965
X	0.017566404153395807	5559	5.559	0.14674457860759077
X	0.017415597117497208	3574	3.574	0.16953574292980264
X	0.018346571225525313	6871	6.871	0.13873253464262597
X	0.017930197337960994	11106	11.106	0.11731200751073931
X	0.017632732020326593	2342	2.342	0.19599465997338114
X	0.017567053712937982	9968	9.968	0.12078981050876948
X	0.0176724659891584	3571	3.571	0.17041288731225532
X	0.017354843196564915	4541	4.541	0.15634706420982275
X	0.018197560019042363	8831	8.831	0.1272529205649424
X	0.017594370923505267	1795	1.795	0.21401116452520597
X	0.018908691179451688	7870	7.87	0.13393546161912936
X	0.01797941061162755	17763	17.763	0.10040446931373724
X	0.0175604946680512	3808	3.808	0.16644851898689955
X	0.017582392627967208	3016	3.016	0.17997639380247415
X	0.01812463450188107	3202	3.202	0.1782170743621166
X	0.017587424434905272	5466	5.466	0.1476309923588538
X	0.017798595753147847	9648	9.648	0.12264489663065745
X	0.017648291353340492	6818	6.818	0.13730358895191708
X	0.017739700421040002	12694	12.694	0.111801920161143
X	0.018158328122555523	7237	7.237	0.1358852848112405
X	0.017580886136088436	4311	4.311	0.15976718359864472
X	0.017485145894078378	5280	5.28	0.14905450705748527
X	0.01758727728155799	5702	5.702	0.1455650562673219
X	0.01765895896608382	9208	9.208	0.12424147410677004
X	0.01750293517110111	633	0.633	0.3023911370695724
X	0.017671828280668846	1515	1.515	0.2267895033707018
X	0.01761641007025253	6997	6.997	0.1360405917651752
X	0.017491941984084748	22277	22.277	0.0922558202346654
X	0.017399247564885503	2937	2.937	0.18094316128120472
X	0.017441737134440936	3233	3.233	0.17538590070209703
X	0.018134997116631333	3513	3.513	0.1728276058131673
X	0.017481990394611343	3028	3.028	0.1793955511880347
X	0.017374164177720336	1791	1.791	0.2132731147935293
X	0.017575640472287265	1945	1.945	0.2082878089300115
X	0.01787332040247461	2641	2.641	0.18915258456826808
X	0.017531861502406786	1202	1.202	0.24432872077222545
X	0.01761583316245477	4913	4.913	0.15305662811369625
X	0.01788257507365027	4407	4.407	0.15950060528154922
X	0.017669513164593708	8808	8.808	0.12611955391193833
X	0.017596392459765804	2286	2.286	0.19744636277748795
X	0.017763445854058032	3068	3.068	0.17956593603904822
X	0.018051072478502083	6836	6.836	0.1382187577223012
X	0.0176618300757124	7315	7.315	0.13415508040053542
X	0.017598487288217623	5783	5.783	0.14491301162007691
X	0.01930979645170079	3883	3.883	0.1706881750599023
X	0.017694862131201528	9711	9.711	0.1221408889111027
X	0.017673233422548958	15260	15.26	0.10501560208068313
X	0.01782121584949506	6962	6.962	0.1367942272627562
X	0.017535171517849526	10313	10.313	0.119355288486909
X	0.017550116212263312	7895	7.895	0.13050964872800913
X	0.017688780501049	5042	5.042	0.15194917641865335
X	0.01753416013460953	1937	1.937	0.2084099499365052
X	0.017645369952316955	11541	11.541	0.11520264287417274
X	0.01760407888217848	2095	2.095	0.20330267494071189
X	0.017585780625725907	4726	4.726	0.1549610836087046
X	0.017504866404153982	1183	1.183	0.24550368692819013
X	0.01753016763276245	2680	2.68	0.18701815075095407
X	0.017511151772510693	1446	1.446	0.22964067058411006
X	0.017558539417026876	3529	3.529	0.17071784089798187
X	0.01727065932057396	3789	3.789	0.1658039110732135
X	0.01745515229215239	3755	3.755	0.1668929150288463
X	0.01747181861304721	4033	4.033	0.1630184014339056
X	0.017644943191520053	6440	6.44	0.1399302034930887
X	0.017504659471482314	4499	4.499	0.1572820729594317
X	0.01827863913122865	4969	4.969	0.15436835896518147
X	0.017208303826167955	896	0.896	0.26779965829721747
X	0.018739346731324436	10814	10.814	0.12011281921177085
X	0.017161570474138993	1215	1.215	0.2417279765722005
X	0.017921762295292663	13367	13.367	0.11026785375302423
X	0.0175161696273623	2194	2.194	0.19986381502009368
X	0.017446392226528742	2877	2.877	0.1823567835284001
X	0.017663166994768255	3491	3.491	0.17167467822104066
X	0.017414668024329723	3235	3.235	0.17525899016275528
X	0.017667464806996873	3489	3.489	0.17172140051445686
X	0.017651564531496973	1640	1.64	0.22079019628412
X	0.017558793274355472	7963	7.963	0.13015853447880438
X	0.018267167002543248	11345	11.345	0.11720754416711116
X	0.017552779384851135	3061	3.061	0.17898948628784508
X	0.017692491325779335	20770	20.77	0.09479472367314275
X	0.017582668227929848	2291	2.291	0.19725131055885747
X	0.017668775525826182	5650	5.65	0.14623545087753279
X	0.01738988868521584	1743	1.743	0.21527807680011596
X	0.017321506548323325	3175	3.175	0.1760410610726771
X	0.017496929032313986	2606	2.606	0.18865246011851863
X	0.017500874143801973	3418	3.418	0.1723571196462624
X	0.017681846981058947	2799	2.799	0.18485941590054944
X	0.017504652555413308	2889	2.889	0.18230642906448113
X	0.017478750544076264	2248	2.248	0.19810922198888298
X	0.01788803278006591	3758	3.758	0.1682165036878321
X	0.017511290468902287	1342	1.342	0.23542642728996685
X	0.018315071636980557	10784	10.784	0.11930983566002853
X	0.018046498472893305	10217	10.217	0.12088057539018657
X	0.019111722651933956	6552	6.552	0.14288127441066567
X	0.01734482418383977	2875	2.875	0.18204441138159416
X	0.0177214537561935	3245	3.245	0.17610088892750056
X	0.018504320001579058	1583	1.583	0.22695028813447052
X	0.017541028163693826	2425	2.425	0.19339612481661458
X	0.017592925650270697	3385	3.385	0.17321804449824182
X	0.01745478518160069	4091	4.091	0.16219159433639324
X	0.017473872551489505	4270	4.27	0.15995105032995718
X	0.017672560131309904	4249	4.249	0.16081908209911558
X	0.017578918916470104	9235	9.235	0.12393246496608587
X	0.017688662853930434	5716	5.716	0.14572506624349263
X	0.018958751832209256	22975	22.975	0.09379609127008337
X	0.017691920592813055	4882	4.882	0.15360041961214105
X	0.017564807078843118	5999	5.999	0.1430610826649608
X	0.01769935600858996	2851	2.851	0.18378924026419063
X	0.017532764583739403	804	0.804	0.27938180417005426
X	0.018932939613376786	16573	16.573	0.1045375473371896
X	0.017477155124781478	1814	1.814	0.21278670329195642
X	0.017620405389397336	9023	9.023	0.12499375434904414
X	0.017510463972403016	1177	1.177	0.24594635861226957
X	0.017593171045737217	7577	7.577	0.13241858596098757
X	0.01763786917179227	3563	3.563	0.1704289747905979
X	0.01764475226343988	2305	2.305	0.19708256767179627
X	0.017706819135639547	4935	4.935	0.15309151871814358
X	0.01758885113262179	4966	4.966	0.1524322707670235
X	0.01740621638065792	941	0.941	0.2644670585005971
X	0.017213282519571084	683	0.683	0.2931889947142853
X	0.017091114265566903	1806	1.806	0.21151972698702737
X	0.01816283198762821	10393	10.393	0.12045205471968838
X	0.01760104952549742	2585	2.585	0.1895364196405563
X	0.017534090557920125	5313	5.313	0.14888392509182982
X	0.017809373663233336	7336	7.336	0.13439912495690484
X	0.018097718625924908	6692	6.692	0.13932294088618039
X	0.017576273803498313	2766	2.766	0.18522156293718103
X	0.01761208052922246	6049	6.049	0.14279369311131626
X	0.017561846242390464	2967	2.967	0.18089124231358325
X	0.018062431906506637	12482	12.482	0.1131090482891537
X	0.017512253717268868	2274	2.274	0.19747730578389086
X	0.01766521674412305	7352	7.352	0.13393821003305903
X	0.01767174070790828	2940	2.94	0.18182098244043596
X	0.017974702879716142	5212	5.212	0.15108421916930892
X	0.017715723640109142	5368	5.368	0.14888426337781616
X	0.017580485080869176	3236	3.236	0.1757953757976704
X	0.01772749400607941	7526	7.526	0.1330539313132003
X	0.01941551329580426	7120	7.12	0.13970854220324422
X	0.01751361746064852	7382	7.382	0.13337280126163725
X	0.017636369976274303	4544	4.544	0.1571533437977924
X	0.017693570482943646	1159	1.159	0.24807177159306953
X	0.017568621340252947	2627	2.627	0.18840504876463948
X	0.017645954637910757	3062	3.062	0.17928611536749217
X	0.017586263994681835	8995	8.995	0.1250424497537082
X	0.0173565742767749	2590	2.59	0.18853330568065738
X	0.017750807720309215	3133	3.133	0.17827313766034036
X	0.017650431058819843	2616	2.616	0.18896115093992308
X	0.01757375527838338	6365	6.365	0.14028849476796598
X	0.01765908396393068	1748	1.748	0.21617671566887472
X	0.018475877772860638	13117	13.117	0.11209601850248896
X	0.01759817228037948	4391	4.391	0.15884298603694508
X	0.018012687808246844	24329	24.329	0.09046586800508066
X	0.0177295265790891	2651	2.651	0.1884064661417175
X	0.017647456363899496	7456	7.456	0.13326785443423939
X	0.017707260248777425	2394	2.394	0.1948389324770145
X	0.01825302501391974	4275	4.275	0.1622306506403373
X	0.01775768871262932	5250	5.25	0.15010979957250645
X	0.01750578971509923	2105	2.105	0.20260175468991226
X	0.017728020562598267	8160	8.16	0.1295159819779658
X	0.01765797467387383	10902	10.902	0.1174387907243546
X	0.017607879073380687	3076	3.076	0.17888485444645255
X	0.0175420912443433	3371	3.371	0.17329028036903343
X	0.017730201286184088	14219	14.219	0.10763368368549867
X	0.017347925657956544	1230	1.23	0.241609578101647
X	0.017262385826056322	1834	1.834	0.21113836910268477
X	0.017318406201735255	990	0.99	0.2595917636753862
X	0.017564798534688834	1943	1.943	0.20831639793242898
X	0.017414102829277002	13958	13.958	0.10765295866830493
X	0.01759949661696335	3008	3.008	0.18019419865530695
X	0.018024573207668267	5600	5.6	0.1476473492507846
X	0.017526238785428142	1118	1.118	0.2502739455515046
X	0.017983221723837508	8440	8.44	0.12867919956942184
X	0.017552371959184552	3837	3.837	0.16600251640082572
X	0.017380764301062827	2621	2.621	0.18787426003506388
X	0.017537762713398488	4909	4.909	0.15287168500662704
X	0.017753621895759315	2459	2.459	0.19327520646174018
X	0.017846251192123148	2315	2.315	0.19754467865249248
X	0.0174876294368761	5292	5.292	0.14894880919308948
X	0.01764953156670722	1609	1.609	0.22219061604293686
X	0.017608155548955718	3660	3.66	0.16881495871969399
X	0.017372191961492246	4310	4.31	0.15914479892119623
X	0.017472795102856948	2412	2.412	0.19349141185288787
X	0.018948191194048083	6184	6.184	0.14524440967495364
X	0.018092709711125796	2956	2.956	0.1829221912272119
X	0.01744201335324198	3962	3.962	0.1638931168265572
X	0.019178716609959796	11684	11.684	0.1179621459431935
X	0.018332993436593868	6908	6.908	0.13845023099936918
X	0.01755767380185521	3876	3.876	0.16546052675162565
X	0.01836664714848265	28216	28.216	0.08666521364811705
X	0.01847511156272961	60917	60.917	0.0671866294009758
X	0.018042367706384475	8214	8.214	0.12999089792092808
X	0.017666476270372166	12544	12.544	0.11209114264883313
X	0.0174918796431408	1131	1.131	0.24914831737072388
X	0.0180080402857992	9248	9.248	0.12487424558394049
X	0.018498341411015175	13538	13.538	0.1109667228433997
X	0.018859369119703925	145823	145.823	0.050570864436694285
X	0.017727842316403824	4270	4.27	0.16072224929866658
X	0.019783494816582658	224383	224.383	0.0445079598659626
X	0.018237317622168903	74561	74.561	0.06253882912070141
X	0.019326356646538315	82036	82.036	0.061761121142434684
X	0.019799581314517253	320972	320.972	0.03951205515989174
X	0.017644412361841923	4790	4.79	0.1544390880625484
X	0.01755562154747618	7851	7.851	0.1307666739383004
X	0.019120910348647484	277258	277.258	0.04100858897927245
X	0.019222442957376087	628074	628.074	0.031279940749887776
X	0.018464887505887536	6947	6.947	0.1385212671705051
X	0.019448229048216543	61028	61.028	0.06830465588274505
X	0.019843056320401325	28677	28.677	0.08844873301580508
X	0.01805918988915055	48608	48.608	0.07188917573579535
X	0.019010711426015636	126592	126.592	0.05315335753162826
X	0.0180235736270344	62617	62.617	0.06602618678109884
X	0.017855997883981938	34174	34.174	0.08054330323764404
X	0.017727491130822365	20076	20.076	0.09593786068464097
X	0.017532856087402588	4371	4.371	0.15888782516763583
X	0.0191227349126417	149122	149.122	0.05042773109762447
X	0.019430458106128506	19520	19.52	0.09984685935962342
X	0.017901279368214618	4297	4.297	0.1609062470398301
X	0.018407065800884838	26331	26.331	0.08875082604266106
X	0.019820566493334735	131974	131.974	0.05315480888422549
X	0.018213014641018356	216500	216.5	0.043816866057663814
X	0.019870161011066353	114838	114.838	0.05572352591375873
X	0.01763655261985961	6573	6.573	0.13895793621155442
X	0.01931412110085078	52379	52.379	0.07170882105598604
X	0.017566335330382703	6712	6.712	0.13780862738209554
X	0.017808958133677156	50264	50.264	0.07076097724074727
X	0.019348112755403296	68911	68.911	0.06548120847092839
X	0.019008632531749444	12866	12.866	0.11389441878525211
X	0.018863934521888295	120593	120.593	0.05388113394480304
X	0.017474966251221347	947	0.947	0.2642543360580432
X	0.017958226416050126	12642	12.642	0.11241283110400191
X	0.019105961541789417	127206	127.206	0.053156180516502734
X	0.0188271216294163	77514	77.514	0.06239281685735824
X	0.019632302802960994	107999	107.999	0.05664790284672542
X	0.01737227728090481	3848	3.848	0.16527502431835756
X	0.017853379046723527	284381	284.381	0.039744156370362056
X	0.01829732494894912	22594	22.594	0.09321049739939331
X	0.01778978087689509	36719	36.719	0.07854042575717404
X	0.019375761248082445	99757	99.757	0.057912431309653185
X	0.018022683620382337	2097	2.097	0.204836341037416
X	0.018000673911718303	198106	198.106	0.04495697125118647
X	0.01988018634811006	231607	231.607	0.044111950009144196
X	0.019892316176049783	530166	530.166	0.03347787064380872
X	0.017710669725432265	110168	110.168	0.05437423664601743
X	0.018642943658446563	50517	50.517	0.07172859297324519
X	0.017680668467614197	12874	12.874	0.11115484360950557
X	0.01777661152428099	9035	9.035	0.12530650249529482
X	0.019726671252710323	174249	174.249	0.048375764072300695
X	0.019105997848938815	12177	12.177	0.11620098411932546
X	0.017754298769190574	29471	29.471	0.08445711014466332
X	0.019139619274562453	47984	47.984	0.07361149115646752
X	0.01784845502876972	22570	22.57	0.09247471966088718
X	0.017834241859957956	24060	24.06	0.09050092324511942
X	0.018053424309617334	5160	5.16	0.15181101919842083
X	0.018028632514264974	119612	119.612	0.053218505952878165
X	0.018991243076272358	65450	65.45	0.06620354530310266
X	0.019803252910019286	29495	29.495	0.08756476325161167
X	0.018285282988364307	12784	12.784	0.11267086832941993
X	0.019331389967694616	55571	55.571	0.0703296230900437
X	0.019745725439660477	43287	43.287	0.0769789297263093
X	0.01996971696362482	264845	264.845	0.04224675703702483
X	0.0177801706778662	7698	7.698	0.13218616600796324
X	0.018382231321028588	41977	41.977	0.07593854584589833
X	0.01778092821558395	13109	13.109	0.11069507233345102
X	0.019771489500486798	150239	150.239	0.05086498273306323
X	0.017658929313670718	39304	39.304	0.07659067064811485
X	0.018200115421401368	25616	25.616	0.08923206480227103
X	0.017801288467884032	148918	148.918	0.049260794759317175
X	0.01966053331420195	205596	205.596	0.045729193757230904
X	0.01928599976779881	36390	36.39	0.08092574394563869
X	0.019345376832576446	213761	213.761	0.04489688813054757
X	0.01861134178701219	217214	217.214	0.0440855865992208
X	0.019942955571682058	475890	475.89	0.03473449434596377
X	0.017770171139328522	24300	24.3	0.09009384297747768
X	0.01750983304668232	3542	3.542	0.17035091976721062
X	0.018517466894231668	13264	13.264	0.1117641075467688
X	0.01782019767203763	22312	22.312	0.09278077701584836
X	0.018800234300917474	31475	31.475	0.08421688510806817
X	0.019909035571749237	107853	107.853	0.056938496824106836
X	0.017733404149642407	2045	2.045	0.20544690345036876
X	0.01973920244561905	80770	80.77	0.06252108498320036
X	0.01921146407672226	136480	136.48	0.05201921766560114
X	0.017476173472820444	9175	9.175	0.12395957864624685
X	0.017598288138306953	11244	11.244	0.11610471651329557
X	0.019630187845786122	81346	81.346	0.06225813177735247
X	0.018636730379178665	16583	16.583	0.10396860268003251
X	0.018925126208714445	14880	14.88	0.1083457676716944
X	0.017795673502297306	20101	20.101	0.09602085937227403
X	0.01765851560434295	19986	19.986	0.09595687665323879
X	0.017705808871306017	30097	30.097	0.08379102849039806
X	0.017919787187988236	50380	50.38	0.07085299750103026
X	0.019667375672229478	29300	29.3	0.08755741496620169
X	0.019702175344528478	43616	43.616	0.07672839526277897
X	0.01777803689951465	18252	18.252	0.09912680635227379
X	0.019904227634776665	1070744	1070.744	0.026490280692242556
X	0.017653160265542464	23239	23.239	0.0912434453829849
X	0.019734154523282508	167566	167.566	0.0490167155554835
X	0.017599706084618597	2589	2.589	0.1894339382869378
X	0.01767521020653958	10396	10.396	0.11935284404158351
X	0.01890661983046149	4983	4.983	0.15596997461818293
X	0.018534687277299774	27183	27.183	0.08801620232283389
X	0.018119447107161256	5960	5.96	0.1448659194812805
X	0.019254653813468584	107083	107.083	0.056442346642878644
X	0.01765542995338753	9763	9.763	0.12183301812271558
X	0.01817729061301883	6036	6.036	0.14440868275090196
X	0.019815724694076487	305285	305.285	0.04018847234050193
X	0.019806981421104552	67988	67.988	0.06629213551741646
X	0.01919560987276541	40031	40.031	0.07827116991007242
X	0.019893669946609398	155237	155.237	0.05041656146115718
X	0.0199553711376217	417517	417.517	0.03629068914542362
X	0.017818023539754656	9495	9.495	0.1233449916062439
X	0.017774603408155256	15987	15.987	0.10359631674595426
X	0.017595003406349844	18215	18.215	0.0988522881741567
X	0.01761259406413831	2231	2.231	0.19911680655939587
X	0.017896362160664942	10872	10.872	0.11807331920306442
X	0.017743701406587698	17831	17.831	0.09983653664917852
X	0.01762120947802481	3539	3.539	0.1707595691971736
X	0.01758580271891753	10384	10.384	0.11919714109884263
X	0.01772406624483186	26638	26.638	0.08730127437270971
X	0.01764751758552134	8066	8.066	0.12982005891544884
X	0.019701341086309446	337795	337.795	0.03878057026135861
X	0.017548469812829766	18129	18.129	0.09892099315900067
X	0.01948116522118682	91509	91.509	0.059710454014584975
X	0.01775415833002531	4511	4.511	0.15788556314464855
X	0.01818707933243201	5823	5.823	0.14617464811334777
X	0.019570998535451595	174272	174.272	0.04824605301307211
X	0.019805909394789354	630424	630.424	0.03155397060249476
X	0.019084845279677267	593256	593.256	0.031804030998939276
X	0.018127871957361538	27812	27.812	0.08670381388326012
X	0.01782382258020733	47307	47.307	0.0722257090730963
X	0.0177682116259948	7499	7.499	0.13331533905254137
X	0.018394285972173834	7678	7.678	0.13380682797477111
X	0.018804836547641646	600760	600.76	0.03151540201476921
X	0.019683219905208632	20850	20.85	0.09809872475132221
X	0.017624275579099118	12278	12.278	0.1128048784695749
X	0.019265485532046087	53076	53.076	0.07133356423482777
X	0.019963214780396567	750043	750.043	0.029857133204052075
X	0.017593986678200238	46851	46.851	0.07214647740511274
X	0.01756121781566281	21704	21.704	0.09318334214492026
X	0.01973934276190606	14393	14.393	0.11110334154798507
X	0.0179287878583325	30297	30.297	0.08395574771889014
X	0.017819873688582264	7530	7.53	0.13326104514380097
X	0.019849621451636764	210328	210.328	0.045528678578527514
X	0.017611774715948433	16919	16.919	0.10134666850446689
X	0.017625405047565885	3421	3.421	0.17271445135581717
X	0.019280417652780893	126823	126.823	0.053371098095420864
X	0.01885417644640165	23326	23.326	0.09315133826612533
X	0.017432086086959443	1111	1.111	0.2503485619147244
X	0.019337583326084932	72246	72.246	0.06444602425802419
X	0.018749487301264744	119976	119.976	0.053863967494270394
X	0.019387100192096336	282864	282.864	0.040924029785840164
X	0.019909150470900465	244123	244.123	0.04336586467229979
X	0.019394258083812912	236552	236.552	0.04344257058552911
X	0.01907675238492352	27861	27.861	0.08813922905335947
X	0.017684455777202235	29612	29.612	0.0842121301276491
X	0.018800851253180748	182843	182.843	0.046848935252604404
X	0.019680688265100624	248512	248.512	0.042943519940870756
X	0.018198386854744106	16710	16.71	0.10288502031288119
X	0.019544834452146824	226675	226.675	0.0441783465145967
X	0.01954233513061365	318972	318.972	0.03942224007592589
X	0.019826314224344426	203341	203.341	0.04602625896190131
X	0.01840720695344007	110527	110.527	0.05501821603572966
X	0.01926914065747947	63459	63.459	0.06721347884691815
X	0.019042911919134507	19600	19.6	0.09904345059779099
X	0.017729562390201556	10309	10.309	0.11981020894053102
X	0.017607153796208398	16590	16.59	0.10200330822175793
X	0.019774290641261485	177267	177.267	0.048138328477815295
X	0.0178259727275491	136383	136.383	0.05074941021686302
X	0.019688829764616163	289202	289.202	0.040832521929007735
X	0.019790541334977246	98067	98.067	0.058655885379572566
X	0.019441227772194144	92396	92.396	0.05947806612974631
X	0.017627117357975907	17920	17.92	0.09945220813892837
X	0.018887982338131968	209532	209.532	0.04483787995099452
X	0.01772988164571912	7924	7.924	0.1307937896219202
X	0.017648422529854133	15524	15.524	0.10436801404106018
X	0.019423635612219693	154845	154.845	0.05005849385744948
X	0.01864501307862309	99673	99.673	0.05719109179447083
X	0.018028762048118225	6728	6.728	0.13889714147670265
X	0.018539881162691636	22694	22.694	0.09348285451525169
X	0.01992803998723711	651697	651.697	0.031270845324119466
X	0.018238855179562356	21503	21.503	0.09465992877294453
X	0.01957396054591843	129291	129.291	0.053297086808686805
X	0.01755774256082618	11974	11.974	0.11360823367729168
X	0.01746144619066742	8580	8.58	0.12672558577331106
X	0.017575912734028445	6350	6.35	0.14040461644147284
X	0.017530327967697372	6476	6.476	0.1393673579624749
X	0.019658089047952763	25266	25.266	0.09197467645018767
X	0.018724648979138593	77978	77.978	0.06215563832952961
X	0.018942456326426098	5077	5.077	0.15509925452324147
X	0.017976751615409865	183294	183.294	0.04611628590373953
X	0.017570188246479063	15934	15.934	0.10331194405934609
X	0.019842955695597606	81023	81.023	0.06256517750515013
X	0.01798207960836415	15276	15.276	0.10558691202380972
X	0.01818926517186549	192513	192.513	0.04554615266447392
X	0.017629092824176133	6574	6.574	0.13893129640608667
X	0.01804284871116288	28946	28.946	0.08542248147266734
X	0.01816105793015317	49152	49.152	0.07175739264129868
X	0.019141102045791638	35623	35.623	0.08129780422934119
X	0.019932306465374514	82397	82.397	0.0623087040829673
X	0.01958191285387733	715884	715.884	0.03013034832971706
X	0.01775179901776151	27795	27.795	0.08611759602157648
X	0.017546180651210122	7190	7.19	0.13463291076679842
X	0.018083531222900615	68049	68.049	0.06429154033153851
X	0.017754121231750176	31645	31.645	0.08247671603214841
X	0.019640043106598674	518317	518.317	0.03358785949683662
X	0.01769682092851857	9760	9.76	0.12194064266574287
X	0.019731703622718633	157165	157.165	0.050072919719703
X	0.019958815287650453	446299	446.299	0.03549519897722742
X	0.017580734297226588	9572	9.572	0.12246484584017424
X	0.01839988745016389	273437	273.437	0.04067422580521348
X	0.01829399611144036	22112	22.112	0.09387721249713622
X	0.019863149925232634	575523	575.523	0.0325583068643065
X	0.017586371547841116	22874	22.874	0.09161038518226952
X	0.017696379832953454	19444	19.444	0.0969094821786546
X	0.019619086223307356	15517	15.517	0.10813268956202596
X	0.017723284774672457	5815	5.815	0.14498781296062602
X	0.01788167374783177	10384	10.384	0.11986190132766508
X	0.01844263610681523	52628	52.628	0.07050209845945975
X	0.017593376739799733	8497	8.497	0.1274562670868873
X	0.019957605730603616	208863	208.863	0.045717482678240585
X	0.017668652983505163	19754	19.754	0.09634949970359306
X	0.01809204399757607	12243	12.243	0.11390250146152528
X	0.019199452747746964	27810	27.81	0.0883817548470016
X	0.019981677643109704	214388	214.388	0.045339545107001666
X	0.019928772954794935	207279	207.279	0.045811560422068624
X	0.019923139556531976	406725	406.725	0.036589148529785134
X	0.019751649083724288	17057	17.057	0.10501070412672918
X	0.01769457771729682	8694	8.694	0.12672827467180928
X	0.017574437933818028	6470	6.47	0.13952725639827712
X	0.01766289361857613	4482	4.482	0.15795377158336182
X	0.019894416779029454	232066	232.066	0.04409336381885162
X	0.01888885478118733	7431	7.431	0.1364749012738407
X	0.01780200474775191	27969	27.969	0.08601958134595715
X	0.017697229901573287	26880	26.88	0.08699454268847458
X	0.019767814050447287	25257	25.257	0.09215642684622953
X	0.019759636173324286	46342	46.342	0.07526644265735355
X	0.017509584308853632	12992	12.992	0.11045870346229185
X	0.019682062550028422	219866	219.866	0.044733977065359275
X	0.018053033615006493	72606	72.606	0.06288193439730032
X	0.0176284921232924	10879	10.879	0.11745605129489414
X	0.019335329180733135	36498	36.498	0.08091471357563183
X	0.018822982667417824	85071	85.071	0.06048332177234204
X	0.01897855500462949	94543	94.543	0.058552384753522306
X	0.019879843308028947	521462	521.462	0.03365607217370628
X	0.018164528933508266	120485	120.485	0.05322271837234254
X	0.01778278549393028	6075	6.075	0.14304889339403676
X	0.017948041998402033	37000	37.0	0.07857271940379877
X	0.018718094269001253	33609	33.609	0.0822751494452427
X	0.017658790111530792	8477	8.477	0.12771431758779278
X	0.017554668916599406	13700	13.7	0.10861522143971541
X	0.017792738824442118	7193	7.193	0.1352417913296929
X	0.018067249965582146	31498	31.498	0.08308762749185454
X	0.017690681060473997	75408	75.408	0.0616749837118687
X	0.017259694108590122	8035	8.035	0.12902753620945182
X	0.017666194051585715	26548	26.548	0.0873045870625126
X	0.017835828672696986	9397	9.397	0.12381350603034029
X	0.018223132646925627	27128	27.128	0.08757935718245226
X	0.01770478953527292	7785	7.785	0.1315055404753073
X	0.019677291529634534	29471	29.471	0.08740242502294286
X	0.019066751595613263	21922	21.922	0.09545503584189488
X	0.01769035360743183	15618	15.618	0.10424063104864018
X	0.017823249484181403	8080	8.08	0.13017427727501568
X	0.01770723375721715	11444	11.444	0.1156620690804752
X	0.01831103926338803	33350	33.35	0.08188525740060307
X	0.01870470556263994	137653	137.653	0.0514108832412665
X	0.01802385999605972	7275	7.275	0.13531260941265993
X	0.01753094032884565	3273	3.273	0.1749657604752376
X	0.017674516279420117	14102	14.102	0.10781742171334187
X	0.01777389769696153	88690	88.69	0.05851977851491499
X	0.019948691264763056	608895	608.895	0.031998085933871434
X	0.019074558877020063	135322	135.322	0.05204296505716398
X	0.017614911013434344	7764	7.764	0.13140089065574007
X	0.019920650686218188	306493	306.493	0.04020631954896852
X	0.019785507276250617	223615	223.615	0.04456036631356189
X	0.017688648665187906	3310	3.31	0.1748325213256962
X	0.019037619052515067	10207	10.207	0.12309436505219276
X	0.01936422515851448	64152	64.152	0.06708055406722409
X	0.01927032099262361	114184	114.184	0.055262165443718696
X	0.018173860564139832	269578	269.578	0.04069936015024083
X	0.019749766577022883	202403	202.403	0.046037847210076414
X	0.01779867220160949	41493	41.493	0.07541729003873032
X	0.018096175779538405	9402	9.402	0.12439096961332591
X	0.017483368867595092	3314	3.314	0.17408347157245618
X	0.018019186831534743	79574	79.574	0.06095200547348753
X	0.017814320075727846	17624	17.624	0.10035867580765999
X	0.01766701383556824	18091	18.091	0.09921260627103025
X	0.019875589394407814	136228	136.228	0.052644282604229806
X	0.019899091713578274	633727	633.727	0.031548376445374726
X	0.018091503306255644	12940	12.94	0.1118184479326652
X	0.017485511453673795	2458	2.458	0.1923234113936931
X	0.018926457225472707	18187	18.187	0.10133732131721794
X	0.019198386644460805	100989	100.989	0.05749943192396975
X	0.01810325955546101	52205	52.205	0.07025569904172572
X	0.017468545278510966	11294	11.294	0.11564751862749373
X	0.01755342737762805	3083	3.083	0.17856491609224204
X	0.018251212258922483	40667	40.667	0.07656257416263729
X	0.01766555663736791	7930	7.93	0.13060246674618162
X	0.019725580800456704	48797	48.797	0.0739399056789369
X	0.01786228018832129	7701	7.701	0.13237214042467876
X	0.019037588354521375	18870	18.87	0.10029516809597461
X	0.018247721506550674	18494	18.494	0.0995541259205252
X	0.017824690511292964	11360	11.36	0.11620225488083097
X	0.01766203209900212	22551	22.551	0.09217750630888283
X	0.018665621091536317	183748	183.748	0.046659485063848156
X	0.019712859573098217	363797	363.797	0.037841078438245634
X	0.01824611480762597	95333	95.333	0.05762917217189309
X	0.019328095817033725	150642	150.642	0.050436816376237745
X	0.018959994774339448	23551	23.551	0.09302712949289334
X	0.017569906836588003	19131	19.131	0.09720246137281496
X	0.01979517871700084	27723	27.723	0.08937987316339689
X	0.01964189016332105	159860	159.86	0.04971428028547574
X	0.01946184701336217	29333	29.333	0.08721861595148842
X	0.019583308042885113	39752	39.752	0.07897852595780783
X	0.017742273802023006	15558	15.558	0.10447647309321038
X	0.019979443215103546	1365239	1365.239	0.024460100196844376
X	0.018163069165036762	25398	25.398	0.0894258834500522
X	0.018414463410964338	77450	77.45	0.06195065208201195
X	0.01848776066830004	175876	175.876	0.04719462904878075
X	0.019269250271267958	61316	61.316	0.06798769758460363
X	0.019371554021416708	41659	41.659	0.07747326761898603
X	0.019988058081264127	1081964	1081.964	0.026435402705509065
X	0.019537690530794294	231236	231.236	0.04388060358057048
X	0.01923608431967038	119263	119.263	0.05443400817247819
X	0.017695815638392166	11688	11.688	0.11482685252117354
X	0.019774346566573942	52421	52.421	0.07225461866344006
X	0.017759822223983243	10366	10.366	0.11965820120120375
X	0.019932519071156934	232495	232.495	0.044094341059606995
X	0.01998261113714711	1219415	1219.415	0.025399991309708773
X	0.01991864148178877	102490	102.49	0.057924115014919265
X	0.01975857903270922	164755	164.755	0.04931424271225527
X	0.019216733617028418	103386	103.386	0.05706974679893011
X	0.019668221143118055	31797	31.797	0.08520393360538962
X	0.01944895619090869	38274	38.274	0.07979900602952777
X	0.01763412077130059	14473	14.473	0.10680668016254219
X	0.019724270733344457	163142	163.142	0.04944758180423999
X	0.019959662675408524	553421	553.421	0.03303942806987052
X	0.018904617656136092	100271	100.271	0.05734085759672736
X	0.01990936302448167	868529	868.529	0.02840693301399974
X	0.0197332319062933	163053	163.053	0.04946406551000396
X	0.018409336140542708	59872	59.872	0.06749497704190333
X	0.01767872803818846	29904	29.904	0.08392807142397743
X	0.019434508229182017	116305	116.305	0.055079719939920405
X	0.019462034915611402	196527	196.527	0.04626529555486337
X	0.01767090653945583	15447	15.447	0.10458552025603828
X	0.019757807314483025	659214	659.214	0.03106257403710371
X	0.018878842087420494	26511	26.511	0.08929964821328745
X	0.019071175220342358	48532	48.532	0.07324586111233568
X	0.018043748272927946	6947	6.947	0.13746004663956513
X	0.017764728206730824	16984	16.984	0.10150938472141452
X	0.017580194176760027	2741	2.741	0.18579679182048528
X	0.01762782466499387	6975	6.975	0.13621287830285025
X	0.01820423824218997	24744	24.744	0.09027500777910465
X	0.019741027914059864	91646	91.646	0.059944878286759005
X	0.01997363672780919	3052101	3052.101	0.018704786203421066
X	0.018836224415028403	18066	18.066	0.10140139974306768
X	0.019324712485017933	382822	382.822	0.03695773817803946
X	0.01777198598028575	11688	11.688	0.11499137128470886
X	0.019530456428964525	25012	25.012	0.09208480885907898
X	0.017580734054918355	7922	7.922	0.13043697610980795
X	0.019362847822467284	186959	186.959	0.04696139096366278
X	0.017701364229446032	45829	45.829	0.07282639054919435
X	0.019931948913252066	1034920	1034.92	0.02680490846522517
X	0.017644217914687767	7660	7.66	0.13206605785081452
X	0.01987867532015373	194446	194.446	0.046758740985503686
X	0.01842532020325591	27076	27.076	0.08795830388135902
X	0.017775809241164422	143984	143.984	0.04979339239610108
X	0.019610494190139112	245489	245.489	0.043067748386082495
X	0.017667175883883706	29623	29.623	0.08417427116773675
X	0.01765731429893083	16144	16.144	0.10303177130319471
X	0.019752240187675695	124007	124.007	0.05420716804711181
X	0.018615368432621026	22242	22.242	0.0942394189880202
X	0.019060606814475054	39580	39.58	0.07838271654792126
X	0.018940825472239013	326704	326.704	0.03870331371357267
X	0.018109299078586574	17093	17.093	0.10194386998239559
X	0.019889214203981015	85818	85.818	0.06142517635219062
X	0.01842171758483473	37275	37.275	0.07906255093408003
X	0.019260238533895715	131664	131.664	0.052690405252082385
X	0.019890189539754157	268312	268.312	0.04200809057905169
X	0.018200600372503227	67185	67.185	0.06470499344009639
X	0.018818276571491814	15956	15.956	0.10565383683356115
X	0.018181263494201676	135698	135.698	0.05117017136247587
X	0.017613448671309023	105242	105.242	0.055108480333388006
X	0.01988468808968962	347396	347.396	0.03853878244040289
X	0.01875929236770558	77980	77.98	0.06219341539564161
X	0.018580098022788077	40397	40.397	0.07719093393728366
X	0.0198715945970479	46141	46.141	0.07551766827050847
X	0.01788181767550991	179739	179.739	0.04633648797214618
X	0.01906561933537783	136621	136.621	0.051869391434837875
X	0.018351975929694512	40652	40.652	0.0767126475013756
X	0.017519277910417918	7757	7.757	0.131202105183238
X	0.01962600250239192	9832	9.832	0.12591089954403775
X	0.017930819438343275	67170	67.17	0.06438849334937824
X	0.01872966270909988	113159	113.159	0.054905217784830894
X	0.018664434023742855	90847	90.847	0.05900671148335185
X	0.01872938661362425	13953	13.953	0.1103109572801398
X	0.018256389618215456	29623	29.623	0.0850998156003352
X	0.018242165380399966	41156	41.156	0.07624553392985936
X	0.018707922293969535	55489	55.489	0.06959951603256308
X	0.019149636305001744	28127	28.127	0.08797225119880073
X	0.018419343490555587	70936	70.936	0.06379732713171603
X	0.018586231429385553	10913	10.913	0.11942146514696693
X	0.017983618551518768	47074	47.074	0.07256023151451844
X	0.01758733783015794	4765	4.765	0.15454171720626997
X	0.017669618093874535	10586	10.586	0.11862195875442097
X	0.01987994535779251	265215	265.215	0.04216373104527442
X	0.017983447811275123	21187	21.187	0.09468208887783326
X	0.01923642054148011	107038	107.038	0.056432430977882225
X	0.017763187528069654	6560	6.56	0.13938167751198902
X	0.017812449157338546	7792	7.792	0.13173208467003733
X	0.01819654641124485	42337	42.337	0.07546686276004101
X	0.019907715193456185	96637	96.637	0.059059893556596584
X	0.019129186672345803	43463	43.463	0.07606629589262286
X	0.01767002447537242	7081	7.081	0.1356378359710402
X	0.01801508090728822	14628	14.628	0.1071890499743659
X	0.019312509142997036	46558	46.558	0.07457869771081795
X	0.019075287835763108	28019	28.019	0.087970992056041
X	0.018064876337261507	104295	104.295	0.055743020980417786
X	0.017762457858372865	7329	7.329	0.13432374129516805
X	0.017487257875016208	8764	8.764	0.1258944104734943
X	0.01961421280411482	93512	93.512	0.05941569466338994
X	0.017575917220061858	11230	11.23	0.11610370552010474
X	0.019902829924590797	209955	209.955	0.04559629310245301
X	0.01975424119141037	19841	19.841	0.09985403025127516
X	0.01904067871422175	99996	99.996	0.05753073630498323
X	0.01900927851347377	160321	160.321	0.049127546292917865
X	0.019241451871527163	78811	78.811	0.06250052521461652
X	0.019835478179144328	167322	167.322	0.04912431895655674
X	0.019569333779253534	150162	150.162	0.050699693311972785
X	0.017773949631025222	2347	2.347	0.19637684808225292
X	0.01937241635566194	68168	68.168	0.06574575959547756
X	0.01911677400869329	25000	25.0	0.09144462834143968
X	0.019074550588165356	127312	127.312	0.053112285632112226
X	0.01778866029277932	112896	112.896	0.054011747527039425
X	0.01766683824469012	47576	47.576	0.0718770596078227
X	0.019610224575438588	77887	77.887	0.06314503335247762
X	0.017672219361399406	26856	26.856	0.08697943624833643
X	0.017886065638726942	41446	41.446	0.07556906796331886
X	0.01762245465160341	5769	5.769	0.14509594907532997
X	0.017876974592733796	11416	11.416	0.11612525685303633
X	0.019563815333905974	176482	176.482	0.04803794043667973
X	0.019964244921811303	494658	494.658	0.03430172419530486
X	0.01913945482591239	24573	24.573	0.09200761944086593
X	0.017829469359450525	21404	21.404	0.09409094387746761
X	0.019762878067134216	36917	36.917	0.08119723924108722
X	0.01989896053685335	112604	112.604	0.056116708644404094
X	0.017739954657660484	76428	76.428	0.06145633683955061
X	0.017794969115759462	10148	10.148	0.12058841679570581
X	0.01941750192134755	69695	69.695	0.06531264091330698
X	0.0199803117573291	162862	162.862	0.049689065857324244
X	0.01755683939315912	9641	9.641	0.12211661323963524
X	0.01950108481053207	53455	53.455	0.0714534954600556
X	0.017786281384086108	40887	40.887	0.07577046819465934
time for making epsilon is 3.005845785140991
epsilons are
[0.21796524498346848, 0.08674269071267894, 0.18090122577401155, 0.09419417306122181, 0.0741301642305198, 0.062352839686332054, 0.06686104762006342, 0.06850114085098767, 0.09818836855297683, 0.08071473374197688, 0.11528137793593715, 0.05458908530004277, 0.11426618449859237, 0.0343746154319588, 0.07918558584602806, 0.06677432040482857, 0.06286878034879159, 0.08966242348234825, 0.04659566986922522, 0.05744066695018247, 0.05826953901536223, 0.039406397110584045, 0.10382274287887905, 0.07001744372092757, 0.166163481660134, 0.06010932596357322, 0.06500142304568457, 0.10348631332644366, 0.06064618608416004, 0.022063623564756708, 0.09092780005252081, 0.027529679737784103, 0.08118502989157741, 0.10207932977626082, 0.07179182623043029, 0.04134352528059335, 0.055818050371678446, 0.21521807788697528, 0.15801797957586844, 0.16825995147648082, 0.1518611001882848, 0.20295909771482945, 0.27196024569873994, 0.15031508700488633, 0.22415714465706044, 0.24902244388458863, 0.19855915219635573, 0.14120577911559748, 0.14013133690104793, 0.10625784230657927, 0.09645473707850431, 0.21734606821014965, 0.14674457860759077, 0.16953574292980264, 0.13873253464262597, 0.11731200751073931, 0.19599465997338114, 0.12078981050876948, 0.17041288731225532, 0.15634706420982275, 0.1272529205649424, 0.21401116452520597, 0.13393546161912936, 0.10040446931373724, 0.16644851898689955, 0.17997639380247415, 0.1782170743621166, 0.1476309923588538, 0.12264489663065745, 0.13730358895191708, 0.111801920161143, 0.1358852848112405, 0.15976718359864472, 0.14905450705748527, 0.1455650562673219, 0.12424147410677004, 0.3023911370695724, 0.2267895033707018, 0.1360405917651752, 0.0922558202346654, 0.18094316128120472, 0.17538590070209703, 0.1728276058131673, 0.1793955511880347, 0.2132731147935293, 0.2082878089300115, 0.18915258456826808, 0.24432872077222545, 0.15305662811369625, 0.15950060528154922, 0.12611955391193833, 0.19744636277748795, 0.17956593603904822, 0.1382187577223012, 0.13415508040053542, 0.14491301162007691, 0.1706881750599023, 0.1221408889111027, 0.10501560208068313, 0.1367942272627562, 0.119355288486909, 0.13050964872800913, 0.15194917641865335, 0.2084099499365052, 0.11520264287417274, 0.20330267494071189, 0.1549610836087046, 0.24550368692819013, 0.18701815075095407, 0.22964067058411006, 0.17071784089798187, 0.1658039110732135, 0.1668929150288463, 0.1630184014339056, 0.1399302034930887, 0.1572820729594317, 0.15436835896518147, 0.26779965829721747, 0.12011281921177085, 0.2417279765722005, 0.11026785375302423, 0.19986381502009368, 0.1823567835284001, 0.17167467822104066, 0.17525899016275528, 0.17172140051445686, 0.22079019628412, 0.13015853447880438, 0.11720754416711116, 0.17898948628784508, 0.09479472367314275, 0.19725131055885747, 0.14623545087753279, 0.21527807680011596, 0.1760410610726771, 0.18865246011851863, 0.1723571196462624, 0.18485941590054944, 0.18230642906448113, 0.19810922198888298, 0.1682165036878321, 0.23542642728996685, 0.11930983566002853, 0.12088057539018657, 0.14288127441066567, 0.18204441138159416, 0.17610088892750056, 0.22695028813447052, 0.19339612481661458, 0.17321804449824182, 0.16219159433639324, 0.15995105032995718, 0.16081908209911558, 0.12393246496608587, 0.14572506624349263, 0.09379609127008337, 0.15360041961214105, 0.1430610826649608, 0.18378924026419063, 0.27938180417005426, 0.1045375473371896, 0.21278670329195642, 0.12499375434904414, 0.24594635861226957, 0.13241858596098757, 0.1704289747905979, 0.19708256767179627, 0.15309151871814358, 0.1524322707670235, 0.2644670585005971, 0.2931889947142853, 0.21151972698702737, 0.12045205471968838, 0.1895364196405563, 0.14888392509182982, 0.13439912495690484, 0.13932294088618039, 0.18522156293718103, 0.14279369311131626, 0.18089124231358325, 0.1131090482891537, 0.19747730578389086, 0.13393821003305903, 0.18182098244043596, 0.15108421916930892, 0.14888426337781616, 0.1757953757976704, 0.1330539313132003, 0.13970854220324422, 0.13337280126163725, 0.1571533437977924, 0.24807177159306953, 0.18840504876463948, 0.17928611536749217, 0.1250424497537082, 0.18853330568065738, 0.17827313766034036, 0.18896115093992308, 0.14028849476796598, 0.21617671566887472, 0.11209601850248896, 0.15884298603694508, 0.09046586800508066, 0.1884064661417175, 0.13326785443423939, 0.1948389324770145, 0.1622306506403373, 0.15010979957250645, 0.20260175468991226, 0.1295159819779658, 0.1174387907243546, 0.17888485444645255, 0.17329028036903343, 0.10763368368549867, 0.241609578101647, 0.21113836910268477, 0.2595917636753862, 0.20831639793242898, 0.10765295866830493, 0.18019419865530695, 0.1476473492507846, 0.2502739455515046, 0.12867919956942184, 0.16600251640082572, 0.18787426003506388, 0.15287168500662704, 0.19327520646174018, 0.19754467865249248, 0.14894880919308948, 0.22219061604293686, 0.16881495871969399, 0.15914479892119623, 0.19349141185288787, 0.14524440967495364, 0.1829221912272119, 0.1638931168265572, 0.1179621459431935, 0.13845023099936918, 0.16546052675162565, 0.08666521364811705, 0.0671866294009758, 0.12999089792092808, 0.11209114264883313, 0.24914831737072388, 0.12487424558394049, 0.1109667228433997, 0.050570864436694285, 0.16072224929866658, 0.0445079598659626, 0.06253882912070141, 0.061761121142434684, 0.03951205515989174, 0.1544390880625484, 0.1307666739383004, 0.04100858897927245, 0.031279940749887776, 0.1385212671705051, 0.06830465588274505, 0.08844873301580508, 0.07188917573579535, 0.05315335753162826, 0.06602618678109884, 0.08054330323764404, 0.09593786068464097, 0.15888782516763583, 0.05042773109762447, 0.09984685935962342, 0.1609062470398301, 0.08875082604266106, 0.05315480888422549, 0.043816866057663814, 0.05572352591375873, 0.13895793621155442, 0.07170882105598604, 0.13780862738209554, 0.07076097724074727, 0.06548120847092839, 0.11389441878525211, 0.05388113394480304, 0.2642543360580432, 0.11241283110400191, 0.053156180516502734, 0.06239281685735824, 0.05664790284672542, 0.16527502431835756, 0.039744156370362056, 0.09321049739939331, 0.07854042575717404, 0.057912431309653185, 0.204836341037416, 0.04495697125118647, 0.044111950009144196, 0.03347787064380872, 0.05437423664601743, 0.07172859297324519, 0.11115484360950557, 0.12530650249529482, 0.048375764072300695, 0.11620098411932546, 0.08445711014466332, 0.07361149115646752, 0.09247471966088718, 0.09050092324511942, 0.15181101919842083, 0.053218505952878165, 0.06620354530310266, 0.08756476325161167, 0.11267086832941993, 0.0703296230900437, 0.0769789297263093, 0.04224675703702483, 0.13218616600796324, 0.07593854584589833, 0.11069507233345102, 0.05086498273306323, 0.07659067064811485, 0.08923206480227103, 0.049260794759317175, 0.045729193757230904, 0.08092574394563869, 0.04489688813054757, 0.0440855865992208, 0.03473449434596377, 0.09009384297747768, 0.17035091976721062, 0.1117641075467688, 0.09278077701584836, 0.08421688510806817, 0.056938496824106836, 0.20544690345036876, 0.06252108498320036, 0.05201921766560114, 0.12395957864624685, 0.11610471651329557, 0.06225813177735247, 0.10396860268003251, 0.1083457676716944, 0.09602085937227403, 0.09595687665323879, 0.08379102849039806, 0.07085299750103026, 0.08755741496620169, 0.07672839526277897, 0.09912680635227379, 0.026490280692242556, 0.0912434453829849, 0.0490167155554835, 0.1894339382869378, 0.11935284404158351, 0.15596997461818293, 0.08801620232283389, 0.1448659194812805, 0.056442346642878644, 0.12183301812271558, 0.14440868275090196, 0.04018847234050193, 0.06629213551741646, 0.07827116991007242, 0.05041656146115718, 0.03629068914542362, 0.1233449916062439, 0.10359631674595426, 0.0988522881741567, 0.19911680655939587, 0.11807331920306442, 0.09983653664917852, 0.1707595691971736, 0.11919714109884263, 0.08730127437270971, 0.12982005891544884, 0.03878057026135861, 0.09892099315900067, 0.059710454014584975, 0.15788556314464855, 0.14617464811334777, 0.04824605301307211, 0.03155397060249476, 0.031804030998939276, 0.08670381388326012, 0.0722257090730963, 0.13331533905254137, 0.13380682797477111, 0.03151540201476921, 0.09809872475132221, 0.1128048784695749, 0.07133356423482777, 0.029857133204052075, 0.07214647740511274, 0.09318334214492026, 0.11110334154798507, 0.08395574771889014, 0.13326104514380097, 0.045528678578527514, 0.10134666850446689, 0.17271445135581717, 0.053371098095420864, 0.09315133826612533, 0.2503485619147244, 0.06444602425802419, 0.053863967494270394, 0.040924029785840164, 0.04336586467229979, 0.04344257058552911, 0.08813922905335947, 0.0842121301276491, 0.046848935252604404, 0.042943519940870756, 0.10288502031288119, 0.0441783465145967, 0.03942224007592589, 0.04602625896190131, 0.05501821603572966, 0.06721347884691815, 0.09904345059779099, 0.11981020894053102, 0.10200330822175793, 0.048138328477815295, 0.05074941021686302, 0.040832521929007735, 0.058655885379572566, 0.05947806612974631, 0.09945220813892837, 0.04483787995099452, 0.1307937896219202, 0.10436801404106018, 0.05005849385744948, 0.05719109179447083, 0.13889714147670265, 0.09348285451525169, 0.031270845324119466, 0.09465992877294453, 0.053297086808686805, 0.11360823367729168, 0.12672558577331106, 0.14040461644147284, 0.1393673579624749, 0.09197467645018767, 0.06215563832952961, 0.15509925452324147, 0.04611628590373953, 0.10331194405934609, 0.06256517750515013, 0.10558691202380972, 0.04554615266447392, 0.13893129640608667, 0.08542248147266734, 0.07175739264129868, 0.08129780422934119, 0.0623087040829673, 0.03013034832971706, 0.08611759602157648, 0.13463291076679842, 0.06429154033153851, 0.08247671603214841, 0.03358785949683662, 0.12194064266574287, 0.050072919719703, 0.03549519897722742, 0.12246484584017424, 0.04067422580521348, 0.09387721249713622, 0.0325583068643065, 0.09161038518226952, 0.0969094821786546, 0.10813268956202596, 0.14498781296062602, 0.11986190132766508, 0.07050209845945975, 0.1274562670868873, 0.045717482678240585, 0.09634949970359306, 0.11390250146152528, 0.0883817548470016, 0.045339545107001666, 0.045811560422068624, 0.036589148529785134, 0.10501070412672918, 0.12672827467180928, 0.13952725639827712, 0.15795377158336182, 0.04409336381885162, 0.1364749012738407, 0.08601958134595715, 0.08699454268847458, 0.09215642684622953, 0.07526644265735355, 0.11045870346229185, 0.044733977065359275, 0.06288193439730032, 0.11745605129489414, 0.08091471357563183, 0.06048332177234204, 0.058552384753522306, 0.03365607217370628, 0.05322271837234254, 0.14304889339403676, 0.07857271940379877, 0.0822751494452427, 0.12771431758779278, 0.10861522143971541, 0.1352417913296929, 0.08308762749185454, 0.0616749837118687, 0.12902753620945182, 0.0873045870625126, 0.12381350603034029, 0.08757935718245226, 0.1315055404753073, 0.08740242502294286, 0.09545503584189488, 0.10424063104864018, 0.13017427727501568, 0.1156620690804752, 0.08188525740060307, 0.0514108832412665, 0.13531260941265993, 0.1749657604752376, 0.10781742171334187, 0.05851977851491499, 0.031998085933871434, 0.05204296505716398, 0.13140089065574007, 0.04020631954896852, 0.04456036631356189, 0.1748325213256962, 0.12309436505219276, 0.06708055406722409, 0.055262165443718696, 0.04069936015024083, 0.046037847210076414, 0.07541729003873032, 0.12439096961332591, 0.17408347157245618, 0.06095200547348753, 0.10035867580765999, 0.09921260627103025, 0.052644282604229806, 0.031548376445374726, 0.1118184479326652, 0.1923234113936931, 0.10133732131721794, 0.05749943192396975, 0.07025569904172572, 0.11564751862749373, 0.17856491609224204, 0.07656257416263729, 0.13060246674618162, 0.0739399056789369, 0.13237214042467876, 0.10029516809597461, 0.0995541259205252, 0.11620225488083097, 0.09217750630888283, 0.046659485063848156, 0.037841078438245634, 0.05762917217189309, 0.050436816376237745, 0.09302712949289334, 0.09720246137281496, 0.08937987316339689, 0.04971428028547574, 0.08721861595148842, 0.07897852595780783, 0.10447647309321038, 0.024460100196844376, 0.0894258834500522, 0.06195065208201195, 0.04719462904878075, 0.06798769758460363, 0.07747326761898603, 0.026435402705509065, 0.04388060358057048, 0.05443400817247819, 0.11482685252117354, 0.07225461866344006, 0.11965820120120375, 0.044094341059606995, 0.025399991309708773, 0.057924115014919265, 0.04931424271225527, 0.05706974679893011, 0.08520393360538962, 0.07979900602952777, 0.10680668016254219, 0.04944758180423999, 0.03303942806987052, 0.05734085759672736, 0.02840693301399974, 0.04946406551000396, 0.06749497704190333, 0.08392807142397743, 0.055079719939920405, 0.04626529555486337, 0.10458552025603828, 0.03106257403710371, 0.08929964821328745, 0.07324586111233568, 0.13746004663956513, 0.10150938472141452, 0.18579679182048528, 0.13621287830285025, 0.09027500777910465, 0.059944878286759005, 0.018704786203421066, 0.10140139974306768, 0.03695773817803946, 0.11499137128470886, 0.09208480885907898, 0.13043697610980795, 0.04696139096366278, 0.07282639054919435, 0.02680490846522517, 0.13206605785081452, 0.046758740985503686, 0.08795830388135902, 0.04979339239610108, 0.043067748386082495, 0.08417427116773675, 0.10303177130319471, 0.05420716804711181, 0.0942394189880202, 0.07838271654792126, 0.03870331371357267, 0.10194386998239559, 0.06142517635219062, 0.07906255093408003, 0.052690405252082385, 0.04200809057905169, 0.06470499344009639, 0.10565383683356115, 0.05117017136247587, 0.055108480333388006, 0.03853878244040289, 0.06219341539564161, 0.07719093393728366, 0.07551766827050847, 0.04633648797214618, 0.051869391434837875, 0.0767126475013756, 0.131202105183238, 0.12591089954403775, 0.06438849334937824, 0.054905217784830894, 0.05900671148335185, 0.1103109572801398, 0.0850998156003352, 0.07624553392985936, 0.06959951603256308, 0.08797225119880073, 0.06379732713171603, 0.11942146514696693, 0.07256023151451844, 0.15454171720626997, 0.11862195875442097, 0.04216373104527442, 0.09468208887783326, 0.056432430977882225, 0.13938167751198902, 0.13173208467003733, 0.07546686276004101, 0.059059893556596584, 0.07606629589262286, 0.1356378359710402, 0.1071890499743659, 0.07457869771081795, 0.087970992056041, 0.055743020980417786, 0.13432374129516805, 0.1258944104734943, 0.05941569466338994, 0.11610370552010474, 0.04559629310245301, 0.09985403025127516, 0.05753073630498323, 0.049127546292917865, 0.06250052521461652, 0.04912431895655674, 0.050699693311972785, 0.19637684808225292, 0.06574575959547756, 0.09144462834143968, 0.053112285632112226, 0.054011747527039425, 0.0718770596078227, 0.06314503335247762, 0.08697943624833643, 0.07556906796331886, 0.14509594907532997, 0.11612525685303633, 0.04803794043667973, 0.03430172419530486, 0.09200761944086593, 0.09409094387746761, 0.08119723924108722, 0.056116708644404094, 0.06145633683955061, 0.12058841679570581, 0.06531264091330698, 0.049689065857324244, 0.12211661323963524, 0.0714534954600556, 0.07577046819465934]
0.09159509500143252
Making ranges
torch.Size([43819, 2])
We keep 7.58e+06/6.78e+08 =  1% of the original kernel matrix.

torch.Size([4273, 2])
We keep 1.52e+05/2.81e+06 =  5% of the original kernel matrix.

torch.Size([15073, 2])
We keep 1.10e+06/4.37e+07 =  2% of the original kernel matrix.

torch.Size([45418, 2])
We keep 1.33e+07/8.39e+08 =  1% of the original kernel matrix.

torch.Size([45903, 2])
We keep 9.23e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([7400, 2])
We keep 3.53e+05/9.28e+06 =  3% of the original kernel matrix.

torch.Size([19001, 2])
We keep 1.65e+06/7.93e+07 =  2% of the original kernel matrix.

torch.Size([31846, 2])
We keep 1.47e+07/5.42e+08 =  2% of the original kernel matrix.

torch.Size([38629, 2])
We keep 7.81e+06/6.06e+08 =  1% of the original kernel matrix.

torch.Size([77584, 2])
We keep 2.27e+07/1.91e+09 =  1% of the original kernel matrix.

torch.Size([58831, 2])
We keep 1.25e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([131015, 2])
We keep 6.61e+07/6.36e+09 =  1% of the original kernel matrix.

torch.Size([75801, 2])
We keep 2.13e+07/2.08e+09 =  1% of the original kernel matrix.

torch.Size([106143, 2])
We keep 4.29e+07/3.71e+09 =  1% of the original kernel matrix.

torch.Size([68006, 2])
We keep 1.68e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([100386, 2])
We keep 4.28e+07/3.63e+09 =  1% of the original kernel matrix.

torch.Size([66106, 2])
We keep 1.67e+07/1.57e+09 =  1% of the original kernel matrix.

torch.Size([32585, 2])
We keep 7.53e+06/3.89e+08 =  1% of the original kernel matrix.

torch.Size([39022, 2])
We keep 6.75e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([51030, 2])
We keep 5.48e+07/1.29e+09 =  4% of the original kernel matrix.

torch.Size([47956, 2])
We keep 1.07e+07/9.37e+08 =  1% of the original kernel matrix.

torch.Size([21224, 2])
We keep 2.98e+06/1.32e+08 =  2% of the original kernel matrix.

torch.Size([30927, 2])
We keep 4.38e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([175281, 2])
We keep 1.67e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([88064, 2])
We keep 2.76e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([23216, 2])
We keep 2.60e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([32641, 2])
We keep 4.44e+06/3.08e+08 =  1% of the original kernel matrix.

torch.Size([794321, 2])
We keep 1.78e+09/2.31e+11 =  0% of the original kernel matrix.

torch.Size([196803, 2])
We keep 1.04e+08/1.25e+10 =  0% of the original kernel matrix.

torch.Size([59890, 2])
We keep 1.61e+07/1.29e+09 =  1% of the original kernel matrix.

torch.Size([52024, 2])
We keep 1.08e+07/9.35e+08 =  1% of the original kernel matrix.

torch.Size([108913, 2])
We keep 5.34e+07/4.20e+09 =  1% of the original kernel matrix.

torch.Size([68701, 2])
We keep 1.79e+07/1.69e+09 =  1% of the original kernel matrix.

torch.Size([129104, 2])
We keep 7.78e+07/6.15e+09 =  1% of the original kernel matrix.

torch.Size([75770, 2])
We keep 2.10e+07/2.04e+09 =  1% of the original kernel matrix.

torch.Size([41401, 2])
We keep 9.12e+06/6.16e+08 =  1% of the original kernel matrix.

torch.Size([43458, 2])
We keep 7.96e+06/6.46e+08 =  1% of the original kernel matrix.

torch.Size([285924, 2])
We keep 3.61e+08/3.29e+10 =  1% of the original kernel matrix.

torch.Size([116297, 2])
We keep 4.30e+07/4.72e+09 =  0% of the original kernel matrix.

torch.Size([165932, 2])
We keep 1.35e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([87121, 2])
We keep 2.66e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([137517, 2])
We keep 1.19e+08/8.49e+09 =  1% of the original kernel matrix.

torch.Size([77236, 2])
We keep 2.41e+07/2.40e+09 =  1% of the original kernel matrix.

torch.Size([531553, 2])
We keep 7.32e+08/1.04e+11 =  0% of the original kernel matrix.

torch.Size([159515, 2])
We keep 7.13e+07/8.40e+09 =  0% of the original kernel matrix.

torch.Size([28925, 2])
We keep 4.46e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([36859, 2])
We keep 5.60e+06/4.13e+08 =  1% of the original kernel matrix.

torch.Size([85388, 2])
We keep 8.94e+07/3.27e+09 =  2% of the original kernel matrix.

torch.Size([61542, 2])
We keep 1.63e+07/1.49e+09 =  1% of the original kernel matrix.

torch.Size([8075, 2])
We keep 7.91e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([19353, 2])
We keep 1.82e+06/9.88e+07 =  1% of the original kernel matrix.

torch.Size([148173, 2])
We keep 7.28e+07/7.90e+09 =  0% of the original kernel matrix.

torch.Size([81964, 2])
We keep 2.33e+07/2.31e+09 =  1% of the original kernel matrix.

torch.Size([112637, 2])
We keep 6.27e+07/5.19e+09 =  1% of the original kernel matrix.

torch.Size([70593, 2])
We keep 1.97e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([28941, 2])
We keep 4.35e+06/2.65e+08 =  1% of the original kernel matrix.

torch.Size([36830, 2])
We keep 5.72e+06/4.24e+08 =  1% of the original kernel matrix.

torch.Size([102120, 2])
We keep 2.16e+08/7.35e+09 =  2% of the original kernel matrix.

torch.Size([66808, 2])
We keep 2.24e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([3550288, 2])
We keep 1.54e+10/3.32e+12 =  0% of the original kernel matrix.

torch.Size([433354, 2])
We keep 3.53e+08/4.74e+10 =  0% of the original kernel matrix.

torch.Size([38115, 2])
We keep 1.05e+07/5.46e+08 =  1% of the original kernel matrix.

torch.Size([40655, 2])
We keep 7.45e+06/6.08e+08 =  1% of the original kernel matrix.

torch.Size([1638575, 2])
We keep 5.79e+09/9.17e+11 =  0% of the original kernel matrix.

torch.Size([291504, 2])
We keep 1.95e+08/2.49e+10 =  0% of the original kernel matrix.

torch.Size([54433, 2])
We keep 3.31e+07/1.38e+09 =  2% of the original kernel matrix.

torch.Size([49879, 2])
We keep 1.14e+07/9.66e+08 =  1% of the original kernel matrix.

torch.Size([31019, 2])
We keep 6.62e+06/3.22e+08 =  2% of the original kernel matrix.

torch.Size([38170, 2])
We keep 6.26e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([78925, 2])
We keep 3.81e+07/2.68e+09 =  1% of the original kernel matrix.

torch.Size([58645, 2])
We keep 1.48e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([399625, 2])
We keep 9.93e+08/7.88e+10 =  1% of the original kernel matrix.

torch.Size([136364, 2])
We keep 6.40e+07/7.31e+09 =  0% of the original kernel matrix.

torch.Size([182734, 2])
We keep 1.52e+08/1.30e+10 =  1% of the original kernel matrix.

torch.Size([92496, 2])
We keep 2.91e+07/2.97e+09 =  0% of the original kernel matrix.

torch.Size([4534, 2])
We keep 1.59e+05/3.09e+06 =  5% of the original kernel matrix.

torch.Size([15495, 2])
We keep 1.14e+06/4.58e+07 =  2% of the original kernel matrix.

torch.Size([10284, 2])
We keep 7.77e+05/2.11e+07 =  3% of the original kernel matrix.

torch.Size([21515, 2])
We keep 2.25e+06/1.20e+08 =  1% of the original kernel matrix.

torch.Size([8831, 2])
We keep 5.27e+05/1.35e+07 =  3% of the original kernel matrix.

torch.Size([20206, 2])
We keep 1.89e+06/9.55e+07 =  1% of the original kernel matrix.

torch.Size([11577, 2])
We keep 7.87e+05/2.55e+07 =  3% of the original kernel matrix.

torch.Size([22900, 2])
We keep 2.35e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([3964, 2])
We keep 2.93e+05/4.27e+06 =  6% of the original kernel matrix.

torch.Size([13635, 2])
We keep 1.28e+06/5.38e+07 =  2% of the original kernel matrix.

torch.Size([2422, 2])
We keep 4.88e+04/7.48e+05 =  6% of the original kernel matrix.

torch.Size([12397, 2])
We keep 7.16e+05/2.25e+07 =  3% of the original kernel matrix.

torch.Size([11173, 2])
We keep 9.54e+05/2.71e+07 =  3% of the original kernel matrix.

torch.Size([22067, 2])
We keep 2.45e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([4103, 2])
We keep 1.27e+05/2.47e+06 =  5% of the original kernel matrix.

torch.Size([15079, 2])
We keep 1.05e+06/4.09e+07 =  2% of the original kernel matrix.

torch.Size([3085, 2])
We keep 7.57e+04/1.28e+06 =  5% of the original kernel matrix.

torch.Size([13371, 2])
We keep 8.46e+05/2.95e+07 =  2% of the original kernel matrix.

torch.Size([5275, 2])
We keep 2.37e+05/4.90e+06 =  4% of the original kernel matrix.

torch.Size([16303, 2])
We keep 1.32e+06/5.76e+07 =  2% of the original kernel matrix.

torch.Size([13708, 2])
We keep 1.08e+06/3.89e+07 =  2% of the original kernel matrix.

torch.Size([24574, 2])
We keep 2.75e+06/1.62e+08 =  1% of the original kernel matrix.

torch.Size([14043, 2])
We keep 1.38e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([24992, 2])
We keep 2.89e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([25925, 2])
We keep 5.15e+06/2.17e+08 =  2% of the original kernel matrix.

torch.Size([34195, 2])
We keep 5.32e+06/3.84e+08 =  1% of the original kernel matrix.

torch.Size([35188, 2])
We keep 6.40e+06/4.09e+08 =  1% of the original kernel matrix.

torch.Size([40705, 2])
We keep 6.86e+06/5.26e+08 =  1% of the original kernel matrix.

torch.Size([4218, 2])
We keep 1.58e+05/2.90e+06 =  5% of the original kernel matrix.

torch.Size([14853, 2])
We keep 1.12e+06/4.43e+07 =  2% of the original kernel matrix.

torch.Size([11680, 2])
We keep 1.12e+06/3.09e+07 =  3% of the original kernel matrix.

torch.Size([22894, 2])
We keep 2.58e+06/1.45e+08 =  1% of the original kernel matrix.

torch.Size([8487, 2])
We keep 5.09e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([19805, 2])
We keep 1.86e+06/9.31e+07 =  1% of the original kernel matrix.

torch.Size([14531, 2])
We keep 1.43e+06/4.72e+07 =  3% of the original kernel matrix.

torch.Size([25300, 2])
We keep 3.00e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([20005, 2])
We keep 3.16e+06/1.23e+08 =  2% of the original kernel matrix.

torch.Size([29823, 2])
We keep 4.31e+06/2.89e+08 =  1% of the original kernel matrix.

torch.Size([5676, 2])
We keep 2.45e+05/5.48e+06 =  4% of the original kernel matrix.

torch.Size([16856, 2])
We keep 1.38e+06/6.10e+07 =  2% of the original kernel matrix.

torch.Size([19481, 2])
We keep 2.27e+06/9.94e+07 =  2% of the original kernel matrix.

torch.Size([29576, 2])
We keep 3.93e+06/2.60e+08 =  1% of the original kernel matrix.

torch.Size([8404, 2])
We keep 4.71e+05/1.28e+07 =  3% of the original kernel matrix.

torch.Size([19971, 2])
We keep 1.85e+06/9.30e+07 =  1% of the original kernel matrix.

torch.Size([9589, 2])
We keep 8.17e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([20622, 2])
We keep 2.21e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([18207, 2])
We keep 1.83e+06/7.80e+07 =  2% of the original kernel matrix.

torch.Size([28642, 2])
We keep 3.59e+06/2.30e+08 =  1% of the original kernel matrix.

torch.Size([4495, 2])
We keep 1.66e+05/3.22e+06 =  5% of the original kernel matrix.

torch.Size([15406, 2])
We keep 1.15e+06/4.67e+07 =  2% of the original kernel matrix.

torch.Size([15257, 2])
We keep 1.90e+06/6.19e+07 =  3% of the original kernel matrix.

torch.Size([26060, 2])
We keep 3.37e+06/2.05e+08 =  1% of the original kernel matrix.

torch.Size([31263, 2])
We keep 4.94e+06/3.16e+08 =  1% of the original kernel matrix.

torch.Size([38438, 2])
We keep 6.17e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([8954, 2])
We keep 5.07e+05/1.45e+07 =  3% of the original kernel matrix.

torch.Size([20388, 2])
We keep 1.93e+06/9.91e+07 =  1% of the original kernel matrix.

torch.Size([7554, 2])
We keep 3.56e+05/9.10e+06 =  3% of the original kernel matrix.

torch.Size([19059, 2])
We keep 1.64e+06/7.85e+07 =  2% of the original kernel matrix.

torch.Size([7545, 2])
We keep 4.24e+05/1.03e+07 =  4% of the original kernel matrix.

torch.Size([18950, 2])
We keep 1.72e+06/8.34e+07 =  2% of the original kernel matrix.

torch.Size([12354, 2])
We keep 8.83e+05/2.99e+07 =  2% of the original kernel matrix.

torch.Size([23403, 2])
We keep 2.51e+06/1.42e+08 =  1% of the original kernel matrix.

torch.Size([19827, 2])
We keep 2.62e+06/9.31e+07 =  2% of the original kernel matrix.

torch.Size([29875, 2])
We keep 3.86e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([14389, 2])
We keep 1.32e+06/4.65e+07 =  2% of the original kernel matrix.

torch.Size([25242, 2])
We keep 2.95e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([22099, 2])
We keep 4.46e+06/1.61e+08 =  2% of the original kernel matrix.

torch.Size([31299, 2])
We keep 4.70e+06/3.31e+08 =  1% of the original kernel matrix.

torch.Size([15645, 2])
We keep 1.45e+06/5.24e+07 =  2% of the original kernel matrix.

torch.Size([26314, 2])
We keep 3.12e+06/1.88e+08 =  1% of the original kernel matrix.

torch.Size([10278, 2])
We keep 6.19e+05/1.86e+07 =  3% of the original kernel matrix.

torch.Size([21688, 2])
We keep 2.11e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([11526, 2])
We keep 9.65e+05/2.79e+07 =  3% of the original kernel matrix.

torch.Size([22420, 2])
We keep 2.45e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([11915, 2])
We keep 1.06e+06/3.25e+07 =  3% of the original kernel matrix.

torch.Size([22761, 2])
We keep 2.59e+06/1.48e+08 =  1% of the original kernel matrix.

torch.Size([16707, 2])
We keep 2.42e+06/8.48e+07 =  2% of the original kernel matrix.

torch.Size([27034, 2])
We keep 3.71e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([2021, 2])
We keep 3.07e+04/4.01e+05 =  7% of the original kernel matrix.

torch.Size([11595, 2])
We keep 5.81e+05/1.65e+07 =  3% of the original kernel matrix.

torch.Size([3931, 2])
We keep 1.27e+05/2.30e+06 =  5% of the original kernel matrix.

torch.Size([14766, 2])
We keep 1.04e+06/3.94e+07 =  2% of the original kernel matrix.

torch.Size([14984, 2])
We keep 2.05e+06/4.90e+07 =  4% of the original kernel matrix.

torch.Size([25809, 2])
We keep 2.97e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([38641, 2])
We keep 7.74e+06/4.96e+08 =  1% of the original kernel matrix.

torch.Size([41565, 2])
We keep 7.22e+06/5.80e+08 =  1% of the original kernel matrix.

torch.Size([7056, 2])
We keep 3.64e+05/8.63e+06 =  4% of the original kernel matrix.

torch.Size([18310, 2])
We keep 1.62e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([7163, 2])
We keep 4.93e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([18076, 2])
We keep 1.74e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([7736, 2])
We keep 5.65e+05/1.23e+07 =  4% of the original kernel matrix.

torch.Size([19079, 2])
We keep 1.85e+06/9.15e+07 =  2% of the original kernel matrix.

torch.Size([7268, 2])
We keep 3.79e+05/9.17e+06 =  4% of the original kernel matrix.

torch.Size([18638, 2])
We keep 1.66e+06/7.88e+07 =  2% of the original kernel matrix.

torch.Size([4419, 2])
We keep 1.69e+05/3.21e+06 =  5% of the original kernel matrix.

torch.Size([15125, 2])
We keep 1.16e+06/4.66e+07 =  2% of the original kernel matrix.

torch.Size([4966, 2])
We keep 1.74e+05/3.78e+06 =  4% of the original kernel matrix.

torch.Size([16174, 2])
We keep 1.21e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([6104, 2])
We keep 3.65e+05/6.97e+06 =  5% of the original kernel matrix.

torch.Size([17193, 2])
We keep 1.51e+06/6.88e+07 =  2% of the original kernel matrix.

torch.Size([3302, 2])
We keep 8.26e+04/1.44e+06 =  5% of the original kernel matrix.

torch.Size([13900, 2])
We keep 8.84e+05/3.13e+07 =  2% of the original kernel matrix.

torch.Size([11133, 2])
We keep 7.80e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([22345, 2])
We keep 2.32e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([9991, 2])
We keep 7.18e+05/1.94e+07 =  3% of the original kernel matrix.

torch.Size([21335, 2])
We keep 2.16e+06/1.15e+08 =  1% of the original kernel matrix.

torch.Size([18750, 2])
We keep 1.81e+06/7.76e+07 =  2% of the original kernel matrix.

torch.Size([28884, 2])
We keep 3.58e+06/2.29e+08 =  1% of the original kernel matrix.

torch.Size([5646, 2])
We keep 2.38e+05/5.23e+06 =  4% of the original kernel matrix.

torch.Size([16752, 2])
We keep 1.36e+06/5.95e+07 =  2% of the original kernel matrix.

torch.Size([7319, 2])
We keep 4.07e+05/9.41e+06 =  4% of the original kernel matrix.

torch.Size([18782, 2])
We keep 1.69e+06/7.99e+07 =  2% of the original kernel matrix.

torch.Size([13999, 2])
We keep 1.43e+06/4.67e+07 =  3% of the original kernel matrix.

torch.Size([24947, 2])
We keep 2.98e+06/1.78e+08 =  1% of the original kernel matrix.

torch.Size([15184, 2])
We keep 1.54e+06/5.35e+07 =  2% of the original kernel matrix.

torch.Size([25918, 2])
We keep 3.13e+06/1.90e+08 =  1% of the original kernel matrix.

torch.Size([12797, 2])
We keep 1.03e+06/3.34e+07 =  3% of the original kernel matrix.

torch.Size([23909, 2])
We keep 2.60e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([8431, 2])
We keep 5.55e+05/1.51e+07 =  3% of the original kernel matrix.

torch.Size([19885, 2])
We keep 2.01e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([16924, 2])
We keep 2.85e+06/9.43e+07 =  3% of the original kernel matrix.

torch.Size([27284, 2])
We keep 3.89e+06/2.53e+08 =  1% of the original kernel matrix.

torch.Size([27602, 2])
We keep 4.11e+06/2.33e+08 =  1% of the original kernel matrix.

torch.Size([35651, 2])
We keep 5.46e+06/3.97e+08 =  1% of the original kernel matrix.

torch.Size([15041, 2])
We keep 1.29e+06/4.85e+07 =  2% of the original kernel matrix.

torch.Size([25841, 2])
We keep 2.99e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([19884, 2])
We keep 2.51e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([29811, 2])
We keep 4.03e+06/2.69e+08 =  1% of the original kernel matrix.

torch.Size([16804, 2])
We keep 1.51e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([27252, 2])
We keep 3.27e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([11288, 2])
We keep 8.50e+05/2.54e+07 =  3% of the original kernel matrix.

torch.Size([22522, 2])
We keep 2.36e+06/1.31e+08 =  1% of the original kernel matrix.

torch.Size([4354, 2])
We keep 2.09e+05/3.75e+06 =  5% of the original kernel matrix.

torch.Size([14714, 2])
We keep 1.22e+06/5.04e+07 =  2% of the original kernel matrix.

torch.Size([20770, 2])
We keep 3.10e+06/1.33e+08 =  2% of the original kernel matrix.

torch.Size([30494, 2])
We keep 4.40e+06/3.00e+08 =  1% of the original kernel matrix.

torch.Size([5303, 2])
We keep 2.09e+05/4.39e+06 =  4% of the original kernel matrix.

torch.Size([16497, 2])
We keep 1.29e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([11277, 2])
We keep 7.14e+05/2.23e+07 =  3% of the original kernel matrix.

torch.Size([22523, 2])
We keep 2.25e+06/1.23e+08 =  1% of the original kernel matrix.

torch.Size([2981, 2])
We keep 8.86e+04/1.40e+06 =  6% of the original kernel matrix.

torch.Size([13268, 2])
We keep 8.77e+05/3.08e+07 =  2% of the original kernel matrix.

torch.Size([6292, 2])
We keep 3.61e+05/7.18e+06 =  5% of the original kernel matrix.

torch.Size([17460, 2])
We keep 1.53e+06/6.98e+07 =  2% of the original kernel matrix.

torch.Size([3778, 2])
We keep 1.30e+05/2.09e+06 =  6% of the original kernel matrix.

torch.Size([14434, 2])
We keep 1.01e+06/3.76e+07 =  2% of the original kernel matrix.

torch.Size([8069, 2])
We keep 5.08e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([19476, 2])
We keep 1.84e+06/9.19e+07 =  1% of the original kernel matrix.

torch.Size([7412, 2])
We keep 7.81e+05/1.44e+07 =  5% of the original kernel matrix.

torch.Size([18122, 2])
We keep 1.93e+06/9.87e+07 =  1% of the original kernel matrix.

torch.Size([8551, 2])
We keep 5.54e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([19798, 2])
We keep 1.92e+06/9.78e+07 =  1% of the original kernel matrix.

torch.Size([9328, 2])
We keep 6.19e+05/1.63e+07 =  3% of the original kernel matrix.

torch.Size([20860, 2])
We keep 2.02e+06/1.05e+08 =  1% of the original kernel matrix.

torch.Size([13896, 2])
We keep 1.33e+06/4.15e+07 =  3% of the original kernel matrix.

torch.Size([24751, 2])
We keep 2.84e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([9389, 2])
We keep 7.86e+05/2.02e+07 =  3% of the original kernel matrix.

torch.Size([20373, 2])
We keep 2.20e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([11100, 2])
We keep 9.05e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([22451, 2])
We keep 2.34e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([2500, 2])
We keep 5.98e+04/8.03e+05 =  7% of the original kernel matrix.

torch.Size([12212, 2])
We keep 7.17e+05/2.33e+07 =  3% of the original kernel matrix.

torch.Size([20329, 2])
We keep 2.87e+06/1.17e+08 =  2% of the original kernel matrix.

torch.Size([30320, 2])
We keep 4.26e+06/2.82e+08 =  1% of the original kernel matrix.

torch.Size([3179, 2])
We keep 8.50e+04/1.48e+06 =  5% of the original kernel matrix.

torch.Size([13453, 2])
We keep 8.86e+05/3.16e+07 =  2% of the original kernel matrix.

torch.Size([25003, 2])
We keep 3.32e+06/1.79e+08 =  1% of the original kernel matrix.

torch.Size([33921, 2])
We keep 4.92e+06/3.48e+08 =  1% of the original kernel matrix.

torch.Size([5443, 2])
We keep 2.51e+05/4.81e+06 =  5% of the original kernel matrix.

torch.Size([16617, 2])
We keep 1.33e+06/5.71e+07 =  2% of the original kernel matrix.

torch.Size([6404, 2])
We keep 3.50e+05/8.28e+06 =  4% of the original kernel matrix.

torch.Size([17230, 2])
We keep 1.60e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([8501, 2])
We keep 4.44e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([20195, 2])
We keep 1.80e+06/9.09e+07 =  1% of the original kernel matrix.

torch.Size([7761, 2])
We keep 4.54e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([19154, 2])
We keep 1.73e+06/8.42e+07 =  2% of the original kernel matrix.

torch.Size([8185, 2])
We keep 4.66e+05/1.22e+07 =  3% of the original kernel matrix.

torch.Size([19486, 2])
We keep 1.83e+06/9.08e+07 =  2% of the original kernel matrix.

torch.Size([4419, 2])
We keep 1.46e+05/2.69e+06 =  5% of the original kernel matrix.

torch.Size([15373, 2])
We keep 1.08e+06/4.27e+07 =  2% of the original kernel matrix.

torch.Size([16729, 2])
We keep 2.27e+06/6.34e+07 =  3% of the original kernel matrix.

torch.Size([27340, 2])
We keep 3.33e+06/2.07e+08 =  1% of the original kernel matrix.

torch.Size([21251, 2])
We keep 2.98e+06/1.29e+08 =  2% of the original kernel matrix.

torch.Size([30916, 2])
We keep 4.36e+06/2.95e+08 =  1% of the original kernel matrix.

torch.Size([7341, 2])
We keep 4.23e+05/9.37e+06 =  4% of the original kernel matrix.

torch.Size([18757, 2])
We keep 1.65e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([37289, 2])
We keep 6.27e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([42088, 2])
We keep 6.96e+06/5.41e+08 =  1% of the original kernel matrix.

torch.Size([5457, 2])
We keep 2.47e+05/5.25e+06 =  4% of the original kernel matrix.

torch.Size([16553, 2])
We keep 1.35e+06/5.96e+07 =  2% of the original kernel matrix.

torch.Size([12019, 2])
We keep 1.06e+06/3.19e+07 =  3% of the original kernel matrix.

torch.Size([23343, 2])
We keep 2.59e+06/1.47e+08 =  1% of the original kernel matrix.

torch.Size([3963, 2])
We keep 1.73e+05/3.04e+06 =  5% of the original kernel matrix.

torch.Size([14181, 2])
We keep 1.14e+06/4.54e+07 =  2% of the original kernel matrix.

torch.Size([7553, 2])
We keep 4.00e+05/1.01e+07 =  3% of the original kernel matrix.

torch.Size([18829, 2])
We keep 1.71e+06/8.27e+07 =  2% of the original kernel matrix.

torch.Size([6457, 2])
We keep 2.91e+05/6.79e+06 =  4% of the original kernel matrix.

torch.Size([17971, 2])
We keep 1.48e+06/6.78e+07 =  2% of the original kernel matrix.

torch.Size([7746, 2])
We keep 5.13e+05/1.17e+07 =  4% of the original kernel matrix.

torch.Size([19075, 2])
We keep 1.81e+06/8.90e+07 =  2% of the original kernel matrix.

torch.Size([6727, 2])
We keep 3.28e+05/7.83e+06 =  4% of the original kernel matrix.

torch.Size([18149, 2])
We keep 1.57e+06/7.29e+07 =  2% of the original kernel matrix.

torch.Size([7173, 2])
We keep 3.29e+05/8.35e+06 =  3% of the original kernel matrix.

torch.Size([18658, 2])
We keep 1.60e+06/7.52e+07 =  2% of the original kernel matrix.

torch.Size([5736, 2])
We keep 2.15e+05/5.05e+06 =  4% of the original kernel matrix.

torch.Size([17009, 2])
We keep 1.34e+06/5.85e+07 =  2% of the original kernel matrix.

torch.Size([8637, 2])
We keep 5.23e+05/1.41e+07 =  3% of the original kernel matrix.

torch.Size([19943, 2])
We keep 1.93e+06/9.78e+07 =  1% of the original kernel matrix.

torch.Size([3557, 2])
We keep 1.12e+05/1.80e+06 =  6% of the original kernel matrix.

torch.Size([14223, 2])
We keep 9.51e+05/3.49e+07 =  2% of the original kernel matrix.

torch.Size([20521, 2])
We keep 2.67e+06/1.16e+08 =  2% of the original kernel matrix.

torch.Size([30346, 2])
We keep 4.22e+06/2.81e+08 =  1% of the original kernel matrix.

torch.Size([19007, 2])
We keep 2.52e+06/1.04e+08 =  2% of the original kernel matrix.

torch.Size([29104, 2])
We keep 4.04e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([12712, 2])
We keep 1.43e+06/4.29e+07 =  3% of the original kernel matrix.

torch.Size([23623, 2])
We keep 2.94e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([7238, 2])
We keep 3.35e+05/8.27e+06 =  4% of the original kernel matrix.

torch.Size([18710, 2])
We keep 1.60e+06/7.49e+07 =  2% of the original kernel matrix.

torch.Size([7759, 2])
We keep 4.86e+05/1.05e+07 =  4% of the original kernel matrix.

torch.Size([19237, 2])
We keep 1.74e+06/8.45e+07 =  2% of the original kernel matrix.

torch.Size([3941, 2])
We keep 1.44e+05/2.51e+06 =  5% of the original kernel matrix.

torch.Size([14743, 2])
We keep 1.08e+06/4.12e+07 =  2% of the original kernel matrix.

torch.Size([6097, 2])
We keep 2.48e+05/5.88e+06 =  4% of the original kernel matrix.

torch.Size([17489, 2])
We keep 1.42e+06/6.31e+07 =  2% of the original kernel matrix.

torch.Size([7437, 2])
We keep 5.65e+05/1.15e+07 =  4% of the original kernel matrix.

torch.Size([18566, 2])
We keep 1.79e+06/8.81e+07 =  2% of the original kernel matrix.

torch.Size([9077, 2])
We keep 7.41e+05/1.67e+07 =  4% of the original kernel matrix.

torch.Size([20460, 2])
We keep 2.04e+06/1.07e+08 =  1% of the original kernel matrix.

torch.Size([9695, 2])
We keep 6.71e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([21018, 2])
We keep 2.10e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([10104, 2])
We keep 6.32e+05/1.81e+07 =  3% of the original kernel matrix.

torch.Size([21544, 2])
We keep 2.10e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([14983, 2])
We keep 2.91e+06/8.53e+07 =  3% of the original kernel matrix.

torch.Size([25352, 2])
We keep 3.75e+06/2.40e+08 =  1% of the original kernel matrix.

torch.Size([12178, 2])
We keep 1.05e+06/3.27e+07 =  3% of the original kernel matrix.

torch.Size([23189, 2])
We keep 2.62e+06/1.49e+08 =  1% of the original kernel matrix.

torch.Size([34348, 2])
We keep 1.00e+07/5.28e+08 =  1% of the original kernel matrix.

torch.Size([39728, 2])
We keep 7.73e+06/5.98e+08 =  1% of the original kernel matrix.

torch.Size([11529, 2])
We keep 7.66e+05/2.38e+07 =  3% of the original kernel matrix.

torch.Size([22786, 2])
We keep 2.31e+06/1.27e+08 =  1% of the original kernel matrix.

torch.Size([12737, 2])
We keep 1.33e+06/3.60e+07 =  3% of the original kernel matrix.

torch.Size([23914, 2])
We keep 2.71e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([6863, 2])
We keep 3.59e+05/8.13e+06 =  4% of the original kernel matrix.

torch.Size([18232, 2])
We keep 1.60e+06/7.42e+07 =  2% of the original kernel matrix.

torch.Size([2181, 2])
We keep 4.76e+04/6.46e+05 =  7% of the original kernel matrix.

torch.Size([11777, 2])
We keep 6.82e+05/2.09e+07 =  3% of the original kernel matrix.

torch.Size([28924, 2])
We keep 5.54e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([36735, 2])
We keep 5.90e+06/4.31e+08 =  1% of the original kernel matrix.

torch.Size([4898, 2])
We keep 1.67e+05/3.29e+06 =  5% of the original kernel matrix.

torch.Size([15849, 2])
We keep 1.16e+06/4.72e+07 =  2% of the original kernel matrix.

torch.Size([16878, 2])
We keep 3.33e+06/8.14e+07 =  4% of the original kernel matrix.

torch.Size([27367, 2])
We keep 3.62e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([2947, 2])
We keep 9.66e+04/1.39e+06 =  6% of the original kernel matrix.

torch.Size([13091, 2])
We keep 8.81e+05/3.06e+07 =  2% of the original kernel matrix.

torch.Size([14313, 2])
We keep 1.72e+06/5.74e+07 =  2% of the original kernel matrix.

torch.Size([24945, 2])
We keep 3.20e+06/1.97e+08 =  1% of the original kernel matrix.

torch.Size([8472, 2])
We keep 4.63e+05/1.27e+07 =  3% of the original kernel matrix.

torch.Size([19963, 2])
We keep 1.84e+06/9.28e+07 =  1% of the original kernel matrix.

torch.Size([5914, 2])
We keep 2.29e+05/5.31e+06 =  4% of the original kernel matrix.

torch.Size([17332, 2])
We keep 1.35e+06/6.00e+07 =  2% of the original kernel matrix.

torch.Size([10848, 2])
We keep 8.84e+05/2.44e+07 =  3% of the original kernel matrix.

torch.Size([21882, 2])
We keep 2.34e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([11352, 2])
We keep 7.74e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([22551, 2])
We keep 2.34e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([2759, 2])
We keep 5.82e+04/8.85e+05 =  6% of the original kernel matrix.

torch.Size([13032, 2])
We keep 7.59e+05/2.45e+07 =  3% of the original kernel matrix.

torch.Size([1864, 2])
We keep 3.64e+04/4.66e+05 =  7% of the original kernel matrix.

torch.Size([11192, 2])
We keep 6.16e+05/1.78e+07 =  3% of the original kernel matrix.

torch.Size([4566, 2])
We keep 1.96e+05/3.26e+06 =  6% of the original kernel matrix.

torch.Size([15350, 2])
We keep 1.15e+06/4.70e+07 =  2% of the original kernel matrix.

torch.Size([19870, 2])
We keep 2.66e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([29910, 2])
We keep 4.08e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([6645, 2])
We keep 2.77e+05/6.68e+06 =  4% of the original kernel matrix.

torch.Size([18202, 2])
We keep 1.47e+06/6.73e+07 =  2% of the original kernel matrix.

torch.Size([11913, 2])
We keep 9.08e+05/2.82e+07 =  3% of the original kernel matrix.

torch.Size([23132, 2])
We keep 2.45e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([13473, 2])
We keep 7.70e+06/5.38e+07 = 14% of the original kernel matrix.

torch.Size([24834, 2])
We keep 3.17e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([13932, 2])
We keep 1.37e+06/4.48e+07 =  3% of the original kernel matrix.

torch.Size([24787, 2])
We keep 2.94e+06/1.74e+08 =  1% of the original kernel matrix.

torch.Size([6753, 2])
We keep 3.24e+05/7.65e+06 =  4% of the original kernel matrix.

torch.Size([17981, 2])
We keep 1.54e+06/7.20e+07 =  2% of the original kernel matrix.

torch.Size([12914, 2])
We keep 1.21e+06/3.66e+07 =  3% of the original kernel matrix.

torch.Size([23951, 2])
We keep 2.73e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([7377, 2])
We keep 3.48e+05/8.80e+06 =  3% of the original kernel matrix.

torch.Size([18788, 2])
We keep 1.63e+06/7.72e+07 =  2% of the original kernel matrix.

torch.Size([23953, 2])
We keep 3.07e+06/1.56e+08 =  1% of the original kernel matrix.

torch.Size([33197, 2])
We keep 4.69e+06/3.25e+08 =  1% of the original kernel matrix.

torch.Size([5180, 2])
We keep 2.45e+05/5.17e+06 =  4% of the original kernel matrix.

torch.Size([16143, 2])
We keep 1.37e+06/5.92e+07 =  2% of the original kernel matrix.

torch.Size([14444, 2])
We keep 1.76e+06/5.41e+07 =  3% of the original kernel matrix.

torch.Size([25200, 2])
We keep 3.14e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([7506, 2])
We keep 3.33e+05/8.64e+06 =  3% of the original kernel matrix.

torch.Size([19136, 2])
We keep 1.62e+06/7.65e+07 =  2% of the original kernel matrix.

torch.Size([11997, 2])
We keep 8.65e+05/2.72e+07 =  3% of the original kernel matrix.

torch.Size([23305, 2])
We keep 2.45e+06/1.36e+08 =  1% of the original kernel matrix.

torch.Size([12358, 2])
We keep 8.73e+05/2.88e+07 =  3% of the original kernel matrix.

torch.Size([23669, 2])
We keep 2.48e+06/1.40e+08 =  1% of the original kernel matrix.

torch.Size([7831, 2])
We keep 4.15e+05/1.05e+07 =  3% of the original kernel matrix.

torch.Size([19083, 2])
We keep 1.72e+06/8.43e+07 =  2% of the original kernel matrix.

torch.Size([14665, 2])
We keep 1.77e+06/5.66e+07 =  3% of the original kernel matrix.

torch.Size([25402, 2])
We keep 3.21e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([14724, 2])
We keep 1.50e+06/5.07e+07 =  2% of the original kernel matrix.

torch.Size([25748, 2])
We keep 3.14e+06/1.85e+08 =  1% of the original kernel matrix.

torch.Size([14982, 2])
We keep 1.41e+06/5.45e+07 =  2% of the original kernel matrix.

torch.Size([25528, 2])
We keep 3.12e+06/1.92e+08 =  1% of the original kernel matrix.

torch.Size([10446, 2])
We keep 7.05e+05/2.06e+07 =  3% of the original kernel matrix.

torch.Size([21840, 2])
We keep 2.22e+06/1.18e+08 =  1% of the original kernel matrix.

torch.Size([3062, 2])
We keep 8.27e+04/1.34e+06 =  6% of the original kernel matrix.

torch.Size([13406, 2])
We keep 8.65e+05/3.02e+07 =  2% of the original kernel matrix.

torch.Size([6515, 2])
We keep 2.97e+05/6.90e+06 =  4% of the original kernel matrix.

torch.Size([17867, 2])
We keep 1.50e+06/6.84e+07 =  2% of the original kernel matrix.

torch.Size([7139, 2])
We keep 4.55e+05/9.38e+06 =  4% of the original kernel matrix.

torch.Size([18496, 2])
We keep 1.68e+06/7.97e+07 =  2% of the original kernel matrix.

torch.Size([17963, 2])
We keep 2.29e+06/8.09e+07 =  2% of the original kernel matrix.

torch.Size([28322, 2])
We keep 3.65e+06/2.34e+08 =  1% of the original kernel matrix.

torch.Size([6163, 2])
We keep 3.03e+05/6.71e+06 =  4% of the original kernel matrix.

torch.Size([17267, 2])
We keep 1.48e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([7250, 2])
We keep 4.36e+05/9.82e+06 =  4% of the original kernel matrix.

torch.Size([18661, 2])
We keep 1.70e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([6497, 2])
We keep 2.80e+05/6.84e+06 =  4% of the original kernel matrix.

torch.Size([17888, 2])
We keep 1.48e+06/6.81e+07 =  2% of the original kernel matrix.

torch.Size([13735, 2])
We keep 1.76e+06/4.05e+07 =  4% of the original kernel matrix.

torch.Size([24661, 2])
We keep 2.80e+06/1.66e+08 =  1% of the original kernel matrix.

torch.Size([4438, 2])
We keep 1.47e+05/3.06e+06 =  4% of the original kernel matrix.

torch.Size([15453, 2])
We keep 1.12e+06/4.55e+07 =  2% of the original kernel matrix.

torch.Size([24890, 2])
We keep 3.27e+06/1.72e+08 =  1% of the original kernel matrix.

torch.Size([34071, 2])
We keep 4.85e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([10579, 2])
We keep 6.50e+05/1.93e+07 =  3% of the original kernel matrix.

torch.Size([22022, 2])
We keep 2.14e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([41902, 2])
We keep 8.37e+06/5.92e+08 =  1% of the original kernel matrix.

torch.Size([45404, 2])
We keep 8.01e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([6369, 2])
We keep 3.00e+05/7.03e+06 =  4% of the original kernel matrix.

torch.Size([17655, 2])
We keep 1.51e+06/6.90e+07 =  2% of the original kernel matrix.

torch.Size([15446, 2])
We keep 1.44e+06/5.56e+07 =  2% of the original kernel matrix.

torch.Size([26253, 2])
We keep 3.16e+06/1.94e+08 =  1% of the original kernel matrix.

torch.Size([6018, 2])
We keep 2.46e+05/5.73e+06 =  4% of the original kernel matrix.

torch.Size([17302, 2])
We keep 1.40e+06/6.23e+07 =  2% of the original kernel matrix.

torch.Size([8948, 2])
We keep 8.04e+05/1.83e+07 =  4% of the original kernel matrix.

torch.Size([20082, 2])
We keep 2.14e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([11730, 2])
We keep 8.61e+05/2.76e+07 =  3% of the original kernel matrix.

torch.Size([23023, 2])
We keep 2.46e+06/1.37e+08 =  1% of the original kernel matrix.

torch.Size([5380, 2])
We keep 2.10e+05/4.43e+06 =  4% of the original kernel matrix.

torch.Size([16554, 2])
We keep 1.29e+06/5.48e+07 =  2% of the original kernel matrix.

torch.Size([17436, 2])
We keep 1.59e+06/6.66e+07 =  2% of the original kernel matrix.

torch.Size([27987, 2])
We keep 3.37e+06/2.12e+08 =  1% of the original kernel matrix.

torch.Size([21005, 2])
We keep 2.73e+06/1.19e+08 =  2% of the original kernel matrix.

torch.Size([30740, 2])
We keep 4.21e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([7372, 2])
We keep 4.22e+05/9.46e+06 =  4% of the original kernel matrix.

torch.Size([18817, 2])
We keep 1.68e+06/8.01e+07 =  2% of the original kernel matrix.

torch.Size([8220, 2])
We keep 4.41e+05/1.14e+07 =  3% of the original kernel matrix.

torch.Size([19646, 2])
We keep 1.78e+06/8.78e+07 =  2% of the original kernel matrix.

torch.Size([26523, 2])
We keep 3.62e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([34898, 2])
We keep 5.15e+06/3.70e+08 =  1% of the original kernel matrix.

torch.Size([3035, 2])
We keep 9.45e+04/1.51e+06 =  6% of the original kernel matrix.

torch.Size([13160, 2])
We keep 9.05e+05/3.20e+07 =  2% of the original kernel matrix.

torch.Size([4646, 2])
We keep 1.94e+05/3.36e+06 =  5% of the original kernel matrix.

torch.Size([15492, 2])
We keep 1.18e+06/4.78e+07 =  2% of the original kernel matrix.

torch.Size([2758, 2])
We keep 5.96e+04/9.80e+05 =  6% of the original kernel matrix.

torch.Size([13044, 2])
We keep 7.67e+05/2.58e+07 =  2% of the original kernel matrix.

torch.Size([4879, 2])
We keep 1.85e+05/3.78e+06 =  4% of the original kernel matrix.

torch.Size([15794, 2])
We keep 1.21e+06/5.06e+07 =  2% of the original kernel matrix.

torch.Size([23338, 2])
We keep 4.75e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([32112, 2])
We keep 5.08e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([7591, 2])
We keep 3.43e+05/9.05e+06 =  3% of the original kernel matrix.

torch.Size([19156, 2])
We keep 1.64e+06/7.83e+07 =  2% of the original kernel matrix.

torch.Size([11427, 2])
We keep 1.05e+06/3.14e+07 =  3% of the original kernel matrix.

torch.Size([22503, 2])
We keep 2.59e+06/1.46e+08 =  1% of the original kernel matrix.

torch.Size([3117, 2])
We keep 7.21e+04/1.25e+06 =  5% of the original kernel matrix.

torch.Size([13568, 2])
We keep 8.42e+05/2.91e+07 =  2% of the original kernel matrix.

torch.Size([16317, 2])
We keep 1.84e+06/7.12e+07 =  2% of the original kernel matrix.

torch.Size([26861, 2])
We keep 3.49e+06/2.20e+08 =  1% of the original kernel matrix.

torch.Size([8041, 2])
We keep 6.81e+05/1.47e+07 =  4% of the original kernel matrix.

torch.Size([19247, 2])
We keep 1.97e+06/9.99e+07 =  1% of the original kernel matrix.

torch.Size([5817, 2])
We keep 3.28e+05/6.87e+06 =  4% of the original kernel matrix.

torch.Size([16751, 2])
We keep 1.50e+06/6.82e+07 =  2% of the original kernel matrix.

torch.Size([10713, 2])
We keep 9.18e+05/2.41e+07 =  3% of the original kernel matrix.

torch.Size([21780, 2])
We keep 2.33e+06/1.28e+08 =  1% of the original kernel matrix.

torch.Size([6303, 2])
We keep 2.54e+05/6.05e+06 =  4% of the original kernel matrix.

torch.Size([17691, 2])
We keep 1.43e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([5565, 2])
We keep 2.44e+05/5.36e+06 =  4% of the original kernel matrix.

torch.Size([16711, 2])
We keep 1.38e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([11637, 2])
We keep 9.63e+05/2.80e+07 =  3% of the original kernel matrix.

torch.Size([22593, 2])
We keep 2.45e+06/1.38e+08 =  1% of the original kernel matrix.

torch.Size([4290, 2])
We keep 1.42e+05/2.59e+06 =  5% of the original kernel matrix.

torch.Size([15145, 2])
We keep 1.07e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([8600, 2])
We keep 5.94e+05/1.34e+07 =  4% of the original kernel matrix.

torch.Size([20130, 2])
We keep 1.90e+06/9.53e+07 =  1% of the original kernel matrix.

torch.Size([9231, 2])
We keep 9.56e+05/1.86e+07 =  5% of the original kernel matrix.

torch.Size([20462, 2])
We keep 2.10e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([5878, 2])
We keep 2.74e+05/5.82e+06 =  4% of the original kernel matrix.

torch.Size([17082, 2])
We keep 1.41e+06/6.28e+07 =  2% of the original kernel matrix.

torch.Size([12961, 2])
We keep 1.19e+06/3.82e+07 =  3% of the original kernel matrix.

torch.Size([24268, 2])
We keep 2.80e+06/1.61e+08 =  1% of the original kernel matrix.

torch.Size([6859, 2])
We keep 4.02e+05/8.74e+06 =  4% of the original kernel matrix.

torch.Size([18170, 2])
We keep 1.66e+06/7.70e+07 =  2% of the original kernel matrix.

torch.Size([9081, 2])
We keep 6.41e+05/1.57e+07 =  4% of the original kernel matrix.

torch.Size([20406, 2])
We keep 2.00e+06/1.03e+08 =  1% of the original kernel matrix.

torch.Size([21375, 2])
We keep 3.02e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([31391, 2])
We keep 4.55e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([14598, 2])
We keep 1.35e+06/4.77e+07 =  2% of the original kernel matrix.

torch.Size([25669, 2])
We keep 2.98e+06/1.80e+08 =  1% of the original kernel matrix.

torch.Size([9231, 2])
We keep 5.72e+05/1.50e+07 =  3% of the original kernel matrix.

torch.Size([20800, 2])
We keep 1.96e+06/1.01e+08 =  1% of the original kernel matrix.

torch.Size([42702, 2])
We keep 1.79e+07/7.96e+08 =  2% of the original kernel matrix.

torch.Size([43940, 2])
We keep 8.99e+06/7.35e+08 =  1% of the original kernel matrix.

torch.Size([77145, 2])
We keep 8.34e+07/3.71e+09 =  2% of the original kernel matrix.

torch.Size([57142, 2])
We keep 1.69e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([16175, 2])
We keep 2.64e+06/6.75e+07 =  3% of the original kernel matrix.

torch.Size([27249, 2])
We keep 3.25e+06/2.14e+08 =  1% of the original kernel matrix.

torch.Size([22300, 2])
We keep 4.42e+06/1.57e+08 =  2% of the original kernel matrix.

torch.Size([31738, 2])
We keep 4.71e+06/3.27e+08 =  1% of the original kernel matrix.

torch.Size([2754, 2])
We keep 9.14e+04/1.28e+06 =  7% of the original kernel matrix.

torch.Size([12464, 2])
We keep 8.43e+05/2.94e+07 =  2% of the original kernel matrix.

torch.Size([18501, 2])
We keep 2.61e+06/8.55e+07 =  3% of the original kernel matrix.

torch.Size([28845, 2])
We keep 3.70e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([24094, 2])
We keep 6.94e+06/1.83e+08 =  3% of the original kernel matrix.

torch.Size([33133, 2])
We keep 5.03e+06/3.52e+08 =  1% of the original kernel matrix.

torch.Size([193805, 2])
We keep 3.62e+08/2.13e+10 =  1% of the original kernel matrix.

torch.Size([93187, 2])
We keep 3.59e+07/3.80e+09 =  0% of the original kernel matrix.

torch.Size([10042, 2])
We keep 5.99e+05/1.82e+07 =  3% of the original kernel matrix.

torch.Size([21663, 2])
We keep 2.09e+06/1.11e+08 =  1% of the original kernel matrix.

torch.Size([320134, 2])
We keep 4.53e+08/5.03e+10 =  0% of the original kernel matrix.

torch.Size([123245, 2])
We keep 5.24e+07/5.84e+09 =  0% of the original kernel matrix.

torch.Size([87973, 2])
We keep 3.32e+08/5.56e+09 =  5% of the original kernel matrix.

torch.Size([61194, 2])
We keep 1.94e+07/1.94e+09 =  1% of the original kernel matrix.

torch.Size([119923, 2])
We keep 1.08e+08/6.73e+09 =  1% of the original kernel matrix.

torch.Size([72418, 2])
We keep 2.16e+07/2.14e+09 =  1% of the original kernel matrix.

torch.Size([250158, 2])
We keep 9.09e+09/1.03e+11 =  8% of the original kernel matrix.

torch.Size([106028, 2])
We keep 6.99e+07/8.36e+09 =  0% of the original kernel matrix.

torch.Size([11045, 2])
We keep 9.44e+05/2.29e+07 =  4% of the original kernel matrix.

torch.Size([22567, 2])
We keep 2.30e+06/1.25e+08 =  1% of the original kernel matrix.

torch.Size([16566, 2])
We keep 1.65e+06/6.16e+07 =  2% of the original kernel matrix.

torch.Size([27124, 2])
We keep 3.27e+06/2.04e+08 =  1% of the original kernel matrix.

torch.Size([419975, 2])
We keep 7.51e+08/7.69e+10 =  0% of the original kernel matrix.

torch.Size([139237, 2])
We keep 6.30e+07/7.22e+09 =  0% of the original kernel matrix.

torch.Size([1079079, 2])
We keep 2.74e+09/3.94e+11 =  0% of the original kernel matrix.

torch.Size([235172, 2])
We keep 1.31e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([14262, 2])
We keep 1.58e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([25203, 2])
We keep 3.01e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([100352, 2])
We keep 5.65e+07/3.72e+09 =  1% of the original kernel matrix.

torch.Size([66027, 2])
We keep 1.71e+07/1.59e+09 =  1% of the original kernel matrix.

torch.Size([42232, 2])
We keep 2.97e+07/8.22e+08 =  3% of the original kernel matrix.

torch.Size([44257, 2])
We keep 9.35e+06/7.47e+08 =  1% of the original kernel matrix.

torch.Size([81306, 2])
We keep 1.08e+08/2.36e+09 =  4% of the original kernel matrix.

torch.Size([60020, 2])
We keep 1.37e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([156433, 2])
We keep 2.59e+08/1.60e+10 =  1% of the original kernel matrix.

torch.Size([83033, 2])
We keep 3.18e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([65140, 2])
We keep 1.42e+08/3.92e+09 =  3% of the original kernel matrix.

torch.Size([49428, 2])
We keep 1.74e+07/1.63e+09 =  1% of the original kernel matrix.

torch.Size([55828, 2])
We keep 2.38e+07/1.17e+09 =  2% of the original kernel matrix.

torch.Size([50319, 2])
We keep 1.02e+07/8.90e+08 =  1% of the original kernel matrix.

torch.Size([36018, 2])
We keep 7.63e+06/4.03e+08 =  1% of the original kernel matrix.

torch.Size([41062, 2])
We keep 6.77e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([10183, 2])
We keep 7.73e+05/1.91e+07 =  4% of the original kernel matrix.

torch.Size([21665, 2])
We keep 2.12e+06/1.14e+08 =  1% of the original kernel matrix.

torch.Size([169276, 2])
We keep 3.55e+08/2.22e+10 =  1% of the original kernel matrix.

torch.Size([85190, 2])
We keep 3.65e+07/3.88e+09 =  0% of the original kernel matrix.

torch.Size([31138, 2])
We keep 1.86e+07/3.81e+08 =  4% of the original kernel matrix.

torch.Size([37639, 2])
We keep 6.51e+06/5.08e+08 =  1% of the original kernel matrix.

torch.Size([9952, 2])
We keep 6.19e+05/1.85e+07 =  3% of the original kernel matrix.

torch.Size([21556, 2])
We keep 2.11e+06/1.12e+08 =  1% of the original kernel matrix.

torch.Size([42309, 2])
We keep 1.36e+07/6.93e+08 =  1% of the original kernel matrix.

torch.Size([44439, 2])
We keep 8.53e+06/6.86e+08 =  1% of the original kernel matrix.

torch.Size([162248, 2])
We keep 2.79e+08/1.74e+10 =  1% of the original kernel matrix.

torch.Size([82811, 2])
We keep 3.31e+07/3.44e+09 =  0% of the original kernel matrix.

torch.Size([355277, 2])
We keep 4.04e+08/4.69e+10 =  0% of the original kernel matrix.

torch.Size([129943, 2])
We keep 5.02e+07/5.64e+09 =  0% of the original kernel matrix.

torch.Size([176801, 2])
We keep 1.26e+08/1.32e+10 =  0% of the original kernel matrix.

torch.Size([89930, 2])
We keep 2.92e+07/2.99e+09 =  0% of the original kernel matrix.

torch.Size([13720, 2])
We keep 1.27e+06/4.32e+07 =  2% of the original kernel matrix.

torch.Size([24692, 2])
We keep 2.87e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([77055, 2])
We keep 5.93e+07/2.74e+09 =  2% of the original kernel matrix.

torch.Size([57828, 2])
We keep 1.50e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([13323, 2])
We keep 1.64e+06/4.51e+07 =  3% of the original kernel matrix.

torch.Size([24163, 2])
We keep 2.93e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([81339, 2])
We keep 3.75e+07/2.53e+09 =  1% of the original kernel matrix.

torch.Size([60223, 2])
We keep 1.43e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([109689, 2])
We keep 6.09e+07/4.75e+09 =  1% of the original kernel matrix.

torch.Size([70278, 2])
We keep 1.88e+07/1.79e+09 =  1% of the original kernel matrix.

torch.Size([20564, 2])
We keep 7.37e+06/1.66e+08 =  4% of the original kernel matrix.

torch.Size([30244, 2])
We keep 4.80e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([191041, 2])
We keep 1.53e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([93255, 2])
We keep 3.01e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([2540, 2])
We keep 5.60e+04/8.97e+05 =  6% of the original kernel matrix.

torch.Size([12349, 2])
We keep 7.55e+05/2.47e+07 =  3% of the original kernel matrix.

torch.Size([22705, 2])
We keep 3.80e+06/1.60e+08 =  2% of the original kernel matrix.

torch.Size([32207, 2])
We keep 4.71e+06/3.29e+08 =  1% of the original kernel matrix.

torch.Size([174243, 2])
We keep 1.95e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([87497, 2])
We keep 3.20e+07/3.31e+09 =  0% of the original kernel matrix.

torch.Size([100723, 2])
We keep 1.42e+08/6.01e+09 =  2% of the original kernel matrix.

torch.Size([65054, 2])
We keep 2.06e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([140373, 2])
We keep 1.61e+08/1.17e+10 =  1% of the original kernel matrix.

torch.Size([77566, 2])
We keep 2.78e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([8526, 2])
We keep 6.76e+05/1.48e+07 =  4% of the original kernel matrix.

torch.Size([19935, 2])
We keep 1.96e+06/1.00e+08 =  1% of the original kernel matrix.

torch.Size([438801, 2])
We keep 5.53e+08/8.09e+10 =  0% of the original kernel matrix.

torch.Size([143260, 2])
We keep 6.34e+07/7.40e+09 =  0% of the original kernel matrix.

torch.Size([38618, 2])
We keep 7.25e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([43689, 2])
We keep 7.57e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([61137, 2])
We keep 2.04e+07/1.35e+09 =  1% of the original kernel matrix.

torch.Size([52478, 2])
We keep 1.10e+07/9.56e+08 =  1% of the original kernel matrix.

torch.Size([151481, 2])
We keep 1.14e+08/9.95e+09 =  1% of the original kernel matrix.

torch.Size([82689, 2])
We keep 2.59e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([5133, 2])
We keep 1.97e+05/4.40e+06 =  4% of the original kernel matrix.

torch.Size([16313, 2])
We keep 1.29e+06/5.46e+07 =  2% of the original kernel matrix.

torch.Size([316063, 2])
We keep 4.18e+08/3.92e+10 =  1% of the original kernel matrix.

torch.Size([122146, 2])
We keep 4.61e+07/5.16e+09 =  0% of the original kernel matrix.

torch.Size([311412, 2])
We keep 9.82e+08/5.36e+10 =  1% of the original kernel matrix.

torch.Size([121993, 2])
We keep 5.43e+07/6.03e+09 =  0% of the original kernel matrix.

torch.Size([807490, 2])
We keep 3.01e+09/2.81e+11 =  1% of the original kernel matrix.

torch.Size([198631, 2])
We keep 1.12e+08/1.38e+10 =  0% of the original kernel matrix.

torch.Size([171426, 2])
We keep 1.35e+08/1.21e+10 =  1% of the original kernel matrix.

torch.Size([87223, 2])
We keep 2.78e+07/2.87e+09 =  0% of the original kernel matrix.

torch.Size([80034, 2])
We keep 4.13e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([58948, 2])
We keep 1.45e+07/1.32e+09 =  1% of the original kernel matrix.

torch.Size([20223, 2])
We keep 4.84e+06/1.66e+08 =  2% of the original kernel matrix.

torch.Size([29547, 2])
We keep 4.81e+06/3.35e+08 =  1% of the original kernel matrix.

torch.Size([19013, 2])
We keep 1.78e+06/8.16e+07 =  2% of the original kernel matrix.

torch.Size([29323, 2])
We keep 3.62e+06/2.35e+08 =  1% of the original kernel matrix.

torch.Size([231793, 2])
We keep 3.55e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([103858, 2])
We keep 4.18e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([21695, 2])
We keep 3.13e+06/1.48e+08 =  2% of the original kernel matrix.

torch.Size([31452, 2])
We keep 4.66e+06/3.17e+08 =  1% of the original kernel matrix.

torch.Size([46200, 2])
We keep 1.60e+07/8.69e+08 =  1% of the original kernel matrix.

torch.Size([45695, 2])
We keep 9.23e+06/7.67e+08 =  1% of the original kernel matrix.

torch.Size([67842, 2])
We keep 6.28e+07/2.30e+09 =  2% of the original kernel matrix.

torch.Size([55345, 2])
We keep 1.33e+07/1.25e+09 =  1% of the original kernel matrix.

torch.Size([39572, 2])
We keep 1.12e+07/5.09e+08 =  2% of the original kernel matrix.

torch.Size([43101, 2])
We keep 7.29e+06/5.88e+08 =  1% of the original kernel matrix.

torch.Size([33438, 2])
We keep 1.67e+07/5.79e+08 =  2% of the original kernel matrix.

torch.Size([38426, 2])
We keep 8.04e+06/6.26e+08 =  1% of the original kernel matrix.

torch.Size([11496, 2])
We keep 7.96e+05/2.66e+07 =  2% of the original kernel matrix.

torch.Size([22879, 2])
We keep 2.40e+06/1.34e+08 =  1% of the original kernel matrix.

torch.Size([116334, 2])
We keep 2.80e+08/1.43e+10 =  1% of the original kernel matrix.

torch.Size([70117, 2])
We keep 3.00e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([81315, 2])
We keep 4.65e+08/4.28e+09 = 10% of the original kernel matrix.

torch.Size([59856, 2])
We keep 1.83e+07/1.70e+09 =  1% of the original kernel matrix.

torch.Size([36650, 2])
We keep 5.14e+07/8.70e+08 =  5% of the original kernel matrix.

torch.Size([40236, 2])
We keep 9.40e+06/7.68e+08 =  1% of the original kernel matrix.

torch.Size([23040, 2])
We keep 3.53e+06/1.63e+08 =  2% of the original kernel matrix.

torch.Size([32356, 2])
We keep 4.81e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([80611, 2])
We keep 5.09e+07/3.09e+09 =  1% of the original kernel matrix.

torch.Size([59741, 2])
We keep 1.56e+07/1.45e+09 =  1% of the original kernel matrix.

torch.Size([69570, 2])
We keep 2.64e+07/1.87e+09 =  1% of the original kernel matrix.

torch.Size([56489, 2])
We keep 1.28e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([349761, 2])
We keep 9.80e+08/7.01e+10 =  1% of the original kernel matrix.

torch.Size([127493, 2])
We keep 6.09e+07/6.90e+09 =  0% of the original kernel matrix.

torch.Size([14114, 2])
We keep 2.00e+06/5.93e+07 =  3% of the original kernel matrix.

torch.Size([25246, 2])
We keep 3.22e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([52578, 2])
We keep 3.57e+07/1.76e+09 =  2% of the original kernel matrix.

torch.Size([46921, 2])
We keep 1.24e+07/1.09e+09 =  1% of the original kernel matrix.

torch.Size([24554, 2])
We keep 3.50e+06/1.72e+08 =  2% of the original kernel matrix.

torch.Size([33607, 2])
We keep 4.79e+06/3.41e+08 =  1% of the original kernel matrix.

torch.Size([174880, 2])
We keep 4.96e+08/2.26e+10 =  2% of the original kernel matrix.

torch.Size([87105, 2])
We keep 3.70e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([59155, 2])
We keep 1.86e+07/1.54e+09 =  1% of the original kernel matrix.

torch.Size([51476, 2])
We keep 1.16e+07/1.02e+09 =  1% of the original kernel matrix.

torch.Size([43618, 2])
We keep 1.67e+07/6.56e+08 =  2% of the original kernel matrix.

torch.Size([44911, 2])
We keep 8.20e+06/6.67e+08 =  1% of the original kernel matrix.

torch.Size([253634, 2])
We keep 1.67e+08/2.22e+10 =  0% of the original kernel matrix.

torch.Size([109015, 2])
We keep 3.61e+07/3.88e+09 =  0% of the original kernel matrix.

torch.Size([319486, 2])
We keep 3.47e+08/4.23e+10 =  0% of the original kernel matrix.

torch.Size([126385, 2])
We keep 4.91e+07/5.35e+09 =  0% of the original kernel matrix.

torch.Size([39905, 2])
We keep 1.76e+08/1.32e+09 = 13% of the original kernel matrix.

torch.Size([41836, 2])
We keep 1.09e+07/9.47e+08 =  1% of the original kernel matrix.

torch.Size([320091, 2])
We keep 7.52e+08/4.57e+10 =  1% of the original kernel matrix.

torch.Size([123916, 2])
We keep 5.02e+07/5.57e+09 =  0% of the original kernel matrix.

torch.Size([349635, 2])
We keep 3.41e+08/4.72e+10 =  0% of the original kernel matrix.

torch.Size([129027, 2])
We keep 5.04e+07/5.66e+09 =  0% of the original kernel matrix.

torch.Size([806130, 2])
We keep 1.53e+09/2.26e+11 =  0% of the original kernel matrix.

torch.Size([202408, 2])
We keep 1.02e+08/1.24e+10 =  0% of the original kernel matrix.

torch.Size([41923, 2])
We keep 1.65e+07/5.90e+08 =  2% of the original kernel matrix.

torch.Size([44557, 2])
We keep 8.00e+06/6.33e+08 =  1% of the original kernel matrix.

torch.Size([7842, 2])
We keep 5.19e+05/1.25e+07 =  4% of the original kernel matrix.

torch.Size([19135, 2])
We keep 1.85e+06/9.22e+07 =  2% of the original kernel matrix.

torch.Size([23941, 2])
We keep 3.49e+06/1.76e+08 =  1% of the original kernel matrix.

torch.Size([33100, 2])
We keep 4.93e+06/3.45e+08 =  1% of the original kernel matrix.

torch.Size([29136, 2])
We keep 1.36e+07/4.98e+08 =  2% of the original kernel matrix.

torch.Size([36419, 2])
We keep 7.20e+06/5.81e+08 =  1% of the original kernel matrix.

torch.Size([50575, 2])
We keep 1.50e+07/9.91e+08 =  1% of the original kernel matrix.

torch.Size([48133, 2])
We keep 9.73e+06/8.19e+08 =  1% of the original kernel matrix.

torch.Size([162646, 2])
We keep 2.32e+08/1.16e+10 =  1% of the original kernel matrix.

torch.Size([87145, 2])
We keep 2.76e+07/2.81e+09 =  0% of the original kernel matrix.

torch.Size([5039, 2])
We keep 1.94e+05/4.18e+06 =  4% of the original kernel matrix.

torch.Size([16188, 2])
We keep 1.26e+06/5.32e+07 =  2% of the original kernel matrix.

torch.Size([132216, 2])
We keep 8.96e+07/6.52e+09 =  1% of the original kernel matrix.

torch.Size([76876, 2])
We keep 2.17e+07/2.10e+09 =  1% of the original kernel matrix.

torch.Size([213532, 2])
We keep 1.74e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([99199, 2])
We keep 3.38e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([9483, 2])
We keep 1.47e+07/8.42e+07 = 17% of the original kernel matrix.

torch.Size([20042, 2])
We keep 3.48e+06/2.39e+08 =  1% of the original kernel matrix.

torch.Size([21352, 2])
We keep 2.91e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([31087, 2])
We keep 4.31e+06/2.93e+08 =  1% of the original kernel matrix.

torch.Size([124504, 2])
We keep 7.94e+07/6.62e+09 =  1% of the original kernel matrix.

torch.Size([74041, 2])
We keep 2.17e+07/2.12e+09 =  1% of the original kernel matrix.

torch.Size([29002, 2])
We keep 7.08e+06/2.75e+08 =  2% of the original kernel matrix.

torch.Size([36579, 2])
We keep 5.70e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([25064, 2])
We keep 5.54e+06/2.21e+08 =  2% of the original kernel matrix.

torch.Size([33904, 2])
We keep 5.41e+06/3.87e+08 =  1% of the original kernel matrix.

torch.Size([34733, 2])
We keep 6.39e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([40201, 2])
We keep 6.76e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([28803, 2])
We keep 5.03e+07/3.99e+08 = 12% of the original kernel matrix.

torch.Size([35982, 2])
We keep 6.44e+06/5.20e+08 =  1% of the original kernel matrix.

torch.Size([46184, 2])
We keep 2.77e+07/9.06e+08 =  3% of the original kernel matrix.

torch.Size([45144, 2])
We keep 9.18e+06/7.84e+08 =  1% of the original kernel matrix.

torch.Size([88112, 2])
We keep 2.85e+07/2.54e+09 =  1% of the original kernel matrix.

torch.Size([62664, 2])
We keep 1.43e+07/1.31e+09 =  1% of the original kernel matrix.

torch.Size([46374, 2])
We keep 1.54e+07/8.58e+08 =  1% of the original kernel matrix.

torch.Size([46936, 2])
We keep 9.38e+06/7.63e+08 =  1% of the original kernel matrix.

torch.Size([60889, 2])
We keep 3.94e+07/1.90e+09 =  2% of the original kernel matrix.

torch.Size([51972, 2])
We keep 1.30e+07/1.14e+09 =  1% of the original kernel matrix.

torch.Size([31360, 2])
We keep 6.05e+06/3.33e+08 =  1% of the original kernel matrix.

torch.Size([38258, 2])
We keep 6.31e+06/4.75e+08 =  1% of the original kernel matrix.

torch.Size([1921829, 2])
We keep 8.03e+09/1.15e+12 =  0% of the original kernel matrix.

torch.Size([319093, 2])
We keep 2.16e+08/2.79e+10 =  0% of the original kernel matrix.

torch.Size([39275, 2])
We keep 8.33e+06/5.40e+08 =  1% of the original kernel matrix.

torch.Size([40616, 2])
We keep 7.15e+06/6.05e+08 =  1% of the original kernel matrix.

torch.Size([252712, 2])
We keep 2.41e+08/2.81e+10 =  0% of the original kernel matrix.

torch.Size([108659, 2])
We keep 4.03e+07/4.36e+09 =  0% of the original kernel matrix.

torch.Size([6420, 2])
We keep 2.77e+05/6.70e+06 =  4% of the original kernel matrix.

torch.Size([17827, 2])
We keep 1.48e+06/6.74e+07 =  2% of the original kernel matrix.

torch.Size([20653, 2])
We keep 2.94e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([30423, 2])
We keep 4.09e+06/2.71e+08 =  1% of the original kernel matrix.

torch.Size([11174, 2])
We keep 7.97e+05/2.48e+07 =  3% of the original kernel matrix.

torch.Size([22383, 2])
We keep 2.39e+06/1.30e+08 =  1% of the original kernel matrix.

torch.Size([46838, 2])
We keep 1.06e+07/7.39e+08 =  1% of the original kernel matrix.

torch.Size([47353, 2])
We keep 8.48e+06/7.08e+08 =  1% of the original kernel matrix.

torch.Size([12962, 2])
We keep 1.11e+06/3.55e+07 =  3% of the original kernel matrix.

torch.Size([24084, 2])
We keep 2.68e+06/1.55e+08 =  1% of the original kernel matrix.

torch.Size([149556, 2])
We keep 2.62e+08/1.15e+10 =  2% of the original kernel matrix.

torch.Size([81892, 2])
We keep 2.77e+07/2.79e+09 =  0% of the original kernel matrix.

torch.Size([19928, 2])
We keep 2.04e+06/9.53e+07 =  2% of the original kernel matrix.

torch.Size([29967, 2])
We keep 3.85e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([13067, 2])
We keep 1.06e+06/3.64e+07 =  2% of the original kernel matrix.

torch.Size([24105, 2])
We keep 2.72e+06/1.57e+08 =  1% of the original kernel matrix.

torch.Size([467084, 2])
We keep 8.40e+08/9.32e+10 =  0% of the original kernel matrix.

torch.Size([148940, 2])
We keep 6.90e+07/7.95e+09 =  0% of the original kernel matrix.

torch.Size([102264, 2])
We keep 7.21e+07/4.62e+09 =  1% of the original kernel matrix.

torch.Size([66879, 2])
We keep 1.87e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([61137, 2])
We keep 2.58e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([52516, 2])
We keep 1.20e+07/1.04e+09 =  1% of the original kernel matrix.

torch.Size([239062, 2])
We keep 2.46e+08/2.41e+10 =  1% of the original kernel matrix.

torch.Size([106405, 2])
We keep 3.79e+07/4.04e+09 =  0% of the original kernel matrix.

torch.Size([704673, 2])
We keep 1.21e+09/1.74e+11 =  0% of the original kernel matrix.

torch.Size([186382, 2])
We keep 9.14e+07/1.09e+10 =  0% of the original kernel matrix.

torch.Size([19583, 2])
We keep 1.98e+06/9.02e+07 =  2% of the original kernel matrix.

torch.Size([29662, 2])
We keep 3.80e+06/2.47e+08 =  1% of the original kernel matrix.

torch.Size([29067, 2])
We keep 4.10e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([36815, 2])
We keep 5.60e+06/4.16e+08 =  1% of the original kernel matrix.

torch.Size([27931, 2])
We keep 9.88e+06/3.32e+08 =  2% of the original kernel matrix.

torch.Size([35532, 2])
We keep 6.31e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([5277, 2])
We keep 2.21e+05/4.98e+06 =  4% of the original kernel matrix.

torch.Size([16326, 2])
We keep 1.35e+06/5.81e+07 =  2% of the original kernel matrix.

torch.Size([20517, 2])
We keep 2.99e+06/1.18e+08 =  2% of the original kernel matrix.

torch.Size([30399, 2])
We keep 4.18e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([30809, 2])
We keep 5.89e+06/3.18e+08 =  1% of the original kernel matrix.

torch.Size([37823, 2])
We keep 6.17e+06/4.64e+08 =  1% of the original kernel matrix.

torch.Size([7854, 2])
We keep 4.63e+05/1.25e+07 =  3% of the original kernel matrix.

torch.Size([19251, 2])
We keep 1.82e+06/9.21e+07 =  1% of the original kernel matrix.

torch.Size([20387, 2])
We keep 2.58e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([30242, 2])
We keep 4.05e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([45059, 2])
We keep 1.38e+07/7.10e+08 =  1% of the original kernel matrix.

torch.Size([45778, 2])
We keep 8.42e+06/6.94e+08 =  1% of the original kernel matrix.

torch.Size([16413, 2])
We keep 1.76e+06/6.51e+07 =  2% of the original kernel matrix.

torch.Size([27224, 2])
We keep 3.33e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([517091, 2])
We keep 1.01e+09/1.14e+11 =  0% of the original kernel matrix.

torch.Size([156505, 2])
We keep 7.49e+07/8.79e+09 =  0% of the original kernel matrix.

torch.Size([23848, 2])
We keep 8.17e+06/3.29e+08 =  2% of the original kernel matrix.

torch.Size([32087, 2])
We keep 6.29e+06/4.72e+08 =  1% of the original kernel matrix.

torch.Size([142449, 2])
We keep 1.09e+08/8.37e+09 =  1% of the original kernel matrix.

torch.Size([79826, 2])
We keep 2.40e+07/2.38e+09 =  1% of the original kernel matrix.

torch.Size([9213, 2])
We keep 9.43e+05/2.03e+07 =  4% of the original kernel matrix.

torch.Size([20711, 2])
We keep 2.15e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([11767, 2])
We keep 1.15e+06/3.39e+07 =  3% of the original kernel matrix.

torch.Size([22810, 2])
We keep 2.67e+06/1.52e+08 =  1% of the original kernel matrix.

torch.Size([249632, 2])
We keep 3.07e+08/3.04e+10 =  1% of the original kernel matrix.

torch.Size([107539, 2])
We keep 4.22e+07/4.54e+09 =  0% of the original kernel matrix.

torch.Size([997436, 2])
We keep 4.91e+09/3.97e+11 =  1% of the original kernel matrix.

torch.Size([229890, 2])
We keep 1.31e+08/1.64e+10 =  0% of the original kernel matrix.

torch.Size([1012111, 2])
We keep 2.00e+09/3.52e+11 =  0% of the original kernel matrix.

torch.Size([226515, 2])
We keep 1.23e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([47660, 2])
We keep 1.19e+07/7.74e+08 =  1% of the original kernel matrix.

torch.Size([47377, 2])
We keep 8.74e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([82809, 2])
We keep 3.16e+07/2.24e+09 =  1% of the original kernel matrix.

torch.Size([60757, 2])
We keep 1.35e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([16267, 2])
We keep 1.40e+06/5.62e+07 =  2% of the original kernel matrix.

torch.Size([27002, 2])
We keep 3.16e+06/1.95e+08 =  1% of the original kernel matrix.

torch.Size([14417, 2])
We keep 3.02e+06/5.90e+07 =  5% of the original kernel matrix.

torch.Size([25583, 2])
We keep 3.08e+06/2.00e+08 =  1% of the original kernel matrix.

torch.Size([759135, 2])
We keep 6.66e+09/3.61e+11 =  1% of the original kernel matrix.

torch.Size([188625, 2])
We keep 1.25e+08/1.56e+10 =  0% of the original kernel matrix.

torch.Size([34522, 2])
We keep 7.91e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([40587, 2])
We keep 7.00e+06/5.43e+08 =  1% of the original kernel matrix.

torch.Size([23223, 2])
We keep 3.03e+06/1.51e+08 =  2% of the original kernel matrix.

torch.Size([32416, 2])
We keep 4.62e+06/3.20e+08 =  1% of the original kernel matrix.

torch.Size([79538, 2])
We keep 4.53e+07/2.82e+09 =  1% of the original kernel matrix.

torch.Size([58841, 2])
We keep 1.49e+07/1.38e+09 =  1% of the original kernel matrix.

torch.Size([1079017, 2])
We keep 4.44e+09/5.63e+11 =  0% of the original kernel matrix.

torch.Size([227829, 2])
We keep 1.57e+08/1.95e+10 =  0% of the original kernel matrix.

torch.Size([72164, 2])
We keep 3.31e+07/2.20e+09 =  1% of the original kernel matrix.

torch.Size([54858, 2])
We keep 1.34e+07/1.22e+09 =  1% of the original kernel matrix.

torch.Size([33760, 2])
We keep 1.12e+07/4.71e+08 =  2% of the original kernel matrix.

torch.Size([39515, 2])
We keep 7.25e+06/5.65e+08 =  1% of the original kernel matrix.

torch.Size([23084, 2])
We keep 5.60e+06/2.07e+08 =  2% of the original kernel matrix.

torch.Size([32728, 2])
We keep 5.39e+06/3.75e+08 =  1% of the original kernel matrix.

torch.Size([50411, 2])
We keep 1.21e+07/9.18e+08 =  1% of the original kernel matrix.

torch.Size([48303, 2])
We keep 9.45e+06/7.89e+08 =  1% of the original kernel matrix.

torch.Size([16091, 2])
We keep 1.59e+06/5.67e+07 =  2% of the original kernel matrix.

torch.Size([26848, 2])
We keep 3.18e+06/1.96e+08 =  1% of the original kernel matrix.

torch.Size([308094, 2])
We keep 4.67e+08/4.42e+10 =  1% of the original kernel matrix.

torch.Size([121125, 2])
We keep 4.94e+07/5.48e+09 =  0% of the original kernel matrix.

torch.Size([28551, 2])
We keep 7.97e+06/2.86e+08 =  2% of the original kernel matrix.

torch.Size([35704, 2])
We keep 5.94e+06/4.41e+08 =  1% of the original kernel matrix.

torch.Size([7743, 2])
We keep 6.29e+05/1.17e+07 =  5% of the original kernel matrix.

torch.Size([19030, 2])
We keep 1.80e+06/8.91e+07 =  2% of the original kernel matrix.

torch.Size([203599, 2])
We keep 2.03e+08/1.61e+10 =  1% of the original kernel matrix.

torch.Size([97377, 2])
We keep 3.14e+07/3.30e+09 =  0% of the original kernel matrix.

torch.Size([38871, 2])
We keep 8.67e+06/5.44e+08 =  1% of the original kernel matrix.

torch.Size([43861, 2])
We keep 7.89e+06/6.07e+08 =  1% of the original kernel matrix.

torch.Size([2912, 2])
We keep 7.86e+04/1.23e+06 =  6% of the original kernel matrix.

torch.Size([12996, 2])
We keep 8.34e+05/2.89e+07 =  2% of the original kernel matrix.

torch.Size([113413, 2])
We keep 7.43e+07/5.22e+09 =  1% of the original kernel matrix.

torch.Size([69933, 2])
We keep 1.95e+07/1.88e+09 =  1% of the original kernel matrix.

torch.Size([195301, 2])
We keep 1.43e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([95050, 2])
We keep 3.00e+07/3.12e+09 =  0% of the original kernel matrix.

torch.Size([422685, 2])
We keep 1.13e+09/8.00e+10 =  1% of the original kernel matrix.

torch.Size([141415, 2])
We keep 6.49e+07/7.36e+09 =  0% of the original kernel matrix.

torch.Size([334710, 2])
We keep 8.85e+08/5.96e+10 =  1% of the original kernel matrix.

torch.Size([125734, 2])
We keep 5.69e+07/6.36e+09 =  0% of the original kernel matrix.

torch.Size([329459, 2])
We keep 6.31e+08/5.60e+10 =  1% of the original kernel matrix.

torch.Size([123789, 2])
We keep 5.50e+07/6.16e+09 =  0% of the original kernel matrix.

torch.Size([37698, 2])
We keep 1.85e+07/7.76e+08 =  2% of the original kernel matrix.

torch.Size([40544, 2])
We keep 9.07e+06/7.25e+08 =  1% of the original kernel matrix.

torch.Size([49296, 2])
We keep 1.99e+07/8.77e+08 =  2% of the original kernel matrix.

torch.Size([47152, 2])
We keep 9.16e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([295339, 2])
We keep 2.57e+08/3.34e+10 =  0% of the original kernel matrix.

torch.Size([119388, 2])
We keep 4.34e+07/4.76e+09 =  0% of the original kernel matrix.

torch.Size([394063, 2])
We keep 5.05e+08/6.18e+10 =  0% of the original kernel matrix.

torch.Size([138307, 2])
We keep 5.66e+07/6.47e+09 =  0% of the original kernel matrix.

torch.Size([28078, 2])
We keep 5.52e+06/2.79e+08 =  1% of the original kernel matrix.

torch.Size([35953, 2])
We keep 5.90e+06/4.35e+08 =  1% of the original kernel matrix.

torch.Size([217604, 2])
We keep 3.25e+09/5.14e+10 =  6% of the original kernel matrix.

torch.Size([101901, 2])
We keep 5.26e+07/5.90e+09 =  0% of the original kernel matrix.

torch.Size([450961, 2])
We keep 5.27e+09/1.02e+11 =  5% of the original kernel matrix.

torch.Size([146126, 2])
We keep 7.19e+07/8.30e+09 =  0% of the original kernel matrix.

torch.Size([290422, 2])
We keep 5.22e+08/4.13e+10 =  1% of the original kernel matrix.

torch.Size([117705, 2])
We keep 4.86e+07/5.29e+09 =  0% of the original kernel matrix.

torch.Size([166073, 2])
We keep 1.64e+08/1.22e+10 =  1% of the original kernel matrix.

torch.Size([86183, 2])
We keep 2.80e+07/2.88e+09 =  0% of the original kernel matrix.

torch.Size([71868, 2])
We keep 1.31e+08/4.03e+09 =  3% of the original kernel matrix.

torch.Size([54540, 2])
We keep 1.76e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([29790, 2])
We keep 9.49e+06/3.84e+08 =  2% of the original kernel matrix.

torch.Size([36299, 2])
We keep 6.78e+06/5.10e+08 =  1% of the original kernel matrix.

torch.Size([20918, 2])
We keep 2.18e+06/1.06e+08 =  2% of the original kernel matrix.

torch.Size([30679, 2])
We keep 3.98e+06/2.68e+08 =  1% of the original kernel matrix.

torch.Size([26578, 2])
We keep 1.00e+07/2.75e+08 =  3% of the original kernel matrix.

torch.Size([34195, 2])
We keep 5.73e+06/4.32e+08 =  1% of the original kernel matrix.

torch.Size([278500, 2])
We keep 2.28e+08/3.14e+10 =  0% of the original kernel matrix.

torch.Size([115746, 2])
We keep 4.25e+07/4.62e+09 =  0% of the original kernel matrix.

torch.Size([228431, 2])
We keep 1.51e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([103068, 2])
We keep 3.32e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([393842, 2])
We keep 9.35e+08/8.36e+10 =  1% of the original kernel matrix.

torch.Size([135419, 2])
We keep 6.59e+07/7.53e+09 =  0% of the original kernel matrix.

torch.Size([143844, 2])
We keep 1.59e+08/9.62e+09 =  1% of the original kernel matrix.

torch.Size([80406, 2])
We keep 2.56e+07/2.55e+09 =  1% of the original kernel matrix.

torch.Size([139945, 2])
We keep 1.15e+08/8.54e+09 =  1% of the original kernel matrix.

torch.Size([79223, 2])
We keep 2.41e+07/2.41e+09 =  1% of the original kernel matrix.

torch.Size([28281, 2])
We keep 5.29e+06/3.21e+08 =  1% of the original kernel matrix.

torch.Size([35558, 2])
We keep 6.14e+06/4.67e+08 =  1% of the original kernel matrix.

torch.Size([332068, 2])
We keep 4.00e+08/4.39e+10 =  0% of the original kernel matrix.

torch.Size([126250, 2])
We keep 4.88e+07/5.46e+09 =  0% of the original kernel matrix.

torch.Size([13936, 2])
We keep 2.51e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([24747, 2])
We keep 3.35e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([27603, 2])
We keep 4.76e+06/2.41e+08 =  1% of the original kernel matrix.

torch.Size([35574, 2])
We keep 5.54e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([239439, 2])
We keep 2.22e+08/2.40e+10 =  0% of the original kernel matrix.

torch.Size([105746, 2])
We keep 3.77e+07/4.03e+09 =  0% of the original kernel matrix.

torch.Size([157493, 2])
We keep 1.25e+08/9.93e+09 =  1% of the original kernel matrix.

torch.Size([84046, 2])
We keep 2.55e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([13760, 2])
We keep 1.45e+06/4.53e+07 =  3% of the original kernel matrix.

torch.Size([24766, 2])
We keep 2.96e+06/1.75e+08 =  1% of the original kernel matrix.

torch.Size([37513, 2])
We keep 8.12e+06/5.15e+08 =  1% of the original kernel matrix.

torch.Size([41663, 2])
We keep 7.49e+06/5.91e+08 =  1% of the original kernel matrix.

torch.Size([890902, 2])
We keep 4.91e+09/4.25e+11 =  1% of the original kernel matrix.

torch.Size([207143, 2])
We keep 1.38e+08/1.70e+10 =  0% of the original kernel matrix.

torch.Size([35409, 2])
We keep 8.91e+06/4.62e+08 =  1% of the original kernel matrix.

torch.Size([40461, 2])
We keep 7.29e+06/5.60e+08 =  1% of the original kernel matrix.

torch.Size([196829, 2])
We keep 2.35e+08/1.67e+10 =  1% of the original kernel matrix.

torch.Size([95749, 2])
We keep 3.25e+07/3.37e+09 =  0% of the original kernel matrix.

torch.Size([18882, 2])
We keep 1.93e+07/1.43e+08 = 13% of the original kernel matrix.

torch.Size([28804, 2])
We keep 4.30e+06/3.12e+08 =  1% of the original kernel matrix.

torch.Size([16394, 2])
We keep 6.30e+06/7.36e+07 =  8% of the original kernel matrix.

torch.Size([26996, 2])
We keep 3.33e+06/2.23e+08 =  1% of the original kernel matrix.

torch.Size([13400, 2])
We keep 1.20e+06/4.03e+07 =  2% of the original kernel matrix.

torch.Size([24473, 2])
We keep 2.81e+06/1.65e+08 =  1% of the original kernel matrix.

torch.Size([12997, 2])
We keep 1.61e+06/4.19e+07 =  3% of the original kernel matrix.

torch.Size([23898, 2])
We keep 2.81e+06/1.69e+08 =  1% of the original kernel matrix.

torch.Size([40545, 2])
We keep 9.77e+06/6.38e+08 =  1% of the original kernel matrix.

torch.Size([42103, 2])
We keep 7.85e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([117764, 2])
We keep 9.35e+07/6.08e+09 =  1% of the original kernel matrix.

torch.Size([70789, 2])
We keep 2.10e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([10029, 2])
We keep 1.93e+06/2.58e+07 =  7% of the original kernel matrix.

torch.Size([21283, 2])
We keep 2.47e+06/1.32e+08 =  1% of the original kernel matrix.

torch.Size([302478, 2])
We keep 3.69e+08/3.36e+10 =  1% of the original kernel matrix.

torch.Size([119230, 2])
We keep 4.24e+07/4.77e+09 =  0% of the original kernel matrix.

torch.Size([26553, 2])
We keep 6.06e+06/2.54e+08 =  2% of the original kernel matrix.

torch.Size([34753, 2])
We keep 5.64e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([133382, 2])
We keep 1.25e+08/6.56e+09 =  1% of the original kernel matrix.

torch.Size([77799, 2])
We keep 2.16e+07/2.11e+09 =  1% of the original kernel matrix.

torch.Size([25871, 2])
We keep 5.54e+06/2.33e+08 =  2% of the original kernel matrix.

torch.Size([34312, 2])
We keep 5.53e+06/3.98e+08 =  1% of the original kernel matrix.

torch.Size([309479, 2])
We keep 3.11e+08/3.71e+10 =  0% of the original kernel matrix.

torch.Size([121090, 2])
We keep 4.55e+07/5.01e+09 =  0% of the original kernel matrix.

torch.Size([13020, 2])
We keep 1.73e+06/4.32e+07 =  3% of the original kernel matrix.

torch.Size([23890, 2])
We keep 2.91e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([47579, 2])
We keep 1.70e+07/8.38e+08 =  2% of the original kernel matrix.

torch.Size([46679, 2])
We keep 9.09e+06/7.54e+08 =  1% of the original kernel matrix.

torch.Size([84721, 2])
We keep 3.46e+07/2.42e+09 =  1% of the original kernel matrix.

torch.Size([61120, 2])
We keep 1.41e+07/1.28e+09 =  1% of the original kernel matrix.

torch.Size([49599, 2])
We keep 3.23e+07/1.27e+09 =  2% of the original kernel matrix.

torch.Size([46991, 2])
We keep 1.10e+07/9.27e+08 =  1% of the original kernel matrix.

torch.Size([124075, 2])
We keep 8.59e+07/6.79e+09 =  1% of the original kernel matrix.

torch.Size([74624, 2])
We keep 2.22e+07/2.15e+09 =  1% of the original kernel matrix.

torch.Size([1117266, 2])
We keep 5.05e+09/5.12e+11 =  0% of the original kernel matrix.

torch.Size([235186, 2])
We keep 1.48e+08/1.86e+10 =  0% of the original kernel matrix.

torch.Size([48407, 2])
We keep 9.96e+06/7.73e+08 =  1% of the original kernel matrix.

torch.Size([47870, 2])
We keep 8.79e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([14980, 2])
We keep 2.23e+06/5.17e+07 =  4% of the original kernel matrix.

torch.Size([25905, 2])
We keep 3.03e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([107687, 2])
We keep 1.86e+08/4.63e+09 =  4% of the original kernel matrix.

torch.Size([68889, 2])
We keep 1.80e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([51718, 2])
We keep 1.68e+07/1.00e+09 =  1% of the original kernel matrix.

torch.Size([48456, 2])
We keep 9.83e+06/8.24e+08 =  1% of the original kernel matrix.

torch.Size([866208, 2])
We keep 1.55e+09/2.69e+11 =  0% of the original kernel matrix.

torch.Size([208624, 2])
We keep 1.11e+08/1.35e+10 =  0% of the original kernel matrix.

torch.Size([17472, 2])
We keep 3.65e+06/9.53e+07 =  3% of the original kernel matrix.

torch.Size([27727, 2])
We keep 3.82e+06/2.54e+08 =  1% of the original kernel matrix.

torch.Size([225679, 2])
We keep 2.37e+08/2.47e+10 =  0% of the original kernel matrix.

torch.Size([101685, 2])
We keep 3.84e+07/4.09e+09 =  0% of the original kernel matrix.

torch.Size([757037, 2])
We keep 1.16e+09/1.99e+11 =  0% of the original kernel matrix.

torch.Size([193733, 2])
We keep 9.65e+07/1.16e+10 =  0% of the original kernel matrix.

torch.Size([18415, 2])
We keep 3.33e+06/9.16e+07 =  3% of the original kernel matrix.

torch.Size([28634, 2])
We keep 3.80e+06/2.49e+08 =  1% of the original kernel matrix.

torch.Size([363785, 2])
We keep 9.28e+08/7.48e+10 =  1% of the original kernel matrix.

torch.Size([128230, 2])
We keep 6.15e+07/7.12e+09 =  0% of the original kernel matrix.

torch.Size([28414, 2])
We keep 1.38e+07/4.89e+08 =  2% of the original kernel matrix.

torch.Size([35340, 2])
We keep 7.37e+06/5.76e+08 =  1% of the original kernel matrix.

torch.Size([876812, 2])
We keep 2.68e+09/3.31e+11 =  0% of the original kernel matrix.

torch.Size([211411, 2])
We keep 1.23e+08/1.50e+10 =  0% of the original kernel matrix.

torch.Size([38829, 2])
We keep 7.92e+06/5.23e+08 =  1% of the original kernel matrix.

torch.Size([39782, 2])
We keep 6.93e+06/5.96e+08 =  1% of the original kernel matrix.

torch.Size([33530, 2])
We keep 1.18e+07/3.78e+08 =  3% of the original kernel matrix.

torch.Size([39207, 2])
We keep 6.64e+06/5.06e+08 =  1% of the original kernel matrix.

torch.Size([19489, 2])
We keep 7.98e+06/2.41e+08 =  3% of the original kernel matrix.

torch.Size([28816, 2])
We keep 5.76e+06/4.04e+08 =  1% of the original kernel matrix.

torch.Size([13106, 2])
We keep 1.51e+06/3.38e+07 =  4% of the original kernel matrix.

torch.Size([24372, 2])
We keep 2.63e+06/1.51e+08 =  1% of the original kernel matrix.

torch.Size([18584, 2])
We keep 2.83e+06/1.08e+08 =  2% of the original kernel matrix.

torch.Size([28692, 2])
We keep 4.09e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([87757, 2])
We keep 7.62e+07/2.77e+09 =  2% of the original kernel matrix.

torch.Size([62641, 2])
We keep 1.47e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([17648, 2])
We keep 2.29e+06/7.22e+07 =  3% of the original kernel matrix.

torch.Size([28110, 2])
We keep 3.36e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([313216, 2])
We keep 4.07e+08/4.36e+10 =  0% of the original kernel matrix.

torch.Size([125094, 2])
We keep 4.97e+07/5.44e+09 =  0% of the original kernel matrix.

torch.Size([34558, 2])
We keep 6.02e+06/3.90e+08 =  1% of the original kernel matrix.

torch.Size([40234, 2])
We keep 6.69e+06/5.14e+08 =  1% of the original kernel matrix.

torch.Size([22922, 2])
We keep 3.85e+06/1.50e+08 =  2% of the original kernel matrix.

torch.Size([32154, 2])
We keep 4.60e+06/3.19e+08 =  1% of the original kernel matrix.

torch.Size([41009, 2])
We keep 1.25e+07/7.73e+08 =  1% of the original kernel matrix.

torch.Size([43559, 2])
We keep 8.99e+06/7.24e+08 =  1% of the original kernel matrix.

torch.Size([237296, 2])
We keep 5.38e+08/4.60e+10 =  1% of the original kernel matrix.

torch.Size([103344, 2])
We keep 5.05e+07/5.58e+09 =  0% of the original kernel matrix.

torch.Size([252109, 2])
We keep 5.74e+08/4.30e+10 =  1% of the original kernel matrix.

torch.Size([111391, 2])
We keep 4.99e+07/5.40e+09 =  0% of the original kernel matrix.

torch.Size([701639, 2])
We keep 1.14e+09/1.65e+11 =  0% of the original kernel matrix.

torch.Size([185731, 2])
We keep 8.91e+07/1.06e+10 =  0% of the original kernel matrix.

torch.Size([28656, 2])
We keep 6.66e+06/2.91e+08 =  2% of the original kernel matrix.

torch.Size([36675, 2])
We keep 6.03e+06/4.44e+08 =  1% of the original kernel matrix.

torch.Size([15547, 2])
We keep 2.09e+06/7.56e+07 =  2% of the original kernel matrix.

torch.Size([26527, 2])
We keep 3.55e+06/2.26e+08 =  1% of the original kernel matrix.

torch.Size([14903, 2])
We keep 1.11e+06/4.19e+07 =  2% of the original kernel matrix.

torch.Size([25807, 2])
We keep 2.81e+06/1.68e+08 =  1% of the original kernel matrix.

torch.Size([9064, 2])
We keep 9.92e+05/2.01e+07 =  4% of the original kernel matrix.

torch.Size([20371, 2])
We keep 2.18e+06/1.17e+08 =  1% of the original kernel matrix.

torch.Size([357175, 2])
We keep 1.50e+09/5.39e+10 =  2% of the original kernel matrix.

torch.Size([132461, 2])
We keep 5.10e+07/6.04e+09 =  0% of the original kernel matrix.

torch.Size([14899, 2])
We keep 1.59e+06/5.52e+07 =  2% of the original kernel matrix.

torch.Size([25694, 2])
We keep 3.21e+06/1.93e+08 =  1% of the original kernel matrix.

torch.Size([47635, 2])
We keep 1.22e+07/7.82e+08 =  1% of the original kernel matrix.

torch.Size([47090, 2])
We keep 8.86e+06/7.28e+08 =  1% of the original kernel matrix.

torch.Size([44358, 2])
We keep 1.30e+07/7.23e+08 =  1% of the original kernel matrix.

torch.Size([44821, 2])
We keep 8.44e+06/7.00e+08 =  1% of the original kernel matrix.

torch.Size([30313, 2])
We keep 1.62e+07/6.38e+08 =  2% of the original kernel matrix.

torch.Size([35266, 2])
We keep 8.21e+06/6.58e+08 =  1% of the original kernel matrix.

torch.Size([62431, 2])
We keep 5.32e+07/2.15e+09 =  2% of the original kernel matrix.

torch.Size([52456, 2])
We keep 1.36e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([23432, 2])
We keep 4.26e+06/1.69e+08 =  2% of the original kernel matrix.

torch.Size([32495, 2])
We keep 4.83e+06/3.38e+08 =  1% of the original kernel matrix.

torch.Size([296575, 2])
We keep 5.42e+08/4.83e+10 =  1% of the original kernel matrix.

torch.Size([116814, 2])
We keep 5.19e+07/5.72e+09 =  0% of the original kernel matrix.

torch.Size([119611, 2])
We keep 7.64e+07/5.27e+09 =  1% of the original kernel matrix.

torch.Size([72523, 2])
We keep 1.95e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([20210, 2])
We keep 4.81e+06/1.18e+08 =  4% of the original kernel matrix.

torch.Size([30102, 2])
We keep 4.08e+06/2.83e+08 =  1% of the original kernel matrix.

torch.Size([57029, 2])
We keep 4.00e+07/1.33e+09 =  2% of the original kernel matrix.

torch.Size([51247, 2])
We keep 1.09e+07/9.50e+08 =  1% of the original kernel matrix.

torch.Size([118284, 2])
We keep 3.51e+08/7.24e+09 =  4% of the original kernel matrix.

torch.Size([71967, 2])
We keep 2.25e+07/2.21e+09 =  1% of the original kernel matrix.

torch.Size([145464, 2])
We keep 1.22e+08/8.94e+09 =  1% of the original kernel matrix.

torch.Size([80814, 2])
We keep 2.47e+07/2.46e+09 =  1% of the original kernel matrix.

torch.Size([711225, 2])
We keep 2.52e+09/2.72e+11 =  0% of the original kernel matrix.

torch.Size([187096, 2])
We keep 1.12e+08/1.36e+10 =  0% of the original kernel matrix.

torch.Size([200626, 2])
We keep 1.64e+08/1.45e+10 =  1% of the original kernel matrix.

torch.Size([96166, 2])
We keep 3.02e+07/3.14e+09 =  0% of the original kernel matrix.

torch.Size([12842, 2])
We keep 1.38e+06/3.69e+07 =  3% of the original kernel matrix.

torch.Size([24239, 2])
We keep 2.71e+06/1.58e+08 =  1% of the original kernel matrix.

torch.Size([63341, 2])
We keep 1.62e+07/1.37e+09 =  1% of the original kernel matrix.

torch.Size([53607, 2])
We keep 1.10e+07/9.63e+08 =  1% of the original kernel matrix.

torch.Size([40606, 2])
We keep 3.27e+07/1.13e+09 =  2% of the original kernel matrix.

torch.Size([41343, 2])
We keep 1.04e+07/8.75e+08 =  1% of the original kernel matrix.

torch.Size([17340, 2])
We keep 2.25e+06/7.19e+07 =  3% of the original kernel matrix.

torch.Size([27762, 2])
We keep 3.48e+06/2.21e+08 =  1% of the original kernel matrix.

torch.Size([23716, 2])
We keep 3.77e+06/1.88e+08 =  2% of the original kernel matrix.

torch.Size([32445, 2])
We keep 5.00e+06/3.57e+08 =  1% of the original kernel matrix.

torch.Size([15730, 2])
We keep 1.36e+06/5.17e+07 =  2% of the original kernel matrix.

torch.Size([26653, 2])
We keep 3.07e+06/1.87e+08 =  1% of the original kernel matrix.

torch.Size([54226, 2])
We keep 2.93e+07/9.92e+08 =  2% of the original kernel matrix.

torch.Size([49910, 2])
We keep 9.69e+06/8.20e+08 =  1% of the original kernel matrix.

torch.Size([114584, 2])
We keep 9.07e+07/5.69e+09 =  1% of the original kernel matrix.

torch.Size([69409, 2])
We keep 2.00e+07/1.96e+09 =  1% of the original kernel matrix.

torch.Size([14733, 2])
We keep 2.91e+06/6.46e+07 =  4% of the original kernel matrix.

torch.Size([25227, 2])
We keep 3.32e+06/2.09e+08 =  1% of the original kernel matrix.

torch.Size([44313, 2])
We keep 1.03e+07/7.05e+08 =  1% of the original kernel matrix.

torch.Size([45227, 2])
We keep 8.46e+06/6.91e+08 =  1% of the original kernel matrix.

torch.Size([16782, 2])
We keep 2.33e+06/8.83e+07 =  2% of the original kernel matrix.

torch.Size([27469, 2])
We keep 3.69e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([32787, 2])
We keep 2.12e+07/7.36e+08 =  2% of the original kernel matrix.

torch.Size([37536, 2])
We keep 8.66e+06/7.06e+08 =  1% of the original kernel matrix.

torch.Size([15002, 2])
We keep 1.98e+06/6.06e+07 =  3% of the original kernel matrix.

torch.Size([25745, 2])
We keep 3.29e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([42550, 2])
We keep 1.84e+07/8.69e+08 =  2% of the original kernel matrix.

torch.Size([44291, 2])
We keep 9.49e+06/7.67e+08 =  1% of the original kernel matrix.

torch.Size([23465, 2])
We keep 3.89e+07/4.81e+08 =  8% of the original kernel matrix.

torch.Size([32496, 2])
We keep 7.24e+06/5.71e+08 =  1% of the original kernel matrix.

torch.Size([27051, 2])
We keep 5.58e+06/2.44e+08 =  2% of the original kernel matrix.

torch.Size([34959, 2])
We keep 5.57e+06/4.07e+08 =  1% of the original kernel matrix.

torch.Size([16314, 2])
We keep 2.03e+06/6.53e+07 =  3% of the original kernel matrix.

torch.Size([27098, 2])
We keep 3.36e+06/2.10e+08 =  1% of the original kernel matrix.

torch.Size([22269, 2])
We keep 2.69e+06/1.31e+08 =  2% of the original kernel matrix.

torch.Size([31831, 2])
We keep 4.38e+06/2.98e+08 =  1% of the original kernel matrix.

torch.Size([57539, 2])
We keep 1.64e+07/1.11e+09 =  1% of the original kernel matrix.

torch.Size([51722, 2])
We keep 1.00e+07/8.68e+08 =  1% of the original kernel matrix.

torch.Size([216465, 2])
We keep 2.85e+08/1.89e+10 =  1% of the original kernel matrix.

torch.Size([100216, 2])
We keep 3.40e+07/3.58e+09 =  0% of the original kernel matrix.

torch.Size([15381, 2])
We keep 1.50e+06/5.29e+07 =  2% of the original kernel matrix.

torch.Size([26260, 2])
We keep 3.12e+06/1.89e+08 =  1% of the original kernel matrix.

torch.Size([7564, 2])
We keep 4.77e+05/1.07e+07 =  4% of the original kernel matrix.

torch.Size([19021, 2])
We keep 1.75e+06/8.52e+07 =  2% of the original kernel matrix.

torch.Size([25297, 2])
We keep 4.48e+06/1.99e+08 =  2% of the original kernel matrix.

torch.Size([33950, 2])
We keep 5.14e+06/3.67e+08 =  1% of the original kernel matrix.

torch.Size([142740, 2])
We keep 1.12e+08/7.87e+09 =  1% of the original kernel matrix.

torch.Size([79950, 2])
We keep 2.28e+07/2.31e+09 =  0% of the original kernel matrix.

torch.Size([1029007, 2])
We keep 3.13e+09/3.71e+11 =  0% of the original kernel matrix.

torch.Size([234286, 2])
We keep 1.28e+08/1.59e+10 =  0% of the original kernel matrix.

torch.Size([173331, 2])
We keep 2.72e+08/1.83e+10 =  1% of the original kernel matrix.

torch.Size([86454, 2])
We keep 3.32e+07/3.52e+09 =  0% of the original kernel matrix.

torch.Size([15762, 2])
We keep 1.75e+06/6.03e+07 =  2% of the original kernel matrix.

torch.Size([26496, 2])
We keep 3.27e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([401484, 2])
We keep 1.23e+09/9.39e+10 =  1% of the original kernel matrix.

torch.Size([136742, 2])
We keep 6.96e+07/7.98e+09 =  0% of the original kernel matrix.

torch.Size([342631, 2])
We keep 3.40e+08/5.00e+10 =  0% of the original kernel matrix.

torch.Size([130463, 2])
We keep 5.22e+07/5.82e+09 =  0% of the original kernel matrix.

torch.Size([8103, 2])
We keep 4.38e+05/1.10e+07 =  4% of the original kernel matrix.

torch.Size([19412, 2])
We keep 1.76e+06/8.62e+07 =  2% of the original kernel matrix.

torch.Size([17868, 2])
We keep 3.18e+06/1.04e+08 =  3% of the original kernel matrix.

torch.Size([28278, 2])
We keep 3.98e+06/2.66e+08 =  1% of the original kernel matrix.

torch.Size([110329, 2])
We keep 4.25e+07/4.12e+09 =  1% of the original kernel matrix.

torch.Size([69707, 2])
We keep 1.76e+07/1.67e+09 =  1% of the original kernel matrix.

torch.Size([119199, 2])
We keep 4.17e+08/1.30e+10 =  3% of the original kernel matrix.

torch.Size([71348, 2])
We keep 2.92e+07/2.97e+09 =  0% of the original kernel matrix.

torch.Size([310901, 2])
We keep 1.89e+09/7.27e+10 =  2% of the original kernel matrix.

torch.Size([117670, 2])
We keep 6.07e+07/7.02e+09 =  0% of the original kernel matrix.

torch.Size([307143, 2])
We keep 3.41e+08/4.10e+10 =  0% of the original kernel matrix.

torch.Size([121182, 2])
We keep 4.78e+07/5.27e+09 =  0% of the original kernel matrix.

torch.Size([74544, 2])
We keep 3.96e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([57937, 2])
We keep 1.22e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([18412, 2])
We keep 2.15e+06/8.84e+07 =  2% of the original kernel matrix.

torch.Size([28832, 2])
We keep 3.76e+06/2.45e+08 =  1% of the original kernel matrix.

torch.Size([8311, 2])
We keep 4.24e+05/1.10e+07 =  3% of the original kernel matrix.

torch.Size([19773, 2])
We keep 1.75e+06/8.63e+07 =  2% of the original kernel matrix.

torch.Size([131172, 2])
We keep 9.16e+07/6.33e+09 =  1% of the original kernel matrix.

torch.Size([75776, 2])
We keep 2.09e+07/2.07e+09 =  1% of the original kernel matrix.

torch.Size([31686, 2])
We keep 6.33e+06/3.11e+08 =  2% of the original kernel matrix.

torch.Size([38326, 2])
We keep 6.05e+06/4.59e+08 =  1% of the original kernel matrix.

torch.Size([27727, 2])
We keep 1.20e+07/3.27e+08 =  3% of the original kernel matrix.

torch.Size([35147, 2])
We keep 6.24e+06/4.71e+08 =  1% of the original kernel matrix.

torch.Size([199332, 2])
We keep 2.13e+08/1.86e+10 =  1% of the original kernel matrix.

torch.Size([95369, 2])
We keep 3.39e+07/3.55e+09 =  0% of the original kernel matrix.

torch.Size([1023844, 2])
We keep 2.21e+09/4.02e+11 =  0% of the original kernel matrix.

torch.Size([233274, 2])
We keep 1.33e+08/1.65e+10 =  0% of the original kernel matrix.

torch.Size([22199, 2])
We keep 4.29e+06/1.67e+08 =  2% of the original kernel matrix.

torch.Size([31458, 2])
We keep 4.84e+06/3.37e+08 =  1% of the original kernel matrix.

torch.Size([6140, 2])
We keep 2.83e+05/6.04e+06 =  4% of the original kernel matrix.

torch.Size([17422, 2])
We keep 1.42e+06/6.40e+07 =  2% of the original kernel matrix.

torch.Size([29424, 2])
We keep 1.35e+07/3.31e+08 =  4% of the original kernel matrix.

torch.Size([36721, 2])
We keep 6.35e+06/4.74e+08 =  1% of the original kernel matrix.

torch.Size([158845, 2])
We keep 4.30e+08/1.02e+10 =  4% of the original kernel matrix.

torch.Size([85559, 2])
We keep 2.56e+07/2.63e+09 =  0% of the original kernel matrix.

torch.Size([88695, 2])
We keep 5.59e+07/2.73e+09 =  2% of the original kernel matrix.

torch.Size([62713, 2])
We keep 1.47e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([17112, 2])
We keep 1.31e+07/1.28e+08 = 10% of the original kernel matrix.

torch.Size([27344, 2])
We keep 4.36e+06/2.94e+08 =  1% of the original kernel matrix.

torch.Size([7015, 2])
We keep 4.45e+05/9.50e+06 =  4% of the original kernel matrix.

torch.Size([18264, 2])
We keep 1.68e+06/8.03e+07 =  2% of the original kernel matrix.

torch.Size([68379, 2])
We keep 2.46e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([55334, 2])
We keep 1.19e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([15789, 2])
We keep 2.41e+06/6.29e+07 =  3% of the original kernel matrix.

torch.Size([26577, 2])
We keep 3.32e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([63020, 2])
We keep 5.99e+07/2.38e+09 =  2% of the original kernel matrix.

torch.Size([52144, 2])
We keep 1.43e+07/1.27e+09 =  1% of the original kernel matrix.

torch.Size([15337, 2])
We keep 1.82e+06/5.93e+07 =  3% of the original kernel matrix.

torch.Size([26286, 2])
We keep 3.27e+06/2.01e+08 =  1% of the original kernel matrix.

torch.Size([31914, 2])
We keep 5.54e+06/3.56e+08 =  1% of the original kernel matrix.

torch.Size([38924, 2])
We keep 6.52e+06/4.91e+08 =  1% of the original kernel matrix.

torch.Size([32166, 2])
We keep 6.13e+06/3.42e+08 =  1% of the original kernel matrix.

torch.Size([38775, 2])
We keep 6.39e+06/4.82e+08 =  1% of the original kernel matrix.

torch.Size([22429, 2])
We keep 2.46e+06/1.29e+08 =  1% of the original kernel matrix.

torch.Size([31874, 2])
We keep 4.32e+06/2.96e+08 =  1% of the original kernel matrix.

torch.Size([38651, 2])
We keep 8.84e+06/5.09e+08 =  1% of the original kernel matrix.

torch.Size([41315, 2])
We keep 7.18e+06/5.87e+08 =  1% of the original kernel matrix.

torch.Size([296998, 2])
We keep 2.82e+08/3.38e+10 =  0% of the original kernel matrix.

torch.Size([119105, 2])
We keep 4.34e+07/4.78e+09 =  0% of the original kernel matrix.

torch.Size([596618, 2])
We keep 9.97e+08/1.32e+11 =  0% of the original kernel matrix.

torch.Size([167159, 2])
We keep 8.05e+07/9.47e+09 =  0% of the original kernel matrix.

torch.Size([151326, 2])
We keep 1.52e+08/9.09e+09 =  1% of the original kernel matrix.

torch.Size([83031, 2])
We keep 2.46e+07/2.48e+09 =  0% of the original kernel matrix.

torch.Size([223370, 2])
We keep 3.49e+08/2.27e+10 =  1% of the original kernel matrix.

torch.Size([101731, 2])
We keep 3.72e+07/3.92e+09 =  0% of the original kernel matrix.

torch.Size([39395, 2])
We keep 8.35e+06/5.55e+08 =  1% of the original kernel matrix.

torch.Size([43769, 2])
We keep 7.81e+06/6.13e+08 =  1% of the original kernel matrix.

torch.Size([33263, 2])
We keep 8.90e+06/3.66e+08 =  2% of the original kernel matrix.

torch.Size([39183, 2])
We keep 6.53e+06/4.98e+08 =  1% of the original kernel matrix.

torch.Size([41752, 2])
We keep 1.48e+07/7.69e+08 =  1% of the original kernel matrix.

torch.Size([43964, 2])
We keep 8.89e+06/7.22e+08 =  1% of the original kernel matrix.

torch.Size([252199, 2])
We keep 2.24e+08/2.56e+10 =  0% of the original kernel matrix.

torch.Size([108362, 2])
We keep 3.89e+07/4.16e+09 =  0% of the original kernel matrix.

torch.Size([41635, 2])
We keep 1.94e+07/8.60e+08 =  2% of the original kernel matrix.

torch.Size([43505, 2])
We keep 9.41e+06/7.64e+08 =  1% of the original kernel matrix.

torch.Size([59996, 2])
We keep 7.35e+07/1.58e+09 =  4% of the original kernel matrix.

torch.Size([52112, 2])
We keep 1.18e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([28454, 2])
We keep 4.55e+06/2.42e+08 =  1% of the original kernel matrix.

torch.Size([36132, 2])
We keep 5.55e+06/4.05e+08 =  1% of the original kernel matrix.

torch.Size([2404091, 2])
We keep 1.03e+10/1.86e+12 =  0% of the original kernel matrix.

torch.Size([361356, 2])
We keep 2.71e+08/3.55e+10 =  0% of the original kernel matrix.

torch.Size([43488, 2])
We keep 1.03e+07/6.45e+08 =  1% of the original kernel matrix.

torch.Size([44880, 2])
We keep 8.11e+06/6.61e+08 =  1% of the original kernel matrix.

torch.Size([124765, 2])
We keep 7.03e+07/6.00e+09 =  1% of the original kernel matrix.

torch.Size([74060, 2])
We keep 2.06e+07/2.02e+09 =  1% of the original kernel matrix.

torch.Size([261979, 2])
We keep 3.30e+08/3.09e+10 =  1% of the original kernel matrix.

torch.Size([110540, 2])
We keep 4.16e+07/4.58e+09 =  0% of the original kernel matrix.

torch.Size([105513, 2])
We keep 5.42e+07/3.76e+09 =  1% of the original kernel matrix.

torch.Size([68675, 2])
We keep 1.69e+07/1.60e+09 =  1% of the original kernel matrix.

torch.Size([64626, 2])
We keep 3.38e+07/1.74e+09 =  1% of the original kernel matrix.

torch.Size([54333, 2])
We keep 1.23e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([1824821, 2])
We keep 9.33e+09/1.17e+12 =  0% of the original kernel matrix.

torch.Size([310161, 2])
We keep 2.19e+08/2.82e+10 =  0% of the original kernel matrix.

torch.Size([359582, 2])
We keep 3.77e+08/5.35e+10 =  0% of the original kernel matrix.

torch.Size([130596, 2])
We keep 5.36e+07/6.02e+09 =  0% of the original kernel matrix.

torch.Size([180257, 2])
We keep 1.83e+08/1.42e+10 =  1% of the original kernel matrix.

torch.Size([91012, 2])
We keep 3.01e+07/3.11e+09 =  0% of the original kernel matrix.

torch.Size([21187, 2])
We keep 5.34e+06/1.37e+08 =  3% of the original kernel matrix.

torch.Size([31041, 2])
We keep 4.30e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([82010, 2])
We keep 5.34e+07/2.75e+09 =  1% of the original kernel matrix.

torch.Size([60785, 2])
We keep 1.50e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([19651, 2])
We keep 3.33e+06/1.07e+08 =  3% of the original kernel matrix.

torch.Size([29946, 2])
We keep 3.91e+06/2.70e+08 =  1% of the original kernel matrix.

torch.Size([348191, 2])
We keep 1.08e+09/5.41e+10 =  2% of the original kernel matrix.

torch.Size([129604, 2])
We keep 5.44e+07/6.05e+09 =  0% of the original kernel matrix.

torch.Size([2132601, 2])
We keep 8.98e+09/1.49e+12 =  0% of the original kernel matrix.

torch.Size([337381, 2])
We keep 2.44e+08/3.17e+10 =  0% of the original kernel matrix.

torch.Size([147058, 2])
We keep 3.33e+08/1.05e+10 =  3% of the original kernel matrix.

torch.Size([81654, 2])
We keep 2.68e+07/2.67e+09 =  1% of the original kernel matrix.

torch.Size([232308, 2])
We keep 2.87e+08/2.71e+10 =  1% of the original kernel matrix.

torch.Size([103100, 2])
We keep 4.03e+07/4.29e+09 =  0% of the original kernel matrix.

torch.Size([154461, 2])
We keep 3.15e+08/1.07e+10 =  2% of the original kernel matrix.

torch.Size([83634, 2])
We keep 2.67e+07/2.69e+09 =  0% of the original kernel matrix.

torch.Size([44814, 2])
We keep 2.77e+07/1.01e+09 =  2% of the original kernel matrix.

torch.Size([44983, 2])
We keep 1.01e+07/8.28e+08 =  1% of the original kernel matrix.

torch.Size([56512, 2])
We keep 2.98e+07/1.46e+09 =  2% of the original kernel matrix.

torch.Size([50171, 2])
We keep 1.16e+07/9.97e+08 =  1% of the original kernel matrix.

torch.Size([24166, 2])
We keep 5.33e+06/2.09e+08 =  2% of the original kernel matrix.

torch.Size([32815, 2])
We keep 5.26e+06/3.77e+08 =  1% of the original kernel matrix.

torch.Size([150846, 2])
We keep 1.03e+09/2.66e+10 =  3% of the original kernel matrix.

torch.Size([81499, 2])
We keep 3.91e+07/4.25e+09 =  0% of the original kernel matrix.

torch.Size([907494, 2])
We keep 1.69e+09/3.06e+11 =  0% of the original kernel matrix.

torch.Size([215469, 2])
We keep 1.17e+08/1.44e+10 =  0% of the original kernel matrix.

torch.Size([110741, 2])
We keep 1.59e+08/1.01e+10 =  1% of the original kernel matrix.

torch.Size([66763, 2])
We keep 2.61e+07/2.61e+09 =  0% of the original kernel matrix.

torch.Size([1156610, 2])
We keep 1.05e+10/7.54e+11 =  1% of the original kernel matrix.

torch.Size([230466, 2])
We keep 1.77e+08/2.26e+10 =  0% of the original kernel matrix.

torch.Size([209104, 2])
We keep 4.04e+08/2.66e+10 =  1% of the original kernel matrix.

torch.Size([97855, 2])
We keep 3.99e+07/4.25e+09 =  0% of the original kernel matrix.

torch.Size([97648, 2])
We keep 4.33e+07/3.58e+09 =  1% of the original kernel matrix.

torch.Size([65673, 2])
We keep 1.66e+07/1.56e+09 =  1% of the original kernel matrix.

torch.Size([45730, 2])
We keep 1.96e+07/8.94e+08 =  2% of the original kernel matrix.

torch.Size([45297, 2])
We keep 9.42e+06/7.79e+08 =  1% of the original kernel matrix.

torch.Size([175222, 2])
We keep 2.85e+08/1.35e+10 =  2% of the original kernel matrix.

torch.Size([89542, 2])
We keep 2.91e+07/3.03e+09 =  0% of the original kernel matrix.

torch.Size([306705, 2])
We keep 3.08e+08/3.86e+10 =  0% of the original kernel matrix.

torch.Size([121692, 2])
We keep 4.59e+07/5.12e+09 =  0% of the original kernel matrix.

torch.Size([22636, 2])
We keep 7.19e+06/2.39e+08 =  3% of the original kernel matrix.

torch.Size([31758, 2])
We keep 5.57e+06/4.02e+08 =  1% of the original kernel matrix.

torch.Size([1088663, 2])
We keep 2.30e+09/4.35e+11 =  0% of the original kernel matrix.

torch.Size([238259, 2])
We keep 1.38e+08/1.72e+10 =  0% of the original kernel matrix.

torch.Size([40194, 2])
We keep 1.70e+07/7.03e+08 =  2% of the original kernel matrix.

torch.Size([42982, 2])
We keep 8.20e+06/6.90e+08 =  1% of the original kernel matrix.

torch.Size([69946, 2])
We keep 4.40e+07/2.36e+09 =  1% of the original kernel matrix.

torch.Size([55290, 2])
We keep 1.41e+07/1.26e+09 =  1% of the original kernel matrix.

torch.Size([13335, 2])
We keep 1.59e+06/4.83e+07 =  3% of the original kernel matrix.

torch.Size([24753, 2])
We keep 2.87e+06/1.81e+08 =  1% of the original kernel matrix.

torch.Size([30189, 2])
We keep 4.80e+06/2.88e+08 =  1% of the original kernel matrix.

torch.Size([37477, 2])
We keep 5.90e+06/4.42e+08 =  1% of the original kernel matrix.

torch.Size([6533, 2])
We keep 3.42e+05/7.51e+06 =  4% of the original kernel matrix.

torch.Size([17696, 2])
We keep 1.54e+06/7.14e+07 =  2% of the original kernel matrix.

torch.Size([13811, 2])
We keep 1.84e+06/4.87e+07 =  3% of the original kernel matrix.

torch.Size([24536, 2])
We keep 2.95e+06/1.82e+08 =  1% of the original kernel matrix.

torch.Size([41507, 2])
We keep 1.33e+07/6.12e+08 =  2% of the original kernel matrix.

torch.Size([44462, 2])
We keep 8.22e+06/6.44e+08 =  1% of the original kernel matrix.

torch.Size([131976, 2])
We keep 2.66e+08/8.40e+09 =  3% of the original kernel matrix.

torch.Size([75999, 2])
We keep 2.35e+07/2.39e+09 =  0% of the original kernel matrix.

torch.Size([4687456, 2])
We keep 6.13e+10/9.32e+12 =  0% of the original kernel matrix.

torch.Size([486402, 2])
We keep 5.80e+08/7.95e+10 =  0% of the original kernel matrix.

torch.Size([27478, 2])
We keep 9.58e+06/3.26e+08 =  2% of the original kernel matrix.

torch.Size([34727, 2])
We keep 6.28e+06/4.70e+08 =  1% of the original kernel matrix.

torch.Size([644514, 2])
We keep 9.74e+08/1.47e+11 =  0% of the original kernel matrix.

torch.Size([174079, 2])
We keep 8.34e+07/9.97e+09 =  0% of the original kernel matrix.

torch.Size([21817, 2])
We keep 3.11e+06/1.37e+08 =  2% of the original kernel matrix.

torch.Size([31456, 2])
We keep 4.45e+06/3.04e+08 =  1% of the original kernel matrix.

torch.Size([40027, 2])
We keep 1.03e+07/6.26e+08 =  1% of the original kernel matrix.

torch.Size([41868, 2])
We keep 7.94e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([15115, 2])
We keep 2.17e+06/6.28e+07 =  3% of the original kernel matrix.

torch.Size([25818, 2])
We keep 3.33e+06/2.06e+08 =  1% of the original kernel matrix.

torch.Size([228529, 2])
We keep 4.90e+08/3.50e+10 =  1% of the original kernel matrix.

torch.Size([101716, 2])
We keep 4.47e+07/4.87e+09 =  0% of the original kernel matrix.

torch.Size([59504, 2])
We keep 8.60e+07/2.10e+09 =  4% of the original kernel matrix.

torch.Size([50206, 2])
We keep 1.32e+07/1.19e+09 =  1% of the original kernel matrix.

torch.Size([1830807, 2])
We keep 5.88e+09/1.07e+12 =  0% of the original kernel matrix.

torch.Size([305631, 2])
We keep 2.09e+08/2.69e+10 =  0% of the original kernel matrix.

torch.Size([16252, 2])
We keep 1.58e+06/5.87e+07 =  2% of the original kernel matrix.

torch.Size([26921, 2])
We keep 3.23e+06/1.99e+08 =  1% of the original kernel matrix.

torch.Size([296482, 2])
We keep 3.47e+08/3.78e+10 =  0% of the original kernel matrix.

torch.Size([120206, 2])
We keep 4.61e+07/5.06e+09 =  0% of the original kernel matrix.

torch.Size([44904, 2])
We keep 1.89e+07/7.33e+08 =  2% of the original kernel matrix.

torch.Size([45706, 2])
We keep 8.79e+06/7.05e+08 =  1% of the original kernel matrix.

torch.Size([232795, 2])
We keep 2.39e+08/2.07e+10 =  1% of the original kernel matrix.

torch.Size([103638, 2])
We keep 3.51e+07/3.75e+09 =  0% of the original kernel matrix.

torch.Size([338078, 2])
We keep 6.38e+08/6.03e+10 =  1% of the original kernel matrix.

torch.Size([126112, 2])
We keep 5.62e+07/6.39e+09 =  0% of the original kernel matrix.

torch.Size([50748, 2])
We keep 1.41e+07/8.78e+08 =  1% of the original kernel matrix.

torch.Size([48174, 2])
We keep 9.16e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([28017, 2])
We keep 5.41e+06/2.61e+08 =  2% of the original kernel matrix.

torch.Size([36013, 2])
We keep 5.71e+06/4.20e+08 =  1% of the original kernel matrix.

torch.Size([197325, 2])
We keep 1.34e+08/1.54e+10 =  0% of the original kernel matrix.

torch.Size([95584, 2])
We keep 3.10e+07/3.23e+09 =  0% of the original kernel matrix.

torch.Size([36703, 2])
We keep 2.09e+07/4.95e+08 =  4% of the original kernel matrix.

torch.Size([41068, 2])
We keep 7.24e+06/5.79e+08 =  1% of the original kernel matrix.

torch.Size([59747, 2])
We keep 3.37e+07/1.57e+09 =  2% of the original kernel matrix.

torch.Size([51415, 2])
We keep 1.18e+07/1.03e+09 =  1% of the original kernel matrix.

torch.Size([527424, 2])
We keep 1.15e+09/1.07e+11 =  1% of the original kernel matrix.

torch.Size([155866, 2])
We keep 7.22e+07/8.51e+09 =  0% of the original kernel matrix.

torch.Size([27052, 2])
We keep 9.67e+06/2.92e+08 =  3% of the original kernel matrix.

torch.Size([34972, 2])
We keep 5.91e+06/4.45e+08 =  1% of the original kernel matrix.

torch.Size([123118, 2])
We keep 1.50e+08/7.36e+09 =  2% of the original kernel matrix.

torch.Size([73744, 2])
We keep 2.30e+07/2.23e+09 =  1% of the original kernel matrix.

torch.Size([58162, 2])
We keep 2.58e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([50389, 2])
We keep 1.11e+07/9.70e+08 =  1% of the original kernel matrix.

torch.Size([198599, 2])
We keep 1.93e+08/1.73e+10 =  1% of the original kernel matrix.

torch.Size([95285, 2])
We keep 3.26e+07/3.43e+09 =  0% of the original kernel matrix.

torch.Size([377617, 2])
We keep 7.36e+08/7.20e+10 =  1% of the original kernel matrix.

torch.Size([133128, 2])
We keep 6.15e+07/6.99e+09 =  0% of the original kernel matrix.

torch.Size([116790, 2])
We keep 6.93e+07/4.51e+09 =  1% of the original kernel matrix.

torch.Size([71253, 2])
We keep 1.83e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([26726, 2])
We keep 7.19e+06/2.55e+08 =  2% of the original kernel matrix.

torch.Size([35264, 2])
We keep 5.77e+06/4.15e+08 =  1% of the original kernel matrix.

torch.Size([200113, 2])
We keep 2.48e+08/1.84e+10 =  1% of the original kernel matrix.

torch.Size([94721, 2])
We keep 3.33e+07/3.53e+09 =  0% of the original kernel matrix.

torch.Size([165561, 2])
We keep 1.31e+08/1.11e+10 =  1% of the original kernel matrix.

torch.Size([85418, 2])
We keep 2.68e+07/2.74e+09 =  0% of the original kernel matrix.

torch.Size([333930, 2])
We keep 3.17e+09/1.21e+11 =  2% of the original kernel matrix.

torch.Size([118606, 2])
We keep 7.64e+07/9.04e+09 =  0% of the original kernel matrix.

torch.Size([121093, 2])
We keep 1.26e+08/6.08e+09 =  2% of the original kernel matrix.

torch.Size([72488, 2])
We keep 2.07e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([58119, 2])
We keep 3.96e+07/1.63e+09 =  2% of the original kernel matrix.

torch.Size([50303, 2])
We keep 1.17e+07/1.05e+09 =  1% of the original kernel matrix.

torch.Size([69648, 2])
We keep 5.05e+07/2.13e+09 =  2% of the original kernel matrix.

torch.Size([56272, 2])
We keep 1.35e+07/1.20e+09 =  1% of the original kernel matrix.

torch.Size([232434, 2])
We keep 4.95e+08/3.23e+10 =  1% of the original kernel matrix.

torch.Size([100297, 2])
We keep 4.27e+07/4.68e+09 =  0% of the original kernel matrix.

torch.Size([184614, 2])
We keep 3.26e+08/1.87e+10 =  1% of the original kernel matrix.

torch.Size([91033, 2])
We keep 3.38e+07/3.56e+09 =  0% of the original kernel matrix.

torch.Size([68755, 2])
We keep 2.58e+07/1.65e+09 =  1% of the original kernel matrix.

torch.Size([55723, 2])
We keep 1.20e+07/1.06e+09 =  1% of the original kernel matrix.

torch.Size([15707, 2])
We keep 1.68e+06/6.02e+07 =  2% of the original kernel matrix.

torch.Size([26171, 2])
We keep 3.24e+06/2.02e+08 =  1% of the original kernel matrix.

torch.Size([18282, 2])
We keep 2.37e+06/9.67e+07 =  2% of the original kernel matrix.

torch.Size([28742, 2])
We keep 3.98e+06/2.56e+08 =  1% of the original kernel matrix.

torch.Size([95448, 2])
We keep 9.92e+07/4.51e+09 =  2% of the original kernel matrix.

torch.Size([63679, 2])
We keep 1.84e+07/1.75e+09 =  1% of the original kernel matrix.

torch.Size([159934, 2])
We keep 2.80e+08/1.28e+10 =  2% of the original kernel matrix.

torch.Size([84232, 2])
We keep 2.87e+07/2.95e+09 =  0% of the original kernel matrix.

torch.Size([152829, 2])
We keep 8.99e+07/8.25e+09 =  1% of the original kernel matrix.

torch.Size([83110, 2])
We keep 2.37e+07/2.37e+09 =  1% of the original kernel matrix.

torch.Size([24029, 2])
We keep 4.26e+06/1.95e+08 =  2% of the original kernel matrix.

torch.Size([33209, 2])
We keep 5.15e+06/3.63e+08 =  1% of the original kernel matrix.

torch.Size([48569, 2])
We keep 1.16e+07/8.78e+08 =  1% of the original kernel matrix.

torch.Size([47102, 2])
We keep 9.24e+06/7.71e+08 =  1% of the original kernel matrix.

torch.Size([68298, 2])
We keep 3.50e+07/1.69e+09 =  2% of the original kernel matrix.

torch.Size([55312, 2])
We keep 1.22e+07/1.07e+09 =  1% of the original kernel matrix.

torch.Size([94417, 2])
We keep 3.54e+07/3.08e+09 =  1% of the original kernel matrix.

torch.Size([64782, 2])
We keep 1.54e+07/1.44e+09 =  1% of the original kernel matrix.

torch.Size([45416, 2])
We keep 1.82e+07/7.91e+08 =  2% of the original kernel matrix.

torch.Size([46237, 2])
We keep 9.05e+06/7.32e+08 =  1% of the original kernel matrix.

torch.Size([124711, 2])
We keep 5.48e+07/5.03e+09 =  1% of the original kernel matrix.

torch.Size([73919, 2])
We keep 1.92e+07/1.85e+09 =  1% of the original kernel matrix.

torch.Size([16700, 2])
We keep 5.01e+06/1.19e+08 =  4% of the original kernel matrix.

torch.Size([27046, 2])
We keep 4.24e+06/2.84e+08 =  1% of the original kernel matrix.

torch.Size([82882, 2])
We keep 2.87e+07/2.22e+09 =  1% of the original kernel matrix.

torch.Size([60820, 2])
We keep 1.35e+07/1.23e+09 =  1% of the original kernel matrix.

torch.Size([10384, 2])
We keep 9.36e+05/2.27e+07 =  4% of the original kernel matrix.

torch.Size([21669, 2])
We keep 2.29e+06/1.24e+08 =  1% of the original kernel matrix.

torch.Size([20551, 2])
We keep 2.47e+06/1.12e+08 =  2% of the original kernel matrix.

torch.Size([30571, 2])
We keep 4.13e+06/2.76e+08 =  1% of the original kernel matrix.

torch.Size([420521, 2])
We keep 6.49e+08/7.03e+10 =  0% of the original kernel matrix.

torch.Size([143029, 2])
We keep 6.04e+07/6.91e+09 =  0% of the original kernel matrix.

torch.Size([35132, 2])
We keep 9.04e+06/4.49e+08 =  2% of the original kernel matrix.

torch.Size([40106, 2])
We keep 7.16e+06/5.52e+08 =  1% of the original kernel matrix.

torch.Size([173809, 2])
We keep 1.30e+08/1.15e+10 =  1% of the original kernel matrix.

torch.Size([89518, 2])
We keep 2.73e+07/2.79e+09 =  0% of the original kernel matrix.

torch.Size([14139, 2])
We keep 1.46e+06/4.30e+07 =  3% of the original kernel matrix.

torch.Size([25113, 2])
We keep 2.89e+06/1.71e+08 =  1% of the original kernel matrix.

torch.Size([15052, 2])
We keep 3.45e+06/6.07e+07 =  5% of the original kernel matrix.

torch.Size([25944, 2])
We keep 3.29e+06/2.03e+08 =  1% of the original kernel matrix.

torch.Size([67550, 2])
We keep 4.06e+07/1.79e+09 =  2% of the original kernel matrix.

torch.Size([54269, 2])
We keep 1.21e+07/1.10e+09 =  1% of the original kernel matrix.

torch.Size([146324, 2])
We keep 9.89e+07/9.34e+09 =  1% of the original kernel matrix.

torch.Size([80810, 2])
We keep 2.52e+07/2.52e+09 =  1% of the original kernel matrix.

torch.Size([68800, 2])
We keep 2.67e+07/1.89e+09 =  1% of the original kernel matrix.

torch.Size([55834, 2])
We keep 1.28e+07/1.13e+09 =  1% of the original kernel matrix.

torch.Size([15358, 2])
We keep 1.75e+06/5.01e+07 =  3% of the original kernel matrix.

torch.Size([26304, 2])
We keep 3.04e+06/1.84e+08 =  1% of the original kernel matrix.

torch.Size([26446, 2])
We keep 4.42e+06/2.14e+08 =  2% of the original kernel matrix.

torch.Size([34718, 2])
We keep 5.29e+06/3.81e+08 =  1% of the original kernel matrix.

torch.Size([73218, 2])
We keep 4.73e+07/2.17e+09 =  2% of the original kernel matrix.

torch.Size([57311, 2])
We keep 1.36e+07/1.21e+09 =  1% of the original kernel matrix.

torch.Size([44701, 2])
We keep 1.26e+07/7.85e+08 =  1% of the original kernel matrix.

torch.Size([45611, 2])
We keep 9.01e+06/7.30e+08 =  1% of the original kernel matrix.

torch.Size([167827, 2])
We keep 2.05e+08/1.09e+10 =  1% of the original kernel matrix.

torch.Size([87278, 2])
We keep 2.67e+07/2.72e+09 =  0% of the original kernel matrix.

torch.Size([14713, 2])
We keep 1.47e+06/5.37e+07 =  2% of the original kernel matrix.

torch.Size([25800, 2])
We keep 3.09e+06/1.91e+08 =  1% of the original kernel matrix.

torch.Size([16393, 2])
We keep 2.24e+06/7.68e+07 =  2% of the original kernel matrix.

torch.Size([26730, 2])
We keep 3.53e+06/2.28e+08 =  1% of the original kernel matrix.

torch.Size([115724, 2])
We keep 1.37e+08/8.74e+09 =  1% of the original kernel matrix.

torch.Size([69499, 2])
We keep 2.47e+07/2.43e+09 =  1% of the original kernel matrix.

torch.Size([20751, 2])
We keep 3.68e+06/1.26e+08 =  2% of the original kernel matrix.

torch.Size([30323, 2])
We keep 4.31e+06/2.92e+08 =  1% of the original kernel matrix.

torch.Size([293245, 2])
We keep 6.17e+08/4.41e+10 =  1% of the original kernel matrix.

torch.Size([118925, 2])
We keep 4.86e+07/5.47e+09 =  0% of the original kernel matrix.

torch.Size([31892, 2])
We keep 2.42e+07/3.94e+08 =  6% of the original kernel matrix.

torch.Size([38503, 2])
We keep 6.55e+06/5.17e+08 =  1% of the original kernel matrix.

torch.Size([142215, 2])
We keep 1.74e+08/1.00e+10 =  1% of the original kernel matrix.

torch.Size([79332, 2])
We keep 2.57e+07/2.60e+09 =  0% of the original kernel matrix.

torch.Size([263724, 2])
We keep 2.53e+08/2.57e+10 =  0% of the original kernel matrix.

torch.Size([112003, 2])
We keep 3.88e+07/4.17e+09 =  0% of the original kernel matrix.

torch.Size([124306, 2])
We keep 2.04e+08/6.21e+09 =  3% of the original kernel matrix.

torch.Size([74460, 2])
We keep 2.07e+07/2.05e+09 =  1% of the original kernel matrix.

torch.Size([246943, 2])
We keep 3.52e+08/2.80e+10 =  1% of the original kernel matrix.

torch.Size([108206, 2])
We keep 4.07e+07/4.36e+09 =  0% of the original kernel matrix.

torch.Size([237902, 2])
We keep 2.05e+08/2.25e+10 =  0% of the original kernel matrix.

torch.Size([105922, 2])
We keep 3.68e+07/3.91e+09 =  0% of the original kernel matrix.

torch.Size([5730, 2])
We keep 2.70e+05/5.51e+06 =  4% of the original kernel matrix.

torch.Size([17027, 2])
We keep 1.37e+06/6.11e+07 =  2% of the original kernel matrix.

torch.Size([111761, 2])
We keep 5.69e+07/4.65e+09 =  1% of the original kernel matrix.

torch.Size([69779, 2])
We keep 1.87e+07/1.77e+09 =  1% of the original kernel matrix.

torch.Size([40898, 2])
We keep 9.47e+06/6.25e+08 =  1% of the original kernel matrix.

torch.Size([41267, 2])
We keep 7.56e+06/6.51e+08 =  1% of the original kernel matrix.

torch.Size([198834, 2])
We keep 3.02e+08/1.62e+10 =  1% of the original kernel matrix.

torch.Size([95401, 2])
We keep 3.14e+07/3.31e+09 =  0% of the original kernel matrix.

torch.Size([176652, 2])
We keep 1.53e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([89773, 2])
We keep 2.81e+07/2.94e+09 =  0% of the original kernel matrix.

torch.Size([68184, 2])
We keep 6.12e+07/2.26e+09 =  2% of the original kernel matrix.

torch.Size([53347, 2])
We keep 1.37e+07/1.24e+09 =  1% of the original kernel matrix.

torch.Size([125373, 2])
We keep 6.53e+07/6.07e+09 =  1% of the original kernel matrix.

torch.Size([74373, 2])
We keep 2.09e+07/2.03e+09 =  1% of the original kernel matrix.

torch.Size([45641, 2])
We keep 1.29e+07/7.21e+08 =  1% of the original kernel matrix.

torch.Size([45588, 2])
We keep 8.52e+06/6.99e+08 =  1% of the original kernel matrix.

torch.Size([45200, 2])
We keep 4.66e+07/1.72e+09 =  2% of the original kernel matrix.

torch.Size([42933, 2])
We keep 1.22e+07/1.08e+09 =  1% of the original kernel matrix.

torch.Size([13198, 2])
We keep 9.82e+05/3.33e+07 =  2% of the original kernel matrix.

torch.Size([24371, 2])
We keep 2.61e+06/1.50e+08 =  1% of the original kernel matrix.

torch.Size([19718, 2])
We keep 4.04e+06/1.30e+08 =  3% of the original kernel matrix.

torch.Size([29331, 2])
We keep 4.36e+06/2.97e+08 =  1% of the original kernel matrix.

torch.Size([274556, 2])
We keep 5.99e+08/3.11e+10 =  1% of the original kernel matrix.

torch.Size([115071, 2])
We keep 4.24e+07/4.59e+09 =  0% of the original kernel matrix.

torch.Size([808445, 2])
We keep 2.09e+09/2.45e+11 =  0% of the original kernel matrix.

torch.Size([202808, 2])
We keep 1.06e+08/1.29e+10 =  0% of the original kernel matrix.

torch.Size([32793, 2])
We keep 1.12e+07/6.04e+08 =  1% of the original kernel matrix.

torch.Size([36450, 2])
We keep 7.68e+06/6.40e+08 =  1% of the original kernel matrix.

torch.Size([36685, 2])
We keep 7.02e+06/4.58e+08 =  1% of the original kernel matrix.

torch.Size([41554, 2])
We keep 7.02e+06/5.57e+08 =  1% of the original kernel matrix.

torch.Size([57099, 2])
We keep 1.93e+07/1.36e+09 =  1% of the original kernel matrix.

torch.Size([51502, 2])
We keep 1.13e+07/9.61e+08 =  1% of the original kernel matrix.

torch.Size([167148, 2])
We keep 2.04e+08/1.27e+10 =  1% of the original kernel matrix.

torch.Size([87572, 2])
We keep 2.81e+07/2.93e+09 =  0% of the original kernel matrix.

torch.Size([136084, 2])
We keep 6.18e+07/5.84e+09 =  1% of the original kernel matrix.

torch.Size([77287, 2])
We keep 2.03e+07/1.99e+09 =  1% of the original kernel matrix.

torch.Size([19586, 2])
We keep 3.44e+06/1.03e+08 =  3% of the original kernel matrix.

torch.Size([29629, 2])
We keep 3.87e+06/2.64e+08 =  1% of the original kernel matrix.

torch.Size([113249, 2])
We keep 8.81e+07/4.86e+09 =  1% of the original kernel matrix.

torch.Size([70893, 2])
We keep 1.87e+07/1.81e+09 =  1% of the original kernel matrix.

torch.Size([253122, 2])
We keep 2.61e+08/2.65e+10 =  0% of the original kernel matrix.

torch.Size([109405, 2])
We keep 3.96e+07/4.24e+09 =  0% of the original kernel matrix.

torch.Size([17568, 2])
We keep 3.11e+06/9.29e+07 =  3% of the original kernel matrix.

torch.Size([27933, 2])
We keep 3.82e+06/2.51e+08 =  1% of the original kernel matrix.

torch.Size([82557, 2])
We keep 7.15e+07/2.86e+09 =  2% of the original kernel matrix.

torch.Size([60786, 2])
We keep 1.53e+07/1.39e+09 =  1% of the original kernel matrix.

torch.Size([61874, 2])
We keep 9.26e+07/1.67e+09 =  5% of the original kernel matrix.

torch.Size([52728, 2])
We keep 1.18e+07/1.06e+09 =  1% of the original kernel matrix.

time for making ranges is 7.224180459976196
Sorting X and nu_X
time for sorting X is 0.10041451454162598
Sorting Z and nu_Z
time for sorting Z is 0.00027561187744140625
Starting Optim
sum tnu_Z before tensor(53215832., device='cuda:0')
c= tensor(2313.4773, device='cuda:0')
c= tensor(218607.2656, device='cuda:0')
c= tensor(223588.6094, device='cuda:0')
c= tensor(450149.6875, device='cuda:0')
c= tensor(918231.2500, device='cuda:0')
c= tensor(2156030.5000, device='cuda:0')
c= tensor(2923263.2500, device='cuda:0')
c= tensor(3670895.5000, device='cuda:0')
c= tensor(3801036.2500, device='cuda:0')
c= tensor(12089275., device='cuda:0')
c= tensor(12133416., device='cuda:0')
c= tensor(17586932., device='cuda:0')
c= tensor(17624338., device='cuda:0')
c= tensor(69380280., device='cuda:0')
c= tensor(69780000., device='cuda:0')
c= tensor(70649512., device='cuda:0')
c= tensor(72237224., device='cuda:0')
c= tensor(72554520., device='cuda:0')
c= tensor(83334568., device='cuda:0')
c= tensor(86834728., device='cuda:0')
c= tensor(89289776., device='cuda:0')
c= tensor(1.1646e+08, device='cuda:0')
c= tensor(1.1653e+08, device='cuda:0')
c= tensor(1.1815e+08, device='cuda:0')
c= tensor(1.1819e+08, device='cuda:0')
c= tensor(1.1993e+08, device='cuda:0')
c= tensor(1.2181e+08, device='cuda:0')
c= tensor(1.2188e+08, device='cuda:0')
c= tensor(1.2713e+08, device='cuda:0')
c= tensor(7.1809e+08, device='cuda:0')
c= tensor(7.1828e+08, device='cuda:0')
c= tensor(9.3161e+08, device='cuda:0')
c= tensor(9.3226e+08, device='cuda:0')
c= tensor(9.3237e+08, device='cuda:0')
c= tensor(9.3307e+08, device='cuda:0')
c= tensor(9.6011e+08, device='cuda:0')
c= tensor(9.6408e+08, device='cuda:0')
c= tensor(9.6408e+08, device='cuda:0')
c= tensor(9.6409e+08, device='cuda:0')
c= tensor(9.6410e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6412e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6415e+08, device='cuda:0')
c= tensor(9.6416e+08, device='cuda:0')
c= tensor(9.6424e+08, device='cuda:0')
c= tensor(9.6434e+08, device='cuda:0')
c= tensor(9.6434e+08, device='cuda:0')
c= tensor(9.6436e+08, device='cuda:0')
c= tensor(9.6437e+08, device='cuda:0')
c= tensor(9.6439e+08, device='cuda:0')
c= tensor(9.6443e+08, device='cuda:0')
c= tensor(9.6443e+08, device='cuda:0')
c= tensor(9.6447e+08, device='cuda:0')
c= tensor(9.6447e+08, device='cuda:0')
c= tensor(9.6448e+08, device='cuda:0')
c= tensor(9.6451e+08, device='cuda:0')
c= tensor(9.6451e+08, device='cuda:0')
c= tensor(9.6453e+08, device='cuda:0')
c= tensor(9.6461e+08, device='cuda:0')
c= tensor(9.6461e+08, device='cuda:0')
c= tensor(9.6462e+08, device='cuda:0')
c= tensor(9.6462e+08, device='cuda:0')
c= tensor(9.6464e+08, device='cuda:0')
c= tensor(9.6469e+08, device='cuda:0')
c= tensor(9.6471e+08, device='cuda:0')
c= tensor(9.6480e+08, device='cuda:0')
c= tensor(9.6481e+08, device='cuda:0')
c= tensor(9.6482e+08, device='cuda:0')
c= tensor(9.6483e+08, device='cuda:0')
c= tensor(9.6485e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6491e+08, device='cuda:0')
c= tensor(9.6505e+08, device='cuda:0')
c= tensor(9.6506e+08, device='cuda:0')
c= tensor(9.6506e+08, device='cuda:0')
c= tensor(9.6507e+08, device='cuda:0')
c= tensor(9.6507e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6509e+08, device='cuda:0')
c= tensor(9.6510e+08, device='cuda:0')
c= tensor(9.6513e+08, device='cuda:0')
c= tensor(9.6513e+08, device='cuda:0')
c= tensor(9.6514e+08, device='cuda:0')
c= tensor(9.6516e+08, device='cuda:0')
c= tensor(9.6518e+08, device='cuda:0')
c= tensor(9.6520e+08, device='cuda:0')
c= tensor(9.6520e+08, device='cuda:0')
c= tensor(9.6524e+08, device='cuda:0')
c= tensor(9.6530e+08, device='cuda:0')
c= tensor(9.6532e+08, device='cuda:0')
c= tensor(9.6535e+08, device='cuda:0')
c= tensor(9.6537e+08, device='cuda:0')
c= tensor(9.6538e+08, device='cuda:0')
c= tensor(9.6539e+08, device='cuda:0')
c= tensor(9.6544e+08, device='cuda:0')
c= tensor(9.6544e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6547e+08, device='cuda:0')
c= tensor(9.6548e+08, device='cuda:0')
c= tensor(9.6549e+08, device='cuda:0')
c= tensor(9.6550e+08, device='cuda:0')
c= tensor(9.6551e+08, device='cuda:0')
c= tensor(9.6553e+08, device='cuda:0')
c= tensor(9.6553e+08, device='cuda:0')
c= tensor(9.6556e+08, device='cuda:0')
c= tensor(9.6557e+08, device='cuda:0')
c= tensor(9.6561e+08, device='cuda:0')
c= tensor(9.6562e+08, device='cuda:0')
c= tensor(9.6562e+08, device='cuda:0')
c= tensor(9.6563e+08, device='cuda:0')
c= tensor(9.6563e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6570e+08, device='cuda:0')
c= tensor(9.6574e+08, device='cuda:0')
c= tensor(9.6574e+08, device='cuda:0')
c= tensor(9.6585e+08, device='cuda:0')
c= tensor(9.6585e+08, device='cuda:0')
c= tensor(9.6587e+08, device='cuda:0')
c= tensor(9.6587e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6590e+08, device='cuda:0')
c= tensor(9.6590e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6595e+08, device='cuda:0')
c= tensor(9.6598e+08, device='cuda:0')
c= tensor(9.6600e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6602e+08, device='cuda:0')
c= tensor(9.6602e+08, device='cuda:0')
c= tensor(9.6603e+08, device='cuda:0')
c= tensor(9.6604e+08, device='cuda:0')
c= tensor(9.6605e+08, device='cuda:0')
c= tensor(9.6609e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6626e+08, device='cuda:0')
c= tensor(9.6627e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6643e+08, device='cuda:0')
c= tensor(9.6643e+08, device='cuda:0')
c= tensor(9.6645e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6647e+08, device='cuda:0')
c= tensor(9.6648e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6652e+08, device='cuda:0')
c= tensor(9.6653e+08, device='cuda:0')
c= tensor(9.6654e+08, device='cuda:0')
c= tensor(9.6680e+08, device='cuda:0')
c= tensor(9.6681e+08, device='cuda:0')
c= tensor(9.6682e+08, device='cuda:0')
c= tensor(9.6683e+08, device='cuda:0')
c= tensor(9.6684e+08, device='cuda:0')
c= tensor(9.6688e+08, device='cuda:0')
c= tensor(9.6689e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6692e+08, device='cuda:0')
c= tensor(9.6694e+08, device='cuda:0')
c= tensor(9.6694e+08, device='cuda:0')
c= tensor(9.6696e+08, device='cuda:0')
c= tensor(9.6698e+08, device='cuda:0')
c= tensor(9.6700e+08, device='cuda:0')
c= tensor(9.6701e+08, device='cuda:0')
c= tensor(9.6701e+08, device='cuda:0')
c= tensor(9.6702e+08, device='cuda:0')
c= tensor(9.6703e+08, device='cuda:0')
c= tensor(9.6705e+08, device='cuda:0')
c= tensor(9.6706e+08, device='cuda:0')
c= tensor(9.6706e+08, device='cuda:0')
c= tensor(9.6707e+08, device='cuda:0')
c= tensor(9.6709e+08, device='cuda:0')
c= tensor(9.6709e+08, device='cuda:0')
c= tensor(9.6714e+08, device='cuda:0')
c= tensor(9.6715e+08, device='cuda:0')
c= tensor(9.6728e+08, device='cuda:0')
c= tensor(9.6729e+08, device='cuda:0')
c= tensor(9.6731e+08, device='cuda:0')
c= tensor(9.6731e+08, device='cuda:0')
c= tensor(9.6732e+08, device='cuda:0')
c= tensor(9.6733e+08, device='cuda:0')
c= tensor(9.6734e+08, device='cuda:0')
c= tensor(9.6736e+08, device='cuda:0')
c= tensor(9.6740e+08, device='cuda:0')
c= tensor(9.6740e+08, device='cuda:0')
c= tensor(9.6741e+08, device='cuda:0')
c= tensor(9.6746e+08, device='cuda:0')
c= tensor(9.6746e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6755e+08, device='cuda:0')
c= tensor(9.6755e+08, device='cuda:0')
c= tensor(9.6757e+08, device='cuda:0')
c= tensor(9.6757e+08, device='cuda:0')
c= tensor(9.6759e+08, device='cuda:0')
c= tensor(9.6760e+08, device='cuda:0')
c= tensor(9.6760e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6763e+08, device='cuda:0')
c= tensor(9.6764e+08, device='cuda:0')
c= tensor(9.6765e+08, device='cuda:0')
c= tensor(9.6766e+08, device='cuda:0')
c= tensor(9.6766e+08, device='cuda:0')
c= tensor(9.6768e+08, device='cuda:0')
c= tensor(9.6768e+08, device='cuda:0')
c= tensor(9.6769e+08, device='cuda:0')
c= tensor(9.6773e+08, device='cuda:0')
c= tensor(9.6775e+08, device='cuda:0')
c= tensor(9.6776e+08, device='cuda:0')
c= tensor(9.6815e+08, device='cuda:0')
c= tensor(9.7025e+08, device='cuda:0')
c= tensor(9.7033e+08, device='cuda:0')
c= tensor(9.7040e+08, device='cuda:0')
c= tensor(9.7040e+08, device='cuda:0')
c= tensor(9.7045e+08, device='cuda:0')
c= tensor(9.7065e+08, device='cuda:0')
c= tensor(9.8067e+08, device='cuda:0')
c= tensor(9.8068e+08, device='cuda:0')
c= tensor(9.9349e+08, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0037e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3047e+09, device='cuda:0')
c= tensor(1.3246e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4422e+09, device='cuda:0')
c= tensor(1.4430e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4524e+09, device='cuda:0')
c= tensor(1.4554e+09, device='cuda:0')
c= tensor(1.4563e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4701e+09, device='cuda:0')
c= tensor(1.4702e+09, device='cuda:0')
c= tensor(1.4704e+09, device='cuda:0')
c= tensor(1.4766e+09, device='cuda:0')
c= tensor(1.4913e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4966e+09, device='cuda:0')
c= tensor(1.4993e+09, device='cuda:0')
c= tensor(1.5005e+09, device='cuda:0')
c= tensor(1.5046e+09, device='cuda:0')
c= tensor(1.5046e+09, device='cuda:0')
c= tensor(1.5047e+09, device='cuda:0')
c= tensor(1.5091e+09, device='cuda:0')
c= tensor(1.5126e+09, device='cuda:0')
c= tensor(1.5163e+09, device='cuda:0')
c= tensor(1.5163e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5512e+09, device='cuda:0')
c= tensor(1.5538e+09, device='cuda:0')
c= tensor(1.5538e+09, device='cuda:0')
c= tensor(1.5652e+09, device='cuda:0')
c= tensor(1.5918e+09, device='cuda:0')
c= tensor(1.7131e+09, device='cuda:0')
c= tensor(1.7162e+09, device='cuda:0')
c= tensor(1.7170e+09, device='cuda:0')
c= tensor(1.7171e+09, device='cuda:0')
c= tensor(1.7172e+09, device='cuda:0')
c= tensor(1.7300e+09, device='cuda:0')
c= tensor(1.7300e+09, device='cuda:0')
c= tensor(1.7307e+09, device='cuda:0')
c= tensor(1.7366e+09, device='cuda:0')
c= tensor(1.7370e+09, device='cuda:0')
c= tensor(1.7374e+09, device='cuda:0')
c= tensor(1.7374e+09, device='cuda:0')
c= tensor(1.7462e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7693e+09, device='cuda:0')
c= tensor(1.7693e+09, device='cuda:0')
c= tensor(1.7715e+09, device='cuda:0')
c= tensor(1.7719e+09, device='cuda:0')
c= tensor(1.7994e+09, device='cuda:0')
c= tensor(1.7995e+09, device='cuda:0')
c= tensor(1.8027e+09, device='cuda:0')
c= tensor(1.8028e+09, device='cuda:0')
c= tensor(1.8161e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8177e+09, device='cuda:0')
c= tensor(1.8244e+09, device='cuda:0')
c= tensor(1.8362e+09, device='cuda:0')
c= tensor(1.8395e+09, device='cuda:0')
c= tensor(1.8583e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.9259e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9283e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9356e+09, device='cuda:0')
c= tensor(1.9394e+09, device='cuda:0')
c= tensor(1.9400e+09, device='cuda:0')
c= tensor(1.9401e+09, device='cuda:0')
c= tensor(1.9437e+09, device='cuda:0')
c= tensor(1.9439e+09, device='cuda:0')
c= tensor(1.9440e+09, device='cuda:0')
c= tensor(1.9441e+09, device='cuda:0')
c= tensor(1.9450e+09, device='cuda:0')
c= tensor(1.9485e+09, device='cuda:0')
c= tensor(1.9492e+09, device='cuda:0')
c= tensor(1.9495e+09, device='cuda:0')
c= tensor(1.9506e+09, device='cuda:0')
c= tensor(1.9507e+09, device='cuda:0')
c= tensor(2.3404e+09, device='cuda:0')
c= tensor(2.3405e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3518e+09, device='cuda:0')
c= tensor(2.3518e+09, device='cuda:0')
c= tensor(2.3579e+09, device='cuda:0')
c= tensor(2.3580e+09, device='cuda:0')
c= tensor(2.3580e+09, device='cuda:0')
c= tensor(2.3796e+09, device='cuda:0')
c= tensor(2.3811e+09, device='cuda:0')
c= tensor(2.3817e+09, device='cuda:0')
c= tensor(2.3877e+09, device='cuda:0')
c= tensor(2.4204e+09, device='cuda:0')
c= tensor(2.4204e+09, device='cuda:0')
c= tensor(2.4205e+09, device='cuda:0')
c= tensor(2.4206e+09, device='cuda:0')
c= tensor(2.4206e+09, device='cuda:0')
c= tensor(2.4207e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4211e+09, device='cuda:0')
c= tensor(2.4211e+09, device='cuda:0')
c= tensor(2.4603e+09, device='cuda:0')
c= tensor(2.4604e+09, device='cuda:0')
c= tensor(2.4629e+09, device='cuda:0')
c= tensor(2.4630e+09, device='cuda:0')
c= tensor(2.4630e+09, device='cuda:0')
c= tensor(2.4705e+09, device='cuda:0')
c= tensor(2.7125e+09, device='cuda:0')
c= tensor(2.7945e+09, device='cuda:0')
c= tensor(2.7947e+09, device='cuda:0')
c= tensor(2.7955e+09, device='cuda:0')
c= tensor(2.7955e+09, device='cuda:0')
c= tensor(2.7956e+09, device='cuda:0')
c= tensor(3.0286e+09, device='cuda:0')
c= tensor(3.0288e+09, device='cuda:0')
c= tensor(3.0289e+09, device='cuda:0')
c= tensor(3.0304e+09, device='cuda:0')
c= tensor(3.1792e+09, device='cuda:0')
c= tensor(3.1799e+09, device='cuda:0')
c= tensor(3.1801e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1804e+09, device='cuda:0')
c= tensor(3.1804e+09, device='cuda:0')
c= tensor(3.1942e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1988e+09, device='cuda:0')
c= tensor(3.1990e+09, device='cuda:0')
c= tensor(3.1990e+09, device='cuda:0')
c= tensor(3.2005e+09, device='cuda:0')
c= tensor(3.2044e+09, device='cuda:0')
c= tensor(3.2354e+09, device='cuda:0')
c= tensor(3.2593e+09, device='cuda:0')
c= tensor(3.2759e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2830e+09, device='cuda:0')
c= tensor(3.2976e+09, device='cuda:0')
c= tensor(3.2977e+09, device='cuda:0')
c= tensor(3.4962e+09, device='cuda:0')
c= tensor(3.7920e+09, device='cuda:0')
c= tensor(3.8089e+09, device='cuda:0')
c= tensor(3.8125e+09, device='cuda:0')
c= tensor(3.8164e+09, device='cuda:0')
c= tensor(3.8166e+09, device='cuda:0')
c= tensor(3.8166e+09, device='cuda:0')
c= tensor(3.8169e+09, device='cuda:0')
c= tensor(3.8225e+09, device='cuda:0')
c= tensor(3.8286e+09, device='cuda:0')
c= tensor(3.8630e+09, device='cuda:0')
c= tensor(3.8676e+09, device='cuda:0')
c= tensor(3.8711e+09, device='cuda:0')
c= tensor(3.8714e+09, device='cuda:0')
c= tensor(3.8818e+09, device='cuda:0')
c= tensor(3.8818e+09, device='cuda:0')
c= tensor(3.8819e+09, device='cuda:0')
c= tensor(3.8880e+09, device='cuda:0')
c= tensor(3.8909e+09, device='cuda:0')
c= tensor(3.8909e+09, device='cuda:0')
c= tensor(3.8912e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0784e+09, device='cuda:0')
c= tensor(4.0839e+09, device='cuda:0')
c= tensor(4.0842e+09, device='cuda:0')
c= tensor(4.0843e+09, device='cuda:0')
c= tensor(4.0843e+09, device='cuda:0')
c= tensor(4.0844e+09, device='cuda:0')
c= tensor(4.0845e+09, device='cuda:0')
c= tensor(4.0862e+09, device='cuda:0')
c= tensor(4.0862e+09, device='cuda:0')
c= tensor(4.0966e+09, device='cuda:0')
c= tensor(4.0967e+09, device='cuda:0')
c= tensor(4.1005e+09, device='cuda:0')
c= tensor(4.1007e+09, device='cuda:0')
c= tensor(4.1080e+09, device='cuda:0')
c= tensor(4.1081e+09, device='cuda:0')
c= tensor(4.1084e+09, device='cuda:0')
c= tensor(4.1091e+09, device='cuda:0')
c= tensor(4.1097e+09, device='cuda:0')
c= tensor(4.1122e+09, device='cuda:0')
c= tensor(4.3479e+09, device='cuda:0')
c= tensor(4.3481e+09, device='cuda:0')
c= tensor(4.3482e+09, device='cuda:0')
c= tensor(4.3525e+09, device='cuda:0')
c= tensor(4.3528e+09, device='cuda:0')
c= tensor(4.4102e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4160e+09, device='cuda:0')
c= tensor(4.4480e+09, device='cuda:0')
c= tensor(4.4481e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4868e+09, device='cuda:0')
c= tensor(4.5700e+09, device='cuda:0')
c= tensor(4.5701e+09, device='cuda:0')
c= tensor(4.5705e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5732e+09, device='cuda:0')
c= tensor(4.5732e+09, device='cuda:0')
c= tensor(4.5858e+09, device='cuda:0')
c= tensor(4.5859e+09, device='cuda:0')
c= tensor(4.5860e+09, device='cuda:0')
c= tensor(4.5862e+09, device='cuda:0')
c= tensor(4.5995e+09, device='cuda:0')
c= tensor(4.6154e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6482e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.7476e+09, device='cuda:0')
c= tensor(4.7476e+09, device='cuda:0')
c= tensor(4.7478e+09, device='cuda:0')
c= tensor(4.7492e+09, device='cuda:0')
c= tensor(4.7500e+09, device='cuda:0')
c= tensor(4.7512e+09, device='cuda:0')
c= tensor(4.7513e+09, device='cuda:0')
c= tensor(4.7721e+09, device='cuda:0')
c= tensor(4.7737e+09, device='cuda:0')
c= tensor(4.7738e+09, device='cuda:0')
c= tensor(4.7747e+09, device='cuda:0')
c= tensor(4.7843e+09, device='cuda:0')
c= tensor(4.7870e+09, device='cuda:0')
c= tensor(4.8645e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8699e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8706e+09, device='cuda:0')
c= tensor(4.8706e+09, device='cuda:0')
c= tensor(4.8721e+09, device='cuda:0')
c= tensor(4.8742e+09, device='cuda:0')
c= tensor(4.8742e+09, device='cuda:0')
c= tensor(4.8744e+09, device='cuda:0')
c= tensor(4.8745e+09, device='cuda:0')
c= tensor(4.8752e+09, device='cuda:0')
c= tensor(4.8752e+09, device='cuda:0')
c= tensor(4.8756e+09, device='cuda:0')
c= tensor(4.8766e+09, device='cuda:0')
c= tensor(4.8767e+09, device='cuda:0')
c= tensor(4.8767e+09, device='cuda:0')
c= tensor(4.8768e+09, device='cuda:0')
c= tensor(4.8771e+09, device='cuda:0')
c= tensor(4.8873e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8908e+09, device='cuda:0')
c= tensor(5.0073e+09, device='cuda:0')
c= tensor(5.0147e+09, device='cuda:0')
c= tensor(5.0148e+09, device='cuda:0')
c= tensor(5.0500e+09, device='cuda:0')
c= tensor(5.0585e+09, device='cuda:0')
c= tensor(5.0585e+09, device='cuda:0')
c= tensor(5.0586e+09, device='cuda:0')
c= tensor(5.0594e+09, device='cuda:0')
c= tensor(5.0760e+09, device='cuda:0')
c= tensor(5.1450e+09, device='cuda:0')
c= tensor(5.1539e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1576e+09, device='cuda:0')
c= tensor(5.1577e+09, device='cuda:0')
c= tensor(5.1580e+09, device='cuda:0')
c= tensor(5.1633e+09, device='cuda:0')
c= tensor(5.2310e+09, device='cuda:0')
c= tensor(5.2311e+09, device='cuda:0')
c= tensor(5.2311e+09, device='cuda:0')
c= tensor(5.2313e+09, device='cuda:0')
c= tensor(5.2437e+09, device='cuda:0')
c= tensor(5.2455e+09, device='cuda:0')
c= tensor(5.2459e+09, device='cuda:0')
c= tensor(5.2459e+09, device='cuda:0')
c= tensor(5.2465e+09, device='cuda:0')
c= tensor(5.2465e+09, device='cuda:0')
c= tensor(5.2478e+09, device='cuda:0')
c= tensor(5.2479e+09, device='cuda:0')
c= tensor(5.2480e+09, device='cuda:0')
c= tensor(5.2481e+09, device='cuda:0')
c= tensor(5.2481e+09, device='cuda:0')
c= tensor(5.2483e+09, device='cuda:0')
c= tensor(5.2566e+09, device='cuda:0')
c= tensor(5.2865e+09, device='cuda:0')
c= tensor(5.2903e+09, device='cuda:0')
c= tensor(5.3036e+09, device='cuda:0')
c= tensor(5.3038e+09, device='cuda:0')
c= tensor(5.3040e+09, device='cuda:0')
c= tensor(5.3043e+09, device='cuda:0')
c= tensor(5.3090e+09, device='cuda:0')
c= tensor(5.3094e+09, device='cuda:0')
c= tensor(5.3110e+09, device='cuda:0')
c= tensor(5.3110e+09, device='cuda:0')
c= tensor(5.7044e+09, device='cuda:0')
c= tensor(5.7046e+09, device='cuda:0')
c= tensor(5.7062e+09, device='cuda:0')
c= tensor(5.7210e+09, device='cuda:0')
c= tensor(5.7224e+09, device='cuda:0')
c= tensor(5.7233e+09, device='cuda:0')
c= tensor(6.0591e+09, device='cuda:0')
c= tensor(6.0771e+09, device='cuda:0')
c= tensor(6.0811e+09, device='cuda:0')
c= tensor(6.0812e+09, device='cuda:0')
c= tensor(6.0823e+09, device='cuda:0')
c= tensor(6.0823e+09, device='cuda:0')
c= tensor(6.1106e+09, device='cuda:0')
c= tensor(6.4146e+09, device='cuda:0')
c= tensor(6.4231e+09, device='cuda:0')
c= tensor(6.4297e+09, device='cuda:0')
c= tensor(6.4365e+09, device='cuda:0')
c= tensor(6.4370e+09, device='cuda:0')
c= tensor(6.4377e+09, device='cuda:0')
c= tensor(6.4378e+09, device='cuda:0')
c= tensor(6.4623e+09, device='cuda:0')
c= tensor(6.5217e+09, device='cuda:0')
c= tensor(6.5249e+09, device='cuda:0')
c= tensor(6.9076e+09, device='cuda:0')
c= tensor(6.9186e+09, device='cuda:0')
c= tensor(6.9197e+09, device='cuda:0')
c= tensor(6.9201e+09, device='cuda:0')
c= tensor(6.9274e+09, device='cuda:0')
c= tensor(6.9359e+09, device='cuda:0')
c= tensor(6.9360e+09, device='cuda:0')
c= tensor(7.0099e+09, device='cuda:0')
c= tensor(7.0110e+09, device='cuda:0')
c= tensor(7.0120e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0122e+09, device='cuda:0')
c= tensor(7.0125e+09, device='cuda:0')
c= tensor(7.0190e+09, device='cuda:0')
c= tensor(9.5364e+09, device='cuda:0')
c= tensor(9.5376e+09, device='cuda:0')
c= tensor(9.5647e+09, device='cuda:0')
c= tensor(9.5648e+09, device='cuda:0')
c= tensor(9.5650e+09, device='cuda:0')
c= tensor(9.5650e+09, device='cuda:0')
c= tensor(9.5781e+09, device='cuda:0')
c= tensor(9.5799e+09, device='cuda:0')
c= tensor(9.8165e+09, device='cuda:0')
c= tensor(9.8165e+09, device='cuda:0')
c= tensor(9.8259e+09, device='cuda:0')
c= tensor(9.8263e+09, device='cuda:0')
c= tensor(9.8331e+09, device='cuda:0')
c= tensor(9.8574e+09, device='cuda:0')
c= tensor(9.8577e+09, device='cuda:0')
c= tensor(9.8577e+09, device='cuda:0')
c= tensor(9.8613e+09, device='cuda:0')
c= tensor(9.8619e+09, device='cuda:0')
c= tensor(9.8626e+09, device='cuda:0')
c= tensor(9.8959e+09, device='cuda:0')
c= tensor(9.8967e+09, device='cuda:0')
c= tensor(9.9000e+09, device='cuda:0')
c= tensor(9.9005e+09, device='cuda:0')
c= tensor(9.9052e+09, device='cuda:0')
c= tensor(9.9266e+09, device='cuda:0')
c= tensor(9.9279e+09, device='cuda:0')
c= tensor(9.9281e+09, device='cuda:0')
c= tensor(9.9365e+09, device='cuda:0')
c= tensor(9.9393e+09, device='cuda:0')
c= tensor(1.0020e+10, device='cuda:0')
c= tensor(1.0023e+10, device='cuda:0')
c= tensor(1.0025e+10, device='cuda:0')
c= tensor(1.0026e+10, device='cuda:0')
c= tensor(1.0051e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0063e+10, device='cuda:0')
c= tensor(1.0069e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0072e+10, device='cuda:0')
c= tensor(1.0073e+10, device='cuda:0')
c= tensor(1.0074e+10, device='cuda:0')
c= tensor(1.0075e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0104e+10, device='cuda:0')
c= tensor(1.0105e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0110e+10, device='cuda:0')
c= tensor(1.0112e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0114e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0122e+10, device='cuda:0')
c= tensor(1.0122e+10, device='cuda:0')
c= tensor(1.0145e+10, device='cuda:0')
c= tensor(1.0145e+10, device='cuda:0')
c= tensor(1.0152e+10, device='cuda:0')
c= tensor(1.0157e+10, device='cuda:0')
c= tensor(1.0161e+10, device='cuda:0')
c= tensor(1.0169e+10, device='cuda:0')
c= tensor(1.0174e+10, device='cuda:0')
c= tensor(1.0174e+10, device='cuda:0')
c= tensor(1.0176e+10, device='cuda:0')
c= tensor(1.0176e+10, device='cuda:0')
c= tensor(1.0182e+10, device='cuda:0')
c= tensor(1.0187e+10, device='cuda:0')
c= tensor(1.0188e+10, device='cuda:0')
c= tensor(1.0189e+10, device='cuda:0')
c= tensor(1.0190e+10, device='cuda:0')
c= tensor(1.0194e+10, device='cuda:0')
c= tensor(1.0194e+10, device='cuda:0')
c= tensor(1.0195e+10, device='cuda:0')
c= tensor(1.0212e+10, device='cuda:0')
c= tensor(1.0286e+10, device='cuda:0')
c= tensor(1.0286e+10, device='cuda:0')
c= tensor(1.0287e+10, device='cuda:0')
c= tensor(1.0287e+10, device='cuda:0')
c= tensor(1.0293e+10, device='cuda:0')
c= tensor(1.0294e+10, device='cuda:0')
c= tensor(1.0294e+10, device='cuda:0')
c= tensor(1.0296e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0304e+10, device='cuda:0')
c= tensor(1.0305e+10, device='cuda:0')
memory (bytes)
5932613632
time for making loss 2 is 10.522788524627686
p0 True
it  0 : 3277690368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 72% |
shape of L is 
torch.Size([])
memory (bytes)
5932752896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 16% |
memory (bytes)
5933158400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  119423670000.0
relative error loss 11.588596
shape of L is 
torch.Size([])
memory (bytes)
6043037696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  7% | 17% |
memory (bytes)
6043037696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  119423310000.0
relative error loss 11.588561
shape of L is 
torch.Size([])
memory (bytes)
6045290496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6045372416
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  119421600000.0
relative error loss 11.588395
shape of L is 
torch.Size([])
memory (bytes)
6047277056
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6047277056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  119412170000.0
relative error loss 11.587481
shape of L is 
torch.Size([])
memory (bytes)
6049468416
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6049566720
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  119360420000.0
relative error loss 11.5824585
shape of L is 
torch.Size([])
memory (bytes)
6051647488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6051647488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  119075406000.0
relative error loss 11.554802
shape of L is 
torch.Size([])
memory (bytes)
6053781504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 |  9% | 17% |
memory (bytes)
6053859328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  115979480000.0
relative error loss 11.25438
shape of L is 
torch.Size([])
memory (bytes)
6055743488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6055976960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  101506280000.0
relative error loss 9.849934
shape of L is 
torch.Size([])
memory (bytes)
6058045440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6058123264
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 92% | 17% |
error is  30120702000.0
relative error loss 2.9228432
shape of L is 
torch.Size([])
memory (bytes)
6060240896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6060240896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  1416294800000.0
relative error loss 137.43398
shape of L is 
torch.Size([])
memory (bytes)
6062411776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
6062411776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  35385143000.0
relative error loss 3.4336922
shape of L is 
torch.Size([])
memory (bytes)
6064369664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6064369664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  23566758000.0
relative error loss 2.2868636
time to take a step is 221.01384091377258
it  1 : 3789750784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6066618368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
6066618368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 90% | 17% |
error is  23566758000.0
relative error loss 2.2868636
shape of L is 
torch.Size([])
memory (bytes)
6068604928
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6068801536
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  15864805000.0
relative error loss 1.5394839
shape of L is 
torch.Size([])
memory (bytes)
6070882304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 17% |
memory (bytes)
6070882304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  11798910000.0
relative error loss 1.144939
shape of L is 
torch.Size([])
memory (bytes)
6073094144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
6073094144
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  17387282000.0
relative error loss 1.6872216
shape of L is 
torch.Size([])
memory (bytes)
6075179008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6075179008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  11174647000.0
relative error loss 1.0843618
shape of L is 
torch.Size([])
memory (bytes)
6077288448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 17% |
memory (bytes)
6077337600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  10410689000.0
relative error loss 1.0102291
shape of L is 
torch.Size([])
memory (bytes)
6079344640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6079344640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  10145302000.0
relative error loss 0.98447657
shape of L is 
torch.Size([])
memory (bytes)
6081421312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6081421312
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  13572058000.0
relative error loss 1.3170011
shape of L is 
torch.Size([])
memory (bytes)
6083489792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6083489792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  9629023000.0
relative error loss 0.9343781
shape of L is 
torch.Size([])
memory (bytes)
6085681152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6085763072
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  9467770000.0
relative error loss 0.91873044
time to take a step is 180.24887323379517
it  2 : 3935970304
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 10% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6087753728
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6087880704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  9467770000.0
relative error loss 0.91873044
shape of L is 
torch.Size([])
memory (bytes)
6089891840
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6089891840
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8936455000.0
relative error loss 0.8671729
shape of L is 
torch.Size([])
memory (bytes)
6092062720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6092062720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  8564646400.0
relative error loss 0.83109343
shape of L is 
torch.Size([])
memory (bytes)
6094180352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
6094180352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  8045682700.0
relative error loss 0.7807344
shape of L is 
torch.Size([])
memory (bytes)
6096240640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6096240640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  7337357300.0
relative error loss 0.71200013
shape of L is 
torch.Size([])
memory (bytes)
6098391040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
6098472960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6848581000.0
relative error loss 0.66457045
shape of L is 
torch.Size([])
memory (bytes)
6100606976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6100606976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  6552339000.0
relative error loss 0.6358238
shape of L is 
torch.Size([])
memory (bytes)
6102618112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6102618112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  6321271000.0
relative error loss 0.6134015
shape of L is 
torch.Size([])
memory (bytes)
6104862720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6104866816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5897592300.0
relative error loss 0.5722887
shape of L is 
torch.Size([])
memory (bytes)
6106914816
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
6106914816
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5452636000.0
relative error loss 0.5291112
time to take a step is 179.37865734100342
it  3 : 3935679488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6108897280
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6109130752
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  5452636000.0
relative error loss 0.5291112
shape of L is 
torch.Size([])
memory (bytes)
6111211520
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
6111211520
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  5076370000.0
relative error loss 0.4925992
shape of L is 
torch.Size([])
memory (bytes)
6113402880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6113402880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 95% | 17% |
error is  4806585300.0
relative error loss 0.4664199
shape of L is 
torch.Size([])
memory (bytes)
6115508224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6115508224
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  4346463700.0
relative error loss 0.42177078
shape of L is 
torch.Size([])
memory (bytes)
6117507072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6117507072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  4106573800.0
relative error loss 0.3984924
shape of L is 
torch.Size([])
memory (bytes)
6119718912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6119800832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3799696400.0
relative error loss 0.36871374
shape of L is 
torch.Size([])
memory (bytes)
6121803776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6121803776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  3662438000.0
relative error loss 0.35539448
shape of L is 
torch.Size([])
memory (bytes)
6124032000
| ID | GPU | MEM |
------------------
|  0 | 10% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6124032000
| ID | GPU  | MEM |
-------------------
|  0 |   8% |  0% |
|  1 | 100% | 17% |
error is  3420993000.0
relative error loss 0.33196524
shape of L is 
torch.Size([])
memory (bytes)
6126141440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 10% | 17% |
memory (bytes)
6126219264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  3056426000.0
relative error loss 0.2965885
shape of L is 
torch.Size([])
memory (bytes)
6128279552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6128279552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 97% | 17% |
error is  2850975200.0
relative error loss 0.27665204
time to take a step is 179.59805297851562
c= tensor(2313.4773, device='cuda:0')
c= tensor(218607.2656, device='cuda:0')
c= tensor(223588.6094, device='cuda:0')
c= tensor(450149.6875, device='cuda:0')
c= tensor(918231.2500, device='cuda:0')
c= tensor(2156030.5000, device='cuda:0')
c= tensor(2923263.2500, device='cuda:0')
c= tensor(3670895.5000, device='cuda:0')
c= tensor(3801036.2500, device='cuda:0')
c= tensor(12089275., device='cuda:0')
c= tensor(12133416., device='cuda:0')
c= tensor(17586932., device='cuda:0')
c= tensor(17624338., device='cuda:0')
c= tensor(69380280., device='cuda:0')
c= tensor(69780000., device='cuda:0')
c= tensor(70649512., device='cuda:0')
c= tensor(72237224., device='cuda:0')
c= tensor(72554520., device='cuda:0')
c= tensor(83334568., device='cuda:0')
c= tensor(86834728., device='cuda:0')
c= tensor(89289776., device='cuda:0')
c= tensor(1.1646e+08, device='cuda:0')
c= tensor(1.1653e+08, device='cuda:0')
c= tensor(1.1815e+08, device='cuda:0')
c= tensor(1.1819e+08, device='cuda:0')
c= tensor(1.1993e+08, device='cuda:0')
c= tensor(1.2181e+08, device='cuda:0')
c= tensor(1.2188e+08, device='cuda:0')
c= tensor(1.2713e+08, device='cuda:0')
c= tensor(7.1809e+08, device='cuda:0')
c= tensor(7.1828e+08, device='cuda:0')
c= tensor(9.3161e+08, device='cuda:0')
c= tensor(9.3226e+08, device='cuda:0')
c= tensor(9.3237e+08, device='cuda:0')
c= tensor(9.3307e+08, device='cuda:0')
c= tensor(9.6011e+08, device='cuda:0')
c= tensor(9.6408e+08, device='cuda:0')
c= tensor(9.6408e+08, device='cuda:0')
c= tensor(9.6409e+08, device='cuda:0')
c= tensor(9.6410e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6411e+08, device='cuda:0')
c= tensor(9.6412e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6413e+08, device='cuda:0')
c= tensor(9.6415e+08, device='cuda:0')
c= tensor(9.6416e+08, device='cuda:0')
c= tensor(9.6424e+08, device='cuda:0')
c= tensor(9.6434e+08, device='cuda:0')
c= tensor(9.6434e+08, device='cuda:0')
c= tensor(9.6436e+08, device='cuda:0')
c= tensor(9.6437e+08, device='cuda:0')
c= tensor(9.6439e+08, device='cuda:0')
c= tensor(9.6443e+08, device='cuda:0')
c= tensor(9.6443e+08, device='cuda:0')
c= tensor(9.6447e+08, device='cuda:0')
c= tensor(9.6447e+08, device='cuda:0')
c= tensor(9.6448e+08, device='cuda:0')
c= tensor(9.6451e+08, device='cuda:0')
c= tensor(9.6451e+08, device='cuda:0')
c= tensor(9.6453e+08, device='cuda:0')
c= tensor(9.6461e+08, device='cuda:0')
c= tensor(9.6461e+08, device='cuda:0')
c= tensor(9.6462e+08, device='cuda:0')
c= tensor(9.6462e+08, device='cuda:0')
c= tensor(9.6464e+08, device='cuda:0')
c= tensor(9.6469e+08, device='cuda:0')
c= tensor(9.6471e+08, device='cuda:0')
c= tensor(9.6480e+08, device='cuda:0')
c= tensor(9.6481e+08, device='cuda:0')
c= tensor(9.6482e+08, device='cuda:0')
c= tensor(9.6483e+08, device='cuda:0')
c= tensor(9.6485e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6488e+08, device='cuda:0')
c= tensor(9.6491e+08, device='cuda:0')
c= tensor(9.6505e+08, device='cuda:0')
c= tensor(9.6506e+08, device='cuda:0')
c= tensor(9.6506e+08, device='cuda:0')
c= tensor(9.6507e+08, device='cuda:0')
c= tensor(9.6507e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6508e+08, device='cuda:0')
c= tensor(9.6509e+08, device='cuda:0')
c= tensor(9.6510e+08, device='cuda:0')
c= tensor(9.6513e+08, device='cuda:0')
c= tensor(9.6513e+08, device='cuda:0')
c= tensor(9.6514e+08, device='cuda:0')
c= tensor(9.6516e+08, device='cuda:0')
c= tensor(9.6518e+08, device='cuda:0')
c= tensor(9.6520e+08, device='cuda:0')
c= tensor(9.6520e+08, device='cuda:0')
c= tensor(9.6524e+08, device='cuda:0')
c= tensor(9.6530e+08, device='cuda:0')
c= tensor(9.6532e+08, device='cuda:0')
c= tensor(9.6535e+08, device='cuda:0')
c= tensor(9.6537e+08, device='cuda:0')
c= tensor(9.6538e+08, device='cuda:0')
c= tensor(9.6539e+08, device='cuda:0')
c= tensor(9.6544e+08, device='cuda:0')
c= tensor(9.6544e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6545e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6546e+08, device='cuda:0')
c= tensor(9.6547e+08, device='cuda:0')
c= tensor(9.6548e+08, device='cuda:0')
c= tensor(9.6549e+08, device='cuda:0')
c= tensor(9.6550e+08, device='cuda:0')
c= tensor(9.6551e+08, device='cuda:0')
c= tensor(9.6553e+08, device='cuda:0')
c= tensor(9.6553e+08, device='cuda:0')
c= tensor(9.6556e+08, device='cuda:0')
c= tensor(9.6557e+08, device='cuda:0')
c= tensor(9.6561e+08, device='cuda:0')
c= tensor(9.6562e+08, device='cuda:0')
c= tensor(9.6562e+08, device='cuda:0')
c= tensor(9.6563e+08, device='cuda:0')
c= tensor(9.6563e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6564e+08, device='cuda:0')
c= tensor(9.6570e+08, device='cuda:0')
c= tensor(9.6574e+08, device='cuda:0')
c= tensor(9.6574e+08, device='cuda:0')
c= tensor(9.6585e+08, device='cuda:0')
c= tensor(9.6585e+08, device='cuda:0')
c= tensor(9.6587e+08, device='cuda:0')
c= tensor(9.6587e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6588e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6589e+08, device='cuda:0')
c= tensor(9.6590e+08, device='cuda:0')
c= tensor(9.6590e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6591e+08, device='cuda:0')
c= tensor(9.6595e+08, device='cuda:0')
c= tensor(9.6598e+08, device='cuda:0')
c= tensor(9.6600e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6601e+08, device='cuda:0')
c= tensor(9.6602e+08, device='cuda:0')
c= tensor(9.6602e+08, device='cuda:0')
c= tensor(9.6603e+08, device='cuda:0')
c= tensor(9.6604e+08, device='cuda:0')
c= tensor(9.6605e+08, device='cuda:0')
c= tensor(9.6609e+08, device='cuda:0')
c= tensor(9.6610e+08, device='cuda:0')
c= tensor(9.6626e+08, device='cuda:0')
c= tensor(9.6627e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6629e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6638e+08, device='cuda:0')
c= tensor(9.6643e+08, device='cuda:0')
c= tensor(9.6643e+08, device='cuda:0')
c= tensor(9.6645e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6646e+08, device='cuda:0')
c= tensor(9.6647e+08, device='cuda:0')
c= tensor(9.6648e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6649e+08, device='cuda:0')
c= tensor(9.6652e+08, device='cuda:0')
c= tensor(9.6653e+08, device='cuda:0')
c= tensor(9.6654e+08, device='cuda:0')
c= tensor(9.6680e+08, device='cuda:0')
c= tensor(9.6681e+08, device='cuda:0')
c= tensor(9.6682e+08, device='cuda:0')
c= tensor(9.6683e+08, device='cuda:0')
c= tensor(9.6684e+08, device='cuda:0')
c= tensor(9.6688e+08, device='cuda:0')
c= tensor(9.6689e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6691e+08, device='cuda:0')
c= tensor(9.6692e+08, device='cuda:0')
c= tensor(9.6694e+08, device='cuda:0')
c= tensor(9.6694e+08, device='cuda:0')
c= tensor(9.6696e+08, device='cuda:0')
c= tensor(9.6698e+08, device='cuda:0')
c= tensor(9.6700e+08, device='cuda:0')
c= tensor(9.6701e+08, device='cuda:0')
c= tensor(9.6701e+08, device='cuda:0')
c= tensor(9.6702e+08, device='cuda:0')
c= tensor(9.6703e+08, device='cuda:0')
c= tensor(9.6705e+08, device='cuda:0')
c= tensor(9.6706e+08, device='cuda:0')
c= tensor(9.6706e+08, device='cuda:0')
c= tensor(9.6707e+08, device='cuda:0')
c= tensor(9.6709e+08, device='cuda:0')
c= tensor(9.6709e+08, device='cuda:0')
c= tensor(9.6714e+08, device='cuda:0')
c= tensor(9.6715e+08, device='cuda:0')
c= tensor(9.6728e+08, device='cuda:0')
c= tensor(9.6729e+08, device='cuda:0')
c= tensor(9.6731e+08, device='cuda:0')
c= tensor(9.6731e+08, device='cuda:0')
c= tensor(9.6732e+08, device='cuda:0')
c= tensor(9.6733e+08, device='cuda:0')
c= tensor(9.6734e+08, device='cuda:0')
c= tensor(9.6736e+08, device='cuda:0')
c= tensor(9.6740e+08, device='cuda:0')
c= tensor(9.6740e+08, device='cuda:0')
c= tensor(9.6741e+08, device='cuda:0')
c= tensor(9.6746e+08, device='cuda:0')
c= tensor(9.6746e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6747e+08, device='cuda:0')
c= tensor(9.6755e+08, device='cuda:0')
c= tensor(9.6755e+08, device='cuda:0')
c= tensor(9.6757e+08, device='cuda:0')
c= tensor(9.6757e+08, device='cuda:0')
c= tensor(9.6759e+08, device='cuda:0')
c= tensor(9.6760e+08, device='cuda:0')
c= tensor(9.6760e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6762e+08, device='cuda:0')
c= tensor(9.6763e+08, device='cuda:0')
c= tensor(9.6764e+08, device='cuda:0')
c= tensor(9.6765e+08, device='cuda:0')
c= tensor(9.6766e+08, device='cuda:0')
c= tensor(9.6766e+08, device='cuda:0')
c= tensor(9.6768e+08, device='cuda:0')
c= tensor(9.6768e+08, device='cuda:0')
c= tensor(9.6769e+08, device='cuda:0')
c= tensor(9.6773e+08, device='cuda:0')
c= tensor(9.6775e+08, device='cuda:0')
c= tensor(9.6776e+08, device='cuda:0')
c= tensor(9.6815e+08, device='cuda:0')
c= tensor(9.7025e+08, device='cuda:0')
c= tensor(9.7033e+08, device='cuda:0')
c= tensor(9.7040e+08, device='cuda:0')
c= tensor(9.7040e+08, device='cuda:0')
c= tensor(9.7045e+08, device='cuda:0')
c= tensor(9.7065e+08, device='cuda:0')
c= tensor(9.8067e+08, device='cuda:0')
c= tensor(9.8068e+08, device='cuda:0')
c= tensor(9.9349e+08, device='cuda:0')
c= tensor(1.0012e+09, device='cuda:0')
c= tensor(1.0037e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3046e+09, device='cuda:0')
c= tensor(1.3047e+09, device='cuda:0')
c= tensor(1.3246e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4414e+09, device='cuda:0')
c= tensor(1.4422e+09, device='cuda:0')
c= tensor(1.4430e+09, device='cuda:0')
c= tensor(1.4457e+09, device='cuda:0')
c= tensor(1.4524e+09, device='cuda:0')
c= tensor(1.4554e+09, device='cuda:0')
c= tensor(1.4563e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4565e+09, device='cuda:0')
c= tensor(1.4694e+09, device='cuda:0')
c= tensor(1.4701e+09, device='cuda:0')
c= tensor(1.4702e+09, device='cuda:0')
c= tensor(1.4704e+09, device='cuda:0')
c= tensor(1.4766e+09, device='cuda:0')
c= tensor(1.4913e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4942e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4953e+09, device='cuda:0')
c= tensor(1.4966e+09, device='cuda:0')
c= tensor(1.4993e+09, device='cuda:0')
c= tensor(1.5005e+09, device='cuda:0')
c= tensor(1.5046e+09, device='cuda:0')
c= tensor(1.5046e+09, device='cuda:0')
c= tensor(1.5047e+09, device='cuda:0')
c= tensor(1.5091e+09, device='cuda:0')
c= tensor(1.5126e+09, device='cuda:0')
c= tensor(1.5163e+09, device='cuda:0')
c= tensor(1.5163e+09, device='cuda:0')
c= tensor(1.5505e+09, device='cuda:0')
c= tensor(1.5507e+09, device='cuda:0')
c= tensor(1.5512e+09, device='cuda:0')
c= tensor(1.5538e+09, device='cuda:0')
c= tensor(1.5538e+09, device='cuda:0')
c= tensor(1.5652e+09, device='cuda:0')
c= tensor(1.5918e+09, device='cuda:0')
c= tensor(1.7131e+09, device='cuda:0')
c= tensor(1.7162e+09, device='cuda:0')
c= tensor(1.7170e+09, device='cuda:0')
c= tensor(1.7171e+09, device='cuda:0')
c= tensor(1.7172e+09, device='cuda:0')
c= tensor(1.7300e+09, device='cuda:0')
c= tensor(1.7300e+09, device='cuda:0')
c= tensor(1.7307e+09, device='cuda:0')
c= tensor(1.7366e+09, device='cuda:0')
c= tensor(1.7370e+09, device='cuda:0')
c= tensor(1.7374e+09, device='cuda:0')
c= tensor(1.7374e+09, device='cuda:0')
c= tensor(1.7462e+09, device='cuda:0')
c= tensor(1.7674e+09, device='cuda:0')
c= tensor(1.7693e+09, device='cuda:0')
c= tensor(1.7693e+09, device='cuda:0')
c= tensor(1.7715e+09, device='cuda:0')
c= tensor(1.7719e+09, device='cuda:0')
c= tensor(1.7994e+09, device='cuda:0')
c= tensor(1.7995e+09, device='cuda:0')
c= tensor(1.8027e+09, device='cuda:0')
c= tensor(1.8028e+09, device='cuda:0')
c= tensor(1.8161e+09, device='cuda:0')
c= tensor(1.8172e+09, device='cuda:0')
c= tensor(1.8177e+09, device='cuda:0')
c= tensor(1.8244e+09, device='cuda:0')
c= tensor(1.8362e+09, device='cuda:0')
c= tensor(1.8395e+09, device='cuda:0')
c= tensor(1.8583e+09, device='cuda:0')
c= tensor(1.8748e+09, device='cuda:0')
c= tensor(1.9259e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9265e+09, device='cuda:0')
c= tensor(1.9279e+09, device='cuda:0')
c= tensor(1.9283e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9336e+09, device='cuda:0')
c= tensor(1.9356e+09, device='cuda:0')
c= tensor(1.9394e+09, device='cuda:0')
c= tensor(1.9400e+09, device='cuda:0')
c= tensor(1.9401e+09, device='cuda:0')
c= tensor(1.9437e+09, device='cuda:0')
c= tensor(1.9439e+09, device='cuda:0')
c= tensor(1.9440e+09, device='cuda:0')
c= tensor(1.9441e+09, device='cuda:0')
c= tensor(1.9450e+09, device='cuda:0')
c= tensor(1.9485e+09, device='cuda:0')
c= tensor(1.9492e+09, device='cuda:0')
c= tensor(1.9495e+09, device='cuda:0')
c= tensor(1.9506e+09, device='cuda:0')
c= tensor(1.9507e+09, device='cuda:0')
c= tensor(2.3404e+09, device='cuda:0')
c= tensor(2.3405e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3515e+09, device='cuda:0')
c= tensor(2.3518e+09, device='cuda:0')
c= tensor(2.3518e+09, device='cuda:0')
c= tensor(2.3579e+09, device='cuda:0')
c= tensor(2.3580e+09, device='cuda:0')
c= tensor(2.3580e+09, device='cuda:0')
c= tensor(2.3796e+09, device='cuda:0')
c= tensor(2.3811e+09, device='cuda:0')
c= tensor(2.3817e+09, device='cuda:0')
c= tensor(2.3877e+09, device='cuda:0')
c= tensor(2.4204e+09, device='cuda:0')
c= tensor(2.4204e+09, device='cuda:0')
c= tensor(2.4205e+09, device='cuda:0')
c= tensor(2.4206e+09, device='cuda:0')
c= tensor(2.4206e+09, device='cuda:0')
c= tensor(2.4207e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4208e+09, device='cuda:0')
c= tensor(2.4211e+09, device='cuda:0')
c= tensor(2.4211e+09, device='cuda:0')
c= tensor(2.4603e+09, device='cuda:0')
c= tensor(2.4604e+09, device='cuda:0')
c= tensor(2.4629e+09, device='cuda:0')
c= tensor(2.4630e+09, device='cuda:0')
c= tensor(2.4630e+09, device='cuda:0')
c= tensor(2.4705e+09, device='cuda:0')
c= tensor(2.7125e+09, device='cuda:0')
c= tensor(2.7945e+09, device='cuda:0')
c= tensor(2.7947e+09, device='cuda:0')
c= tensor(2.7955e+09, device='cuda:0')
c= tensor(2.7955e+09, device='cuda:0')
c= tensor(2.7956e+09, device='cuda:0')
c= tensor(3.0286e+09, device='cuda:0')
c= tensor(3.0288e+09, device='cuda:0')
c= tensor(3.0289e+09, device='cuda:0')
c= tensor(3.0304e+09, device='cuda:0')
c= tensor(3.1792e+09, device='cuda:0')
c= tensor(3.1799e+09, device='cuda:0')
c= tensor(3.1801e+09, device='cuda:0')
c= tensor(3.1802e+09, device='cuda:0')
c= tensor(3.1804e+09, device='cuda:0')
c= tensor(3.1804e+09, device='cuda:0')
c= tensor(3.1942e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1943e+09, device='cuda:0')
c= tensor(3.1988e+09, device='cuda:0')
c= tensor(3.1990e+09, device='cuda:0')
c= tensor(3.1990e+09, device='cuda:0')
c= tensor(3.2005e+09, device='cuda:0')
c= tensor(3.2044e+09, device='cuda:0')
c= tensor(3.2354e+09, device='cuda:0')
c= tensor(3.2593e+09, device='cuda:0')
c= tensor(3.2759e+09, device='cuda:0')
c= tensor(3.2762e+09, device='cuda:0')
c= tensor(3.2767e+09, device='cuda:0')
c= tensor(3.2830e+09, device='cuda:0')
c= tensor(3.2976e+09, device='cuda:0')
c= tensor(3.2977e+09, device='cuda:0')
c= tensor(3.4962e+09, device='cuda:0')
c= tensor(3.7920e+09, device='cuda:0')
c= tensor(3.8089e+09, device='cuda:0')
c= tensor(3.8125e+09, device='cuda:0')
c= tensor(3.8164e+09, device='cuda:0')
c= tensor(3.8166e+09, device='cuda:0')
c= tensor(3.8166e+09, device='cuda:0')
c= tensor(3.8169e+09, device='cuda:0')
c= tensor(3.8225e+09, device='cuda:0')
c= tensor(3.8286e+09, device='cuda:0')
c= tensor(3.8630e+09, device='cuda:0')
c= tensor(3.8676e+09, device='cuda:0')
c= tensor(3.8711e+09, device='cuda:0')
c= tensor(3.8714e+09, device='cuda:0')
c= tensor(3.8818e+09, device='cuda:0')
c= tensor(3.8818e+09, device='cuda:0')
c= tensor(3.8819e+09, device='cuda:0')
c= tensor(3.8880e+09, device='cuda:0')
c= tensor(3.8909e+09, device='cuda:0')
c= tensor(3.8909e+09, device='cuda:0')
c= tensor(3.8912e+09, device='cuda:0')
c= tensor(4.0782e+09, device='cuda:0')
c= tensor(4.0784e+09, device='cuda:0')
c= tensor(4.0839e+09, device='cuda:0')
c= tensor(4.0842e+09, device='cuda:0')
c= tensor(4.0843e+09, device='cuda:0')
c= tensor(4.0843e+09, device='cuda:0')
c= tensor(4.0844e+09, device='cuda:0')
c= tensor(4.0845e+09, device='cuda:0')
c= tensor(4.0862e+09, device='cuda:0')
c= tensor(4.0862e+09, device='cuda:0')
c= tensor(4.0966e+09, device='cuda:0')
c= tensor(4.0967e+09, device='cuda:0')
c= tensor(4.1005e+09, device='cuda:0')
c= tensor(4.1007e+09, device='cuda:0')
c= tensor(4.1080e+09, device='cuda:0')
c= tensor(4.1081e+09, device='cuda:0')
c= tensor(4.1084e+09, device='cuda:0')
c= tensor(4.1091e+09, device='cuda:0')
c= tensor(4.1097e+09, device='cuda:0')
c= tensor(4.1122e+09, device='cuda:0')
c= tensor(4.3479e+09, device='cuda:0')
c= tensor(4.3481e+09, device='cuda:0')
c= tensor(4.3482e+09, device='cuda:0')
c= tensor(4.3525e+09, device='cuda:0')
c= tensor(4.3528e+09, device='cuda:0')
c= tensor(4.4102e+09, device='cuda:0')
c= tensor(4.4103e+09, device='cuda:0')
c= tensor(4.4160e+09, device='cuda:0')
c= tensor(4.4480e+09, device='cuda:0')
c= tensor(4.4481e+09, device='cuda:0')
c= tensor(4.4860e+09, device='cuda:0')
c= tensor(4.4868e+09, device='cuda:0')
c= tensor(4.5700e+09, device='cuda:0')
c= tensor(4.5701e+09, device='cuda:0')
c= tensor(4.5705e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5708e+09, device='cuda:0')
c= tensor(4.5732e+09, device='cuda:0')
c= tensor(4.5732e+09, device='cuda:0')
c= tensor(4.5858e+09, device='cuda:0')
c= tensor(4.5859e+09, device='cuda:0')
c= tensor(4.5860e+09, device='cuda:0')
c= tensor(4.5862e+09, device='cuda:0')
c= tensor(4.5995e+09, device='cuda:0')
c= tensor(4.6154e+09, device='cuda:0')
c= tensor(4.6481e+09, device='cuda:0')
c= tensor(4.6482e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.6483e+09, device='cuda:0')
c= tensor(4.7476e+09, device='cuda:0')
c= tensor(4.7476e+09, device='cuda:0')
c= tensor(4.7478e+09, device='cuda:0')
c= tensor(4.7492e+09, device='cuda:0')
c= tensor(4.7500e+09, device='cuda:0')
c= tensor(4.7512e+09, device='cuda:0')
c= tensor(4.7513e+09, device='cuda:0')
c= tensor(4.7721e+09, device='cuda:0')
c= tensor(4.7737e+09, device='cuda:0')
c= tensor(4.7738e+09, device='cuda:0')
c= tensor(4.7747e+09, device='cuda:0')
c= tensor(4.7843e+09, device='cuda:0')
c= tensor(4.7870e+09, device='cuda:0')
c= tensor(4.8645e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8693e+09, device='cuda:0')
c= tensor(4.8699e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8705e+09, device='cuda:0')
c= tensor(4.8706e+09, device='cuda:0')
c= tensor(4.8706e+09, device='cuda:0')
c= tensor(4.8721e+09, device='cuda:0')
c= tensor(4.8742e+09, device='cuda:0')
c= tensor(4.8742e+09, device='cuda:0')
c= tensor(4.8744e+09, device='cuda:0')
c= tensor(4.8745e+09, device='cuda:0')
c= tensor(4.8752e+09, device='cuda:0')
c= tensor(4.8752e+09, device='cuda:0')
c= tensor(4.8756e+09, device='cuda:0')
c= tensor(4.8766e+09, device='cuda:0')
c= tensor(4.8767e+09, device='cuda:0')
c= tensor(4.8767e+09, device='cuda:0')
c= tensor(4.8768e+09, device='cuda:0')
c= tensor(4.8771e+09, device='cuda:0')
c= tensor(4.8873e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8874e+09, device='cuda:0')
c= tensor(4.8908e+09, device='cuda:0')
c= tensor(5.0073e+09, device='cuda:0')
c= tensor(5.0147e+09, device='cuda:0')
c= tensor(5.0148e+09, device='cuda:0')
c= tensor(5.0500e+09, device='cuda:0')
c= tensor(5.0585e+09, device='cuda:0')
c= tensor(5.0585e+09, device='cuda:0')
c= tensor(5.0586e+09, device='cuda:0')
c= tensor(5.0594e+09, device='cuda:0')
c= tensor(5.0760e+09, device='cuda:0')
c= tensor(5.1450e+09, device='cuda:0')
c= tensor(5.1539e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1553e+09, device='cuda:0')
c= tensor(5.1576e+09, device='cuda:0')
c= tensor(5.1577e+09, device='cuda:0')
c= tensor(5.1580e+09, device='cuda:0')
c= tensor(5.1633e+09, device='cuda:0')
c= tensor(5.2310e+09, device='cuda:0')
c= tensor(5.2311e+09, device='cuda:0')
c= tensor(5.2311e+09, device='cuda:0')
c= tensor(5.2313e+09, device='cuda:0')
c= tensor(5.2437e+09, device='cuda:0')
c= tensor(5.2455e+09, device='cuda:0')
c= tensor(5.2459e+09, device='cuda:0')
c= tensor(5.2459e+09, device='cuda:0')
c= tensor(5.2465e+09, device='cuda:0')
c= tensor(5.2465e+09, device='cuda:0')
c= tensor(5.2478e+09, device='cuda:0')
c= tensor(5.2479e+09, device='cuda:0')
c= tensor(5.2480e+09, device='cuda:0')
c= tensor(5.2481e+09, device='cuda:0')
c= tensor(5.2481e+09, device='cuda:0')
c= tensor(5.2483e+09, device='cuda:0')
c= tensor(5.2566e+09, device='cuda:0')
c= tensor(5.2865e+09, device='cuda:0')
c= tensor(5.2903e+09, device='cuda:0')
c= tensor(5.3036e+09, device='cuda:0')
c= tensor(5.3038e+09, device='cuda:0')
c= tensor(5.3040e+09, device='cuda:0')
c= tensor(5.3043e+09, device='cuda:0')
c= tensor(5.3090e+09, device='cuda:0')
c= tensor(5.3094e+09, device='cuda:0')
c= tensor(5.3110e+09, device='cuda:0')
c= tensor(5.3110e+09, device='cuda:0')
c= tensor(5.7044e+09, device='cuda:0')
c= tensor(5.7046e+09, device='cuda:0')
c= tensor(5.7062e+09, device='cuda:0')
c= tensor(5.7210e+09, device='cuda:0')
c= tensor(5.7224e+09, device='cuda:0')
c= tensor(5.7233e+09, device='cuda:0')
c= tensor(6.0591e+09, device='cuda:0')
c= tensor(6.0771e+09, device='cuda:0')
c= tensor(6.0811e+09, device='cuda:0')
c= tensor(6.0812e+09, device='cuda:0')
c= tensor(6.0823e+09, device='cuda:0')
c= tensor(6.0823e+09, device='cuda:0')
c= tensor(6.1106e+09, device='cuda:0')
c= tensor(6.4146e+09, device='cuda:0')
c= tensor(6.4231e+09, device='cuda:0')
c= tensor(6.4297e+09, device='cuda:0')
c= tensor(6.4365e+09, device='cuda:0')
c= tensor(6.4370e+09, device='cuda:0')
c= tensor(6.4377e+09, device='cuda:0')
c= tensor(6.4378e+09, device='cuda:0')
c= tensor(6.4623e+09, device='cuda:0')
c= tensor(6.5217e+09, device='cuda:0')
c= tensor(6.5249e+09, device='cuda:0')
c= tensor(6.9076e+09, device='cuda:0')
c= tensor(6.9186e+09, device='cuda:0')
c= tensor(6.9197e+09, device='cuda:0')
c= tensor(6.9201e+09, device='cuda:0')
c= tensor(6.9274e+09, device='cuda:0')
c= tensor(6.9359e+09, device='cuda:0')
c= tensor(6.9360e+09, device='cuda:0')
c= tensor(7.0099e+09, device='cuda:0')
c= tensor(7.0110e+09, device='cuda:0')
c= tensor(7.0120e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0121e+09, device='cuda:0')
c= tensor(7.0122e+09, device='cuda:0')
c= tensor(7.0125e+09, device='cuda:0')
c= tensor(7.0190e+09, device='cuda:0')
c= tensor(9.5364e+09, device='cuda:0')
c= tensor(9.5376e+09, device='cuda:0')
c= tensor(9.5647e+09, device='cuda:0')
c= tensor(9.5648e+09, device='cuda:0')
c= tensor(9.5650e+09, device='cuda:0')
c= tensor(9.5650e+09, device='cuda:0')
c= tensor(9.5781e+09, device='cuda:0')
c= tensor(9.5799e+09, device='cuda:0')
c= tensor(9.8165e+09, device='cuda:0')
c= tensor(9.8165e+09, device='cuda:0')
c= tensor(9.8259e+09, device='cuda:0')
c= tensor(9.8263e+09, device='cuda:0')
c= tensor(9.8331e+09, device='cuda:0')
c= tensor(9.8574e+09, device='cuda:0')
c= tensor(9.8577e+09, device='cuda:0')
c= tensor(9.8577e+09, device='cuda:0')
c= tensor(9.8613e+09, device='cuda:0')
c= tensor(9.8619e+09, device='cuda:0')
c= tensor(9.8626e+09, device='cuda:0')
c= tensor(9.8959e+09, device='cuda:0')
c= tensor(9.8967e+09, device='cuda:0')
c= tensor(9.9000e+09, device='cuda:0')
c= tensor(9.9005e+09, device='cuda:0')
c= tensor(9.9052e+09, device='cuda:0')
c= tensor(9.9266e+09, device='cuda:0')
c= tensor(9.9279e+09, device='cuda:0')
c= tensor(9.9281e+09, device='cuda:0')
c= tensor(9.9365e+09, device='cuda:0')
c= tensor(9.9393e+09, device='cuda:0')
c= tensor(1.0020e+10, device='cuda:0')
c= tensor(1.0023e+10, device='cuda:0')
c= tensor(1.0025e+10, device='cuda:0')
c= tensor(1.0026e+10, device='cuda:0')
c= tensor(1.0051e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0060e+10, device='cuda:0')
c= tensor(1.0063e+10, device='cuda:0')
c= tensor(1.0069e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0071e+10, device='cuda:0')
c= tensor(1.0072e+10, device='cuda:0')
c= tensor(1.0073e+10, device='cuda:0')
c= tensor(1.0074e+10, device='cuda:0')
c= tensor(1.0075e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0076e+10, device='cuda:0')
c= tensor(1.0104e+10, device='cuda:0')
c= tensor(1.0105e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0109e+10, device='cuda:0')
c= tensor(1.0110e+10, device='cuda:0')
c= tensor(1.0112e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0113e+10, device='cuda:0')
c= tensor(1.0114e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0119e+10, device='cuda:0')
c= tensor(1.0122e+10, device='cuda:0')
c= tensor(1.0122e+10, device='cuda:0')
c= tensor(1.0145e+10, device='cuda:0')
c= tensor(1.0145e+10, device='cuda:0')
c= tensor(1.0152e+10, device='cuda:0')
c= tensor(1.0157e+10, device='cuda:0')
c= tensor(1.0161e+10, device='cuda:0')
c= tensor(1.0169e+10, device='cuda:0')
c= tensor(1.0174e+10, device='cuda:0')
c= tensor(1.0174e+10, device='cuda:0')
c= tensor(1.0176e+10, device='cuda:0')
c= tensor(1.0176e+10, device='cuda:0')
c= tensor(1.0182e+10, device='cuda:0')
c= tensor(1.0187e+10, device='cuda:0')
c= tensor(1.0188e+10, device='cuda:0')
c= tensor(1.0189e+10, device='cuda:0')
c= tensor(1.0190e+10, device='cuda:0')
c= tensor(1.0194e+10, device='cuda:0')
c= tensor(1.0194e+10, device='cuda:0')
c= tensor(1.0195e+10, device='cuda:0')
c= tensor(1.0212e+10, device='cuda:0')
c= tensor(1.0286e+10, device='cuda:0')
c= tensor(1.0286e+10, device='cuda:0')
c= tensor(1.0287e+10, device='cuda:0')
c= tensor(1.0287e+10, device='cuda:0')
c= tensor(1.0293e+10, device='cuda:0')
c= tensor(1.0294e+10, device='cuda:0')
c= tensor(1.0294e+10, device='cuda:0')
c= tensor(1.0296e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0302e+10, device='cuda:0')
c= tensor(1.0304e+10, device='cuda:0')
c= tensor(1.0305e+10, device='cuda:0')
time to make c is 7.925119400024414
time for making loss is 7.925241708755493
p0 True
it  0 : 3278003200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6130393088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6130712576
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2850975200.0
relative error loss 0.27665204
shape of L is 
torch.Size([])
memory (bytes)
6156533760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6156533760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2840473600.0
relative error loss 0.27563298
shape of L is 
torch.Size([])
memory (bytes)
6160121856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6160211968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2779543000.0
relative error loss 0.2697204
shape of L is 
torch.Size([])
memory (bytes)
6163210240
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6163398656
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2741700600.0
relative error loss 0.26604828
shape of L is 
torch.Size([])
memory (bytes)
6166585344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
6166589440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2704981000.0
relative error loss 0.2624851
shape of L is 
torch.Size([])
memory (bytes)
6169747456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6169747456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2683894300.0
relative error loss 0.2604389
shape of L is 
torch.Size([])
memory (bytes)
6172958720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6172987392
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2665268700.0
relative error loss 0.2586315
shape of L is 
torch.Size([])
memory (bytes)
6176178176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6176178176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2650894300.0
relative error loss 0.25723666
shape of L is 
torch.Size([])
memory (bytes)
6179377152
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6179377152
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2634782700.0
relative error loss 0.2556732
shape of L is 
torch.Size([])
memory (bytes)
6182477824
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6182604800
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 93% | 17% |
error is  2627488300.0
relative error loss 0.25496536
time to take a step is 235.05017590522766
it  1 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6185684992
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6185811968
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2627488300.0
relative error loss 0.25496536
shape of L is 
torch.Size([])
memory (bytes)
6188929024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6189072384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2615245800.0
relative error loss 0.25377738
shape of L is 
torch.Size([])
memory (bytes)
6192041984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6192283648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  2607181800.0
relative error loss 0.2529949
shape of L is 
torch.Size([])
memory (bytes)
6195367936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6195494912
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2598148000.0
relative error loss 0.25211826
shape of L is 
torch.Size([])
memory (bytes)
6198439936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 33% | 17% |
memory (bytes)
6198685696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2592027600.0
relative error loss 0.25152436
shape of L is 
torch.Size([])
memory (bytes)
6201896960
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6201896960
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2586204700.0
relative error loss 0.2509593
shape of L is 
torch.Size([])
memory (bytes)
6205034496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 20% | 17% |
memory (bytes)
6205034496
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2581488600.0
relative error loss 0.25050166
shape of L is 
torch.Size([])
memory (bytes)
6208184320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6208303104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2574507500.0
relative error loss 0.24982424
shape of L is 
torch.Size([])
memory (bytes)
6211301376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6211514368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2571607000.0
relative error loss 0.24954279
shape of L is 
torch.Size([])
memory (bytes)
6214725632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6214725632
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2568013300.0
relative error loss 0.24919406
time to take a step is 231.12948369979858
it  2 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6217936896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6217936896
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2568013300.0
relative error loss 0.24919406
shape of L is 
torch.Size([])
memory (bytes)
6221139968
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6221144064
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2562943500.0
relative error loss 0.2487021
shape of L is 
torch.Size([])
memory (bytes)
6224228352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6224355328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2561055700.0
relative error loss 0.24851891
shape of L is 
torch.Size([])
memory (bytes)
6227554304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6227554304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2555575300.0
relative error loss 0.2479871
shape of L is 
torch.Size([])
memory (bytes)
6230642688
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6230769664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2553969200.0
relative error loss 0.24783126
shape of L is 
torch.Size([])
memory (bytes)
6233821184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6233976832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2550598700.0
relative error loss 0.24750419
shape of L is 
torch.Size([])
memory (bytes)
6237143040
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6237179904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2547744800.0
relative error loss 0.24722725
shape of L is 
torch.Size([])
memory (bytes)
6240354304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6240354304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2545890300.0
relative error loss 0.24704729
shape of L is 
torch.Size([])
memory (bytes)
6243454976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6243577856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2544142300.0
relative error loss 0.24687769
shape of L is 
torch.Size([])
memory (bytes)
6246674432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6246674432
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2542489600.0
relative error loss 0.2467173
time to take a step is 231.58592677116394
it  3 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6249996288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6250000384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2542489600.0
relative error loss 0.2467173
shape of L is 
torch.Size([])
memory (bytes)
6253031424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6253203456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2540722700.0
relative error loss 0.24654584
shape of L is 
torch.Size([])
memory (bytes)
6256406528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6256406528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2538627600.0
relative error loss 0.24634254
shape of L is 
torch.Size([])
memory (bytes)
6259568640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6259609600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2536739300.0
relative error loss 0.2461593
shape of L is 
torch.Size([])
memory (bytes)
6262689792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6262812672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2534584300.0
relative error loss 0.24595019
shape of L is 
torch.Size([])
memory (bytes)
6266007552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6266015744
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2533071400.0
relative error loss 0.24580337
shape of L is 
torch.Size([])
memory (bytes)
6269227008
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6269227008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2531474000.0
relative error loss 0.24564837
shape of L is 
torch.Size([])
memory (bytes)
6272401408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6272401408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2530077700.0
relative error loss 0.24551287
shape of L is 
torch.Size([])
memory (bytes)
6275543040
| ID | GPU | MEM |
------------------
|  0 |  2% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6275637248
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2529108500.0
relative error loss 0.24541883
shape of L is 
torch.Size([])
memory (bytes)
6278717440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6278717440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 17% |
error is  2528048600.0
relative error loss 0.24531598
time to take a step is 231.15325236320496
it  4 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6282051584
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6282055680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2528048600.0
relative error loss 0.24531598
shape of L is 
torch.Size([])
memory (bytes)
6285217792
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 19% | 17% |
memory (bytes)
6285217792
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2526135800.0
relative error loss 0.24513036
shape of L is 
torch.Size([])
memory (bytes)
6288453632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6288453632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  2525546500.0
relative error loss 0.24507318
shape of L is 
torch.Size([])
memory (bytes)
6291607552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6291607552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2524598300.0
relative error loss 0.24498117
shape of L is 
torch.Size([])
memory (bytes)
6294757376
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 17% |
memory (bytes)
6294884352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2523999200.0
relative error loss 0.24492304
shape of L is 
torch.Size([])
memory (bytes)
6297956352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
6298083328
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2523031000.0
relative error loss 0.24482909
shape of L is 
torch.Size([])
memory (bytes)
6301171712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6301298688
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2522004500.0
relative error loss 0.24472947
shape of L is 
torch.Size([])
memory (bytes)
6304477184
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6304477184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2520815600.0
relative error loss 0.24461411
shape of L is 
torch.Size([])
memory (bytes)
6307606528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6307704832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2520046000.0
relative error loss 0.24453944
shape of L is 
torch.Size([])
memory (bytes)
6310797312
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6310924288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2518813700.0
relative error loss 0.24441984
time to take a step is 232.15890860557556
it  5 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6314131456
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6314131456
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2518813700.0
relative error loss 0.24441984
shape of L is 
torch.Size([])
memory (bytes)
6317182976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6317182976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  2518095400.0
relative error loss 0.24435014
shape of L is 
torch.Size([])
memory (bytes)
6320463872
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6320549888
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2517066800.0
relative error loss 0.24425033
shape of L is 
torch.Size([])
memory (bytes)
6323769344
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6323769344
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2516216800.0
relative error loss 0.24416785
shape of L is 
torch.Size([])
memory (bytes)
6326964224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6326964224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 96% | 17% |
error is  2515566600.0
relative error loss 0.24410476
shape of L is 
torch.Size([])
memory (bytes)
6330073088
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6330155008
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2514119700.0
relative error loss 0.24396434
shape of L is 
torch.Size([])
memory (bytes)
6333370368
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6333370368
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2513020000.0
relative error loss 0.24385762
shape of L is 
torch.Size([])
memory (bytes)
6336430080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6336585728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2512400400.0
relative error loss 0.24379751
shape of L is 
torch.Size([])
memory (bytes)
6339784704
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6339788800
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2511852000.0
relative error loss 0.2437443
shape of L is 
torch.Size([])
memory (bytes)
6342840320
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
6342840320
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2511007700.0
relative error loss 0.24366237
time to take a step is 231.99358820915222
it  6 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6346203136
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6346203136
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2511007700.0
relative error loss 0.24366237
shape of L is 
torch.Size([])
memory (bytes)
6349344768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6349344768
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2510972000.0
relative error loss 0.2436589
shape of L is 
torch.Size([])
memory (bytes)
6352625664
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6352625664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2510389200.0
relative error loss 0.24360235
shape of L is 
torch.Size([])
memory (bytes)
6355824640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6355824640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  2509453800.0
relative error loss 0.24351159
shape of L is 
torch.Size([])
memory (bytes)
6358876160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 27% | 17% |
memory (bytes)
6359031808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2508767700.0
relative error loss 0.24344501
shape of L is 
torch.Size([])
memory (bytes)
6362116096
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6362238976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  2508223000.0
relative error loss 0.24339214
shape of L is 
torch.Size([])
memory (bytes)
6365446144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6365446144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2507695000.0
relative error loss 0.24334092
shape of L is 
torch.Size([])
memory (bytes)
6368641024
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6368645120
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2507207700.0
relative error loss 0.24329363
shape of L is 
torch.Size([])
memory (bytes)
6371827712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
6371827712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2506692000.0
relative error loss 0.24324359
shape of L is 
torch.Size([])
memory (bytes)
6375018496
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6375051264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 91% | 17% |
error is  2506175500.0
relative error loss 0.24319346
time to take a step is 233.71734166145325
it  7 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6378135552
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6378135552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2506175500.0
relative error loss 0.24319346
shape of L is 
torch.Size([])
memory (bytes)
6381465600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6381465600
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2505845200.0
relative error loss 0.24316141
shape of L is 
torch.Size([])
memory (bytes)
6384627712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6384627712
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2505392000.0
relative error loss 0.24311745
shape of L is 
torch.Size([])
memory (bytes)
6387879936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6387879936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2505227800.0
relative error loss 0.24310149
shape of L is 
torch.Size([])
memory (bytes)
6391033856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6391033856
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2504878600.0
relative error loss 0.24306762
shape of L is 
torch.Size([])
memory (bytes)
6394167296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6394290176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2504711200.0
relative error loss 0.24305137
shape of L is 
torch.Size([])
memory (bytes)
6397497344
| ID | GPU | MEM |
------------------
|  0 | 23% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6397497344
| ID | GPU  | MEM |
-------------------
|  0 |  22% |  0% |
|  1 | 100% | 17% |
error is  2504495600.0
relative error loss 0.24303046
shape of L is 
torch.Size([])
memory (bytes)
6400692224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
6400704512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2504116200.0
relative error loss 0.24299364
shape of L is 
torch.Size([])
memory (bytes)
6403792896
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6403919872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2504043000.0
relative error loss 0.24298653
shape of L is 
torch.Size([])
memory (bytes)
6407049216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6407049216
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2503381500.0
relative error loss 0.24292234
time to take a step is 232.0602731704712
it  8 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6410342400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6410342400
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2503381500.0
relative error loss 0.24292234
shape of L is 
torch.Size([])
memory (bytes)
6413504512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6413504512
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2503172000.0
relative error loss 0.24290203
shape of L is 
torch.Size([])
memory (bytes)
6416760832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 21% | 17% |
memory (bytes)
6416760832
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2502620700.0
relative error loss 0.24284852
shape of L is 
torch.Size([])
memory (bytes)
6419808256
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 24% | 17% |
memory (bytes)
6419963904
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2502124000.0
relative error loss 0.24280033
shape of L is 
torch.Size([])
memory (bytes)
6423171072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6423171072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2501523000.0
relative error loss 0.24274199
shape of L is 
torch.Size([])
memory (bytes)
6426251264
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6426378240
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2500949500.0
relative error loss 0.24268635
shape of L is 
torch.Size([])
memory (bytes)
6429458432
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6429585408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2500481000.0
relative error loss 0.24264088
shape of L is 
torch.Size([])
memory (bytes)
6432661504
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6432784384
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2500000300.0
relative error loss 0.24259423
shape of L is 
torch.Size([])
memory (bytes)
6435823616
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6435991552
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2499479600.0
relative error loss 0.24254371
shape of L is 
torch.Size([])
memory (bytes)
6439067648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6439194624
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2499199500.0
relative error loss 0.24251653
time to take a step is 232.3401825428009
it  9 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6442246144
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 11% | 17% |
memory (bytes)
6442397696
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2499199500.0
relative error loss 0.24251653
shape of L is 
torch.Size([])
memory (bytes)
6445596672
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6445596672
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2498936800.0
relative error loss 0.24249104
shape of L is 
torch.Size([])
memory (bytes)
6448795648
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6448795648
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2498730500.0
relative error loss 0.24247102
shape of L is 
torch.Size([])
memory (bytes)
6452023296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6452023296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2498545200.0
relative error loss 0.24245304
shape of L is 
torch.Size([])
memory (bytes)
6455111680
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6455111680
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2498119200.0
relative error loss 0.2424117
shape of L is 
torch.Size([])
memory (bytes)
6458449920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6458449920
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2497530400.0
relative error loss 0.24235456
shape of L is 
torch.Size([])
memory (bytes)
6461538304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6461661184
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2496998000.0
relative error loss 0.2423029
shape of L is 
torch.Size([])
memory (bytes)
6464860160
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6464860160
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2496602000.0
relative error loss 0.24226448
shape of L is 
torch.Size([])
memory (bytes)
6467948544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 28% | 17% |
memory (bytes)
6467948544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2496164900.0
relative error loss 0.24222206
shape of L is 
torch.Size([])
memory (bytes)
6471286784
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6471286784
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2495759400.0
relative error loss 0.2421827
time to take a step is 233.33820867538452
it  10 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6474366976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6474489856
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2495759400.0
relative error loss 0.2421827
shape of L is 
torch.Size([])
memory (bytes)
6477688832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6477688832
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 92% | 17% |
error is  2495249000.0
relative error loss 0.24213317
shape of L is 
torch.Size([])
memory (bytes)
6480908288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6480908288
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2494906400.0
relative error loss 0.24209994
shape of L is 
torch.Size([])
memory (bytes)
6484037632
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6484107264
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2494586400.0
relative error loss 0.24206889
shape of L is 
torch.Size([])
memory (bytes)
6487318528
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 22% | 17% |
memory (bytes)
6487318528
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2494275600.0
relative error loss 0.24203873
shape of L is 
torch.Size([])
memory (bytes)
6490398720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6490398720
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  2494066200.0
relative error loss 0.2420184
shape of L is 
torch.Size([])
memory (bytes)
6493605888
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6493732864
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2493871000.0
relative error loss 0.24199948
shape of L is 
torch.Size([])
memory (bytes)
6496841728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6496927744
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2493656000.0
relative error loss 0.2419786
shape of L is 
torch.Size([])
memory (bytes)
6500020224
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6500143104
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2493416000.0
relative error loss 0.24195531
shape of L is 
torch.Size([])
memory (bytes)
6503223296
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6503223296
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2493243000.0
relative error loss 0.24193852
time to take a step is 238.85988211631775
it  11 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6506557440
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6506557440
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2493243000.0
relative error loss 0.24193852
shape of L is 
torch.Size([])
memory (bytes)
6509592576
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6509768704
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2492860000.0
relative error loss 0.24190135
shape of L is 
torch.Size([])
memory (bytes)
6512885760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6512975872
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2492770800.0
relative error loss 0.24189271
shape of L is 
torch.Size([])
memory (bytes)
6516072448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6516072448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2492569000.0
relative error loss 0.24187313
shape of L is 
torch.Size([])
memory (bytes)
6519386112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6519390208
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2492449300.0
relative error loss 0.2418615
shape of L is 
torch.Size([])
memory (bytes)
6522478592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6522478592
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2492234800.0
relative error loss 0.24184069
shape of L is 
torch.Size([])
memory (bytes)
6525804544
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 17% |
memory (bytes)
6525808640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2492067800.0
relative error loss 0.2418245
shape of L is 
torch.Size([])
memory (bytes)
6528872448
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6529024000
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2491930000.0
relative error loss 0.24181113
shape of L is 
torch.Size([])
memory (bytes)
6532222976
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 30% | 17% |
memory (bytes)
6532222976
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2491631600.0
relative error loss 0.24178216
shape of L is 
torch.Size([])
memory (bytes)
6535278592
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6535438336
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2491441700.0
relative error loss 0.24176373
time to take a step is 239.28562712669373
it  12 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6538641408
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 16% | 17% |
memory (bytes)
6538641408
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2491441700.0
relative error loss 0.24176373
shape of L is 
torch.Size([])
memory (bytes)
6541762560
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6541762560
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2491119000.0
relative error loss 0.24173243
shape of L is 
torch.Size([])
memory (bytes)
6545063936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6545063936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2490970000.0
relative error loss 0.24171796
shape of L is 
torch.Size([])
memory (bytes)
6548135936
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6548135936
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2490691000.0
relative error loss 0.24169089
shape of L is 
torch.Size([])
memory (bytes)
6551474176
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6551474176
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2490496500.0
relative error loss 0.24167201
shape of L is 
torch.Size([])
memory (bytes)
6554636288
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6554636288
| ID | GPU  | MEM |
-------------------
|  0 |   6% |  0% |
|  1 | 100% | 17% |
error is  2490246100.0
relative error loss 0.24164772
shape of L is 
torch.Size([])
memory (bytes)
6557892608
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6557892608
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2490052600.0
relative error loss 0.24162893
shape of L is 
torch.Size([])
memory (bytes)
6561013760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6561099776
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 98% | 17% |
error is  2489876000.0
relative error loss 0.2416118
shape of L is 
torch.Size([])
memory (bytes)
6564306944
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6564306944
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2489521700.0
relative error loss 0.24157742
shape of L is 
torch.Size([])
memory (bytes)
6567514112
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6567514112
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2489220600.0
relative error loss 0.2415482
time to take a step is 236.1547873020172
it  13 : 3938803200
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6570618880
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6570725376
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2489220600.0
relative error loss 0.2415482
shape of L is 
torch.Size([])
memory (bytes)
6573936640
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6573936640
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2488989700.0
relative error loss 0.2415258
shape of L is 
torch.Size([])
memory (bytes)
6577094656
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6577147904
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2488823300.0
relative error loss 0.24150965
shape of L is 
torch.Size([])
memory (bytes)
6580355072
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 31% | 17% |
memory (bytes)
6580359168
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2488569300.0
relative error loss 0.241485
shape of L is 
torch.Size([])
memory (bytes)
6583439360
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6583439360
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2488507400.0
relative error loss 0.241479
shape of L is 
torch.Size([])
memory (bytes)
6586777600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6586777600
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  2488246800.0
relative error loss 0.2414537
shape of L is 
torch.Size([])
memory (bytes)
6589943808
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6589943808
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2488087000.0
relative error loss 0.2414382
shape of L is 
torch.Size([])
memory (bytes)
6593200128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6593200128
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2487891000.0
relative error loss 0.24141917
shape of L is 
torch.Size([])
memory (bytes)
6596341760
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 14% | 17% |
memory (bytes)
6596341760
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2487624200.0
relative error loss 0.24139328
shape of L is 
torch.Size([])
memory (bytes)
6599598080
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 15% | 17% |
memory (bytes)
6599598080
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2487714300.0
relative error loss 0.24140203
shape of L is 
torch.Size([])
memory (bytes)
6602653696
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 25% | 17% |
memory (bytes)
6602797056
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2487436800.0
relative error loss 0.2413751
time to take a step is 246.77016639709473
it  14 : 3938803712
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
shape of L is 
torch.Size([])
memory (bytes)
6605905920
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6606016512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 94% | 17% |
error is  2487436800.0
relative error loss 0.2413751
shape of L is 
torch.Size([])
memory (bytes)
6609215488
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6609215488
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2487139300.0
relative error loss 0.24134624
shape of L is 
torch.Size([])
memory (bytes)
6612324352
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 32% | 17% |
memory (bytes)
6612324352
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2486876700.0
relative error loss 0.24132074
shape of L is 
torch.Size([])
memory (bytes)
6615625728
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 23% | 17% |
memory (bytes)
6615625728
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2486511000.0
relative error loss 0.24128528
shape of L is 
torch.Size([])
memory (bytes)
6618722304
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 29% | 17% |
memory (bytes)
6618722304
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2486262300.0
relative error loss 0.24126112
shape of L is 
torch.Size([])
memory (bytes)
6622040064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 12% | 17% |
memory (bytes)
6622040064
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 99% | 17% |
error is  2486059500.0
relative error loss 0.24124146
shape of L is 
torch.Size([])
memory (bytes)
6625193984
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 26% | 17% |
memory (bytes)
6625255424
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2485889500.0
relative error loss 0.24122496
shape of L is 
torch.Size([])
memory (bytes)
6628327424
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 13% | 17% |
memory (bytes)
6628454400
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2485679000.0
relative error loss 0.24120454
shape of L is 
torch.Size([])
memory (bytes)
6631661568
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 18% | 17% |
memory (bytes)
6631665664
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2485374500.0
relative error loss 0.24117498
shape of L is 
torch.Size([])
memory (bytes)
6634688512
| ID | GPU | MEM |
------------------
|  0 |  0% |  0% |
|  1 | 17% | 17% |
memory (bytes)
6634860544
| ID | GPU  | MEM |
-------------------
|  0 |   0% |  0% |
|  1 | 100% | 17% |
error is  2485175300.0
relative error loss 0.24115565
time to take a step is 233.53359389305115
sum tnnu_Z after tensor(17657388., device='cuda:0')
shape of features
(6509,)
shape of features
(6509,)
number of orig particles 26036
number of new particles after remove low mass 23152
tnuZ shape should be parts x labs
torch.Size([26036, 702])
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.
error without small mass is  2850462500.0
relative error without small mass is  0.27660227
nnu_Z shape should be number of particles by maxV
(26036, 702)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
shape of features
(26036,)
Wed Feb 1 02:44:16 EST 2023
