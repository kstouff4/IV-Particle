# -*- coding: utf-8 -*-
"""ApproxSliced.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VG0MQwQzUItBSF2BNksSW7NJJhwcTGL0

Implementation of an approximation scheme by slicing along bandwidths to speed up computation with pykeops :

$$\mu = \sum_{b\in B}{(i_b\circ\pi_b)}_{\#}\mu= \sum_{b\in Bands}{i_b}_{\#}\mu_b$$

where the features spaces is sliced into disjoint bands
$$\mathcal F=\cup_{b\in B}\mathcal{F_b}$$
and $\mu_b={\pi_b}_{\#}\mu$ where $\pi_b:\mathbb{R}^d\times\mathcal{F}\to \mathbb{R}^d\times\mathcal{F}_b$ is the canonical projection and $i_b:\mathbb{R}^d\times\mathcal{F}_b\to \mathbb{R}^d\times\mathcal{F}$ is the inclusion embedding.

The core equality is: 
$$\|\mu\|_{W^*}^2=\sum_{b\in B}\|\mu_b\|_{W^*}^2$$
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pykeops > install.log
!pip install git+github.com/
import time

import torch

from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
import numpy as np
import numpy.matlib
from pykeops.torch import Vi, Vj
from pykeops.torch import LazyTensor

dtype = torch.cuda.FloatTensor 
torch.set_default_tensor_type(dtype)

def project3DHtoL(X,nu_X,Z, nu_Z,sig, outfile = 'out', bw = 75):

  def make_loss(X,nu_X):
  
    tx = torch.tensor(X).type(dtype).contiguous()
    LX_i, LX_j= Vi(tx), Vj(tx)

    tnu_X = torch.tensor(nu_X).type(dtype).contiguous()
    Lnu_X_i, Lnu_X_j = Vi(tnu_X), Vj(tnu_X)

    D_ij = ((LX_i - LX_j)**2/sig**2).sum(dim=2)  
    K_ij = (- D_ij).exp()    
    P_ij = (Lnu_X_i*Lnu_X_j).sum(dim=2)
    c= (K_ij*P_ij).sum(dim=1).sum()
    print('c=',c)

    def loss(tZal_Z):

      LZ_i, LZ_j = Vi(tZal_Z[0]), Vj(tZal_Z[0])
    
      Lnu_Z_i= Vi(tZal_Z[1]**2)
      Lnu_Z_j= Vj(tZal_Z[1]**2)

      DZZ_ij = ((LZ_i - LZ_j)**2/sig**2).sum(dim=2)  
      KZZ_ij = (- DZZ_ij).exp()    
      PZZ_ij = (Lnu_Z_i*Lnu_Z_j).sum(dim=2)

      DZX_ij = ((LZ_i - LX_j)**2/sig**2).sum(dim=2) 
      KZX_ij = (- DZX_ij).exp() 
      PZX_ij = (Lnu_Z_i*Lnu_X_j).sum(dim=2)
      
      E = (KZZ_ij*PZZ_ij).sum(dim=1)-2*(KZX_ij*PZX_ij).sum(dim=1)
      L = E.sum() +c
      return L
    return loss 
    
  def make_loss2(X,nu_X,Z):
  
    tx = torch.tensor(X).type(dtype).contiguous()
    LX_i= Vi(tx)
    LX_j= Vj(tx)
    Lnu_X_i= Vi(torch.tensor(nu_X).type(dtype).contiguous())
    Lnu_X_j= Vj(torch.tensor(nu_X).type(dtype).contiguous())

    D_ij = ((LX_i - LX_j)**2/sig**2).sum(dim=2)  
    K_ij = (- D_ij).exp()    
    P_ij = (Lnu_X_i*Lnu_X_j).sum(dim=2)
    c= (K_ij*P_ij).sum(dim=1).sum()

    tz = torch.tensor(Z).type(dtype).contiguous()
    LZ_i, LZ_j= Vi(tz), Vj(tz)
    DZZ_ij = ((LZ_i - LZ_j)**2/sig**2).sum(dim=2)  
    KZZ_ij = (- DZZ_ij).exp()    

    print('c=',c)

    def loss(tal_Z):   
      Lnu_Z_i, Lnu_Z_j = Vi(tal_Z[0]**2), Vj(tal_Z[0]**2)

      PZZ_ij = (Lnu_Z_i*Lnu_Z_j).sum(dim=2)

      DZX_ij = ((LZ_i - LX_j)**2/sig**2).sum(dim=2) 
      KZX_ij = (- DZX_ij).exp() 
      PZX_ij = (Lnu_Z_i*Lnu_X_j).sum(dim=2)
      
      E = (KZZ_ij*PZZ_ij).sum(dim=1)-2*(KZX_ij*PZX_ij).sum(dim=1)
      L = E.sum() +c
      return L
    return loss

  print("mass nu_Z",nu_Z.sum())
  print("mass nu_X", nu_X.sum())

  # Optimization

  nb_bands = int(nu_Z.shape[1]/bw)+1

  bands = [(i*bw, min((i+1)*bw,nu_Z.shape[1])) for i in range(nb_bands)]

  def optimize(Z, nu_Z, nb_iter = 20, flag = 'all'):
    if flag == 'all':
      ltmploss = [make_loss(X,nu_X[:,bands[i][0]:bands[i][1]]) for i in range(nb_bands)]
      def uloss(xu):
        return sum([ltmploss[i]([xu[0],xu[1][:,bands[i][0]:bands[i][1]]]) for i in range(nb_bands)])

      x_init=[torch.tensor(Z).type(dtype), torch.tensor(nu_Z).type(dtype).sqrt()]
      dxmax = [sig, x_init[1].mean()]
      print(dxmax)
    else:
      ltmploss = [make_loss2(X,
                            nu_X[:,bands[i][0]:bands[i][1]],
                            Z) for i in range(nb_bands)]
      def uloss(xu):
        return sum([ltmploss[i]([xu[0][:,bands[i][0]:bands[i][1]]]) for i in range(nb_bands)])
      print("uloss done")
      x_init = [torch.tensor(nu_Z).type(dtype).sqrt()]
      print(x_init[0].type())
      dxmax = [x_init[0].mean()]
      print(dxmax[0].type())
    
    #  normalize the loss
    beta = uloss(x_init)
    def nuloss(xu):
      return uloss(xu)/beta

    optloss, ufromopt, xopt = rescale_loss(nuloss,x_init, dxmax)
    optimizer = torch.optim.LBFGS(xopt, max_iter=15, line_search_fn = 'strong_wolfe', history_size = 10)
    
    def closure():
            optimizer.zero_grad()
            L = optloss(xopt)
            print("loss", L.detach().cpu().numpy())
            #print("nu_Z:", ufromopt(xopt)[-1]**2)
            #print("nu_Z max:", (ufromopt(xopt)[-1]**2).max())
            L.backward()
            return L

    for i in range(nb_iter):
        print("it ", i, ": ", end="")
        optimizer.step(closure)
        if flag == 'all':
          xu = ufromopt(xopt)  
          nZ = xu[0].detach().cpu().numpy()
          nnu_Z = xu[1].detach().cpu().numpy()**2
        else:
          xu = ufromopt(xopt)
          nZ = Z
          nnu_Z = xu[0].detach().cpu().numpy()**2
        np.savez_compressed('/content/drive/My Drive/Kaitlin/'+outfile, Z=nZ, nu_Z=nnu_Z)
    
    return nZ, nnu_Z

  
  Z, nu_Z = optimize(Z, nu_Z, nb_iter = 4, flag = '')
  nZ, nnu_Z = optimize(Z, nu_Z, nb_iter = 16, flag = 'all')
  
  print("mass nu_X", nu_X.sum())
  print("mass nu_Z",nu_Z.sum())
  
  return nZ, nnu_Z

def rescale_loss(loss,x,dmax):
  # implement a change of variable of the loss for lbfg
  # x and dmax as suppose to be a list of tensors for each group of variables

  xr = []
  # Create a randomized initialization point
  for (xh,dmaxh) in zip(x,dmax):
    xr.append((xh+(2*torch.rand(xh.shape)-1)*dmaxh).requires_grad_(True))

  # Compute the gradient at randomized intial point
  L = loss(xr)
  L.backward()

  # Compute the scaling coefficient
  a, cp = [], 0.
  for (xrh,dmaxh) in zip(xr,dmax):
    ah = (dmaxh/xrh.grad.max()).sqrt()
    cp += ah*xrh.grad.abs().sum()
    a.append(ah)
  cp = torch.max(torch.tensor(1.),cp)

  a = [ah*cp for ah in a]
  
  def uvar_from_optvar(tx):
    # tranform optimization var into user variable
    return [xh*ah for (xh,ah) in zip(tx,a)]

  def loss_new(xopt_cur):
    # create the new rescaled using the optimization variable
    return loss(uvar_from_optvar(xopt_cur))

  xopt = []
  for (xh,ah) in zip(x,a):
    xopt.append((xh/ah).requires_grad_(True))
    print("ah:", ah)

  return loss_new, uvar_from_optvar, xopt 

def load(Data):
  # Load data
  b = np.load('/content/drive/My Drive/Kaitlin/High' + Data + '.npz')
  print(b.files)
  HZ = b['Z'] # b['Z']
  print("HZ.shape :", HZ.shape)
  Hnu_Z = b['nu_Z']
  print("Hnu_Z.shape:  ", Hnu_Z.shape)
  b = np.load('/content/drive/My Drive/Kaitlin/Low' + Data + '.npz')
  print(b.files)
  LZ = b['Z'] 
  print("LZ.shape :", LZ.shape)
  Lnu_Z = b['nu_Z']
  print("LZ.shape :", Lnu_Z.shape)
  return (HZ, Hnu_Z, LZ, Lnu_Z)

Data = 'Allen'
sig=0.4
(HZ, Hnu_Z, LZ, Lnu_Z) = load(Data)
project3DHtoL(HZ, Hnu_Z, LZ, Lnu_Z,sig, outfile = 'OutAllenSplit3')
del HZ, Hnu_Z, LZ, Lnu_Z